\documentclass[a4paper, 11pt, oneside]{scrartcl}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[german, english]{babel}

\usepackage{amsmath, amssymb, amsthm}
\usepackage{mleftright}
\usepackage{bbm}
\newtheoremstyle{break}
	{\topsep}{\topsep}%
	{\itshape}{}%
	{\bfseries}{}%
	{\newline}{}%
\theoremstyle{break}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{theorem}[lemma]{Theorem}
\newtheorem{corollary}[lemma]{Corollary}
\newtheorem{definition}[lemma]{Definition}
\newtheorem{assumption}{Assumption}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator{\Normal}{\mathcal{N}}
\DeclareMathOperator{\Natural}{\mathbb{N}}
\DeclareMathOperator{\Rational}{\mathbb{Q}}
\DeclareMathOperator{\Real}{\mathbb{R}}
\DeclareMathOperator{\Complex}{\mathbb{C}}
\DeclareMathOperator{\Prob}{\mathbb{P}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\Diag}{Diag}
\DeclareMathOperator{\OffDiag}{OffDiag}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\Expect}{\mathbb{E}}
\DeclareMathOperator{\grad}{\nabla}
\DeclareMathOperator{\ngrad}{\tilde{\nabla}}
\DeclareMathOperator{\Hessian}{\nabla^2}
\DeclareMathOperator{\ELBO}{\mathcal{L}}
\DeclareMathOperator{\BigO}{\mathcal{O}}
\DeclareMathOperator{\KL}{\mathbb{KL}}
\newcommand{\matr}[1]{\boldsymbol{#1}}
\newcommand{\set}[1]{\mathcal{#1}}
\newcommand{\numberthis}{\addtocounter{equation}{1}\tag{\theequation}}

\usepackage[scaled=1]{helvet}
\usepackage{enumitem}

\usepackage{graphicx}
\usepackage{float}
\usepackage{changepage}
\usepackage{stackengine}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{array}% http://ctan.org/pkg/array
\makeatletter
\g@addto@macro{\endtabular}{\rowfont{}}% Clear row font
\makeatother
\newcommand{\rowfonttype}{}% Current row font
\newcommand{\rowfont}[1]{% Set current row font
   \gdef\rowfonttype{#1}#1%
}
\newcolumntype{L}{>{\rowfonttype}l}

\usepackage[svgnames]{xcolor}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{grffile}
\DeclareCaptionFont{myblue}{\color{RoyalBlue}}
\captionsetup{labelfont={myblue,bf}}
\pdfimageresolution 300

\usepackage{tikz}
\usetikzlibrary{calc}
\definecolor{block}{RGB}{0,162,232}
\newenvironment{blockmatrix}{%
	%\left(%-
	\vcenter\bgroup\hbox\bgroup
	\tikzpicture[
		x=1.5\baselineskip,
		y=1.5\baselineskip,
	]%
	}{%
	\endtikzpicture
	\egroup
	\egroup
	%\right)%
}
% \block[#1](#2,#3)#4(#5,#6)
% #1:      fill color
% (#2,#3): lower left corner
% #4:      text in the middle
% (#5,#6): size of the block
\newcommand*{\block}[1][block]{%
	\blockaux{#1}%
}
\def\blockaux#1(#2,#3)#4(#5,#6){%
	\draw[fill={#1}, fill opacity=0.5, draw=black]
	let \p1=(#2,#3),
	\p2=(#5,#6),
	\p3=(#2+#5,#3+#6),
	\p4=(#2+#5/2,#3+#6/2)
	in
	(\p1) rectangle (\p3)
	(\p4) node [opacity=1.0] {\tiny $#4$}
	;%
}

\usepackage[ruled, vlined]{algorithm2e}
\SetKwInput{KwInput}{Input}
\SetKwInput{KwOutput}{Output}
% For multiple line arguments in algorithm
\newlength\mylen
\newcommand\myinput[1]{%
	\settowidth\mylen{\KwIn{}}%
	\setlength\hangindent{\mylen}%
	\hspace*{\mylen}#1\\
}

\usepackage[bottom=3cm,left=2.8cm,right=2.5cm, top=2.5cm]{geometry}

\usepackage[style=apa]{biblatex}
\addbibresource{Literature/literature.bib}
\numberwithin{equation}{section}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % For printing: 						 %
% \usepackage[hidelinks]{hyperref}		 %
\usepackage{hyperref}					 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{aligned-overset}
\usepackage{tcolorbox}
\usepackage{sistyle}
\usepackage[section]{placeins}
\SIthousandsep{,}
\allowdisplaybreaks

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyhead[L]{Thomas Riedel}
\fancyhead[R]{Structured Natural Gradient Descent for Bayesian Deep Learning}

\begin{document}
	% Title Page
	\thispagestyle{empty} % no page numbering
	\parbox{1.5cm}{\resizebox*{90pt}{!}{
		\includegraphics{{Graphics/TUMLogo}}}
	}
	\hspace{310pt}
	\parbox{1.5cm}{\resizebox*{90pt}{!}{
		\includegraphics{{Graphics/MathematikLogo}}}
	}
	\vspace*{1.5cm}
	\begin{center}
		{\Huge Technische Universität München} 
		\\
		\vspace*{1.5cm}
		{\huge \textsc{ Faculty for Mathematics}} 
		\\
		\vspace*{3cm}
		{\Huge {\textbf {Structured Natural Gradient Descent\\ \vspace*{5mm} for Bayesian Deep Learning}}}
		\\
		\vspace*{3cm}
		{\Large Master Thesis}\linebreak \\ 
		{\Large by}\linebreak \\
		{\Large Thomas Riedel}\\
		\vspace*{3cm}
		{\Large 
			\begin{tabular}{ll}
				Examiner: & Daniel Cremers, Prof.\ Dr.\\ % Aufgabensteller
				Supervisor: & Yuesong Shen, M.Sc.\\ % Betreuer
				Abgabetermin: & \selectlanguage{german}\today\selectlanguage{english}
			\end{tabular}
		}
	\end{center}
	\newpage

	% Eidesstattliche Erklärung
	\thispagestyle{empty}
	\vspace*{4cm}
	\noindent
		Hiermit erkläre ich, dass ich die Masterarbeit selbstständig und nur mit den angegebenen Hilfsmitteln angefertigt habe.
	\\
	\\
	\\
	\\
	\\
	\begin{figure}[H]
		\includegraphics[width=0.2\textwidth]{{Graphics/signature}}
	\end{figure}
	\vspace{0cm}
	\noindent
	\underline{\hspace{5cm}}\\%
		München, den \selectlanguage{german}\today\selectlanguage{english}
	\newpage

	\thispagestyle{empty}
	\vspace*{2.2cm}
	\noindent %kein Einzug
	{\Huge {\textbf{Acknowledgment}}} \\
	\vspace*{1.6cm} \\
	% Seitennummerierung "rï¿½misch
	\pagenumbering{roman}
	% Kopfzeilen (automatisch erzeugt)
	\pagestyle{headings}
		First and foremost, I would like to express my gratitude and appreciation to my supervisor Yuesong Shen for his support, the insightful discussions, and suggestions during this thesis.
		Furthermore, I would like to thank my family for their love and support throughout the years, in kindergarten, school and university.
		I am also incredibly grateful for my partner's love, support and loyalty during this process and in the most trying of times. 
		Furthermore, I would also like to thank my classmates and TUM for presenting me with the environment, classes and opportunities for learning and growing as a person. 

	\newpage

	\thispagestyle{empty}
	\section*{Zusammenfassung auf Deutsch}
	\selectlanguage{german}
		Zusammenfassung...
	\selectlanguage{english}

	\section*{Summary in English}
		Neural Networks trained with Bayesian principles allow to model weight uncertainty and have previously been shown to improve calibration and robustness. 
		Natural Gradient Descent is a method that is often used for variational inference which is concerned with fitting a variational posterior to maximize the evidence lower bound (ELBO).
		In this context, we will investigate a normal distribution with structured covariance according to the neural network layers.
		Previously, this has been investigated for diagonal or fully factorized covariance matrices which were layer-agnostic.
		Our approach extends previously developed Structured Natural Gradient Descent methods and allows us to not only model weight dependencies within a layer but also across neighboring layers.
	\newpage

	\thispagestyle{empty}
	\tableofcontents
	\thispagestyle{empty}
	\newpage

	\pagestyle{fancy}
	\pagenumbering{arabic}
	\section{Introduction}
	\setcounter{page}{1}

		\subsection{Machine Learning}
			During the last years, the field of Machine Learning has experienced an enormous surge of interest both in the research community as well as in the media and remarkable progress has been made. 
			McCulloch and Pitts laid the groundwork by trying to model the nervous activity of the brain in 1943 \cite{MP43} followed by the proposal of the ``perceptron'' by Rosenblatt \cite{R58} in 1958. 
			However, as a linear classifier, it was demonstrated by Minsky and Papert in 1969 \cite{M17} that this perceptron was incapable of separating non-linear data like the XOR problem. Thus ensued a so-called ``Dark Age'' or ``AI Winter'' where the aforementioned ideas where no longer pursued and researchers went on to study other topics as the results in this field seemed unpromising. 
			However, in 1985, Rumelhart et al. introduced the multi-layered perceptron that could solve the XOR problem as well as an algorithm called backpropagation which updates the weights of these networks \cite{RHW85}.
			This is still the basis for modern neural networks and Deep Learning.
			Thus, Machine Learning experienced a revival. 
			Cortes and Vapnik introduced the Support Vector Machine in 1995 \cite{CV95} and over the years, many more algorithms and Deep Learning architectures were developed which were widely successful in fields such as Document Recognition (LeNet \cite{LBB+98}), Image Classification (AlexNet \cite{KSH12}), Object Detection (R-CNN \cite{GDD+14, RHG+15}), Semantic and Instance Segmentation (Mask R-CNN \cite{HGD+17, LDY19}), which have heavily impacted the progress of Autonomous Vehicles. 
			Moreover, outside the field of Computer Vision, Deep Learning algorithms have been applied to problems in healthcare \cite{ERR+19}, Natural Language Processing \cite{VSP+17}, Speech Generation \cite{ODZ+16}, and Reinforcement Learning \cite{alphastarblog}.
			An overview over some of these topics can be found in \cite{S15, SM19, VDD+18}.

			Alongside the theoretical strides, major progress in the field of hardware and software to facilitate running and training these more complex models has been made.
			The amount of data being generated is growing exponentially \cite{ZPG13} while the amount of computing power and data storage to process and store all this data is becoming more powerful and attainable (Moore's Law \cite{S97}), parallelization during the training of these algorithms through specialized hardware such as GPUs or TPUs allowed for much shorter training times \cite{H18} and experimenting of different Deep Learning architectures. 
			Furthermore, major Machine Learning libraries such as \emph{TensorFlow} (\url{https://www.tensorflow.org}), \emph{Keras} (\url{https://keras.io}), \emph{PyTorch} (\url{https://pytorch.org}), and \emph{scikit-learn} (\url{https://scikit-learn.org/stable/}), to only name the most popular ones, have been open-sourced to the public for free use. 

			Machine Learning is thus a heavily active area of research which this thesis aims to contribute to. 

			Essentially, each layer in a neural network with $L$ layers in total takes as input the output of the previous layer, applies a linear-affine function to it and and differentiable scalar function $\sigma$, i.e. for layer $\ell$, we have $\matr{x}_{\ell} = \sigma(\matr{W}^{\ell} \matr{x}_{\ell} + \matr{b}^{\ell})$, where the weights of the layer are $\matr{W}^{\ell}$, and $\matr{b}^{\ell}$, respectively. 
			For the first layer, we simply take the input $\matr{x}$ and apply the linear-affine transformation and scalar function to it as $\matr{x}_{1} = \sigma(\matr{W}^{1} \matr{x} + \matr{b}^{1})$. 
			In the following section, we will present how to train such networks. 

		\subsection{Maximum Likelihood Estimation}
			Given a Neural Network $f$ with weights $\matr{w}$ and data $\set{D} = \{(\matr{x}_i, \matr{y}_i) | i \in \{1, \dots, N\}\}$, we would like the network to approximate the true distribution $\Prob(\matr{y} | \matr{x})$ as best as possible. 
			This is usually done via Maximum Likelihood Estimation (MLE) which aims to maximize the likelihood of the data,
			\begin{equation}
				\matr{\hat{w}}_{MLE} := \argmax_{\matr{w} \in \Real^d} p(\set{D} | \matr{w}).
				\label{eqn:MLE}
			\end{equation}
			The above term can replaced by the negative log-likelihood and turned into an equivalent minimization problem
			\begin{equation}
				\matr{\hat{w}}_{MLE} := \argmin_{\matr{w} \in \Real^d} \ell(\matr{w}),
				\label{eqn:MLE_NLL}
			\end{equation}
			where 
			\begin{equation}
				\ell(\matr{w}) := \frac{1}{N} \sum_{i=1}^N p_i \log p_i
				\label{eqn:NLL}
			\end{equation}
			the cross-entropy for probability $p_i = \max \Prob (\matr{\hat{y}}_i | \matr{x}_i )$. 
			Together with efficient modern optimizers such as SGD, Adam or RMS-Prop, one can train large neural networks on such a dataset. 
			However, this approach is known to be prone to overfitting, where a model performs well on the training dataset $\set{D}_{\text{train}}$, but fails to generalize to some unseen hold-out test set $\set{D}_{\text{test}}$. 
			There exist multiple techniques to remedy this issue such as regularization, early stopping, and dropout to name a few.
			However, other issues such as reliability and robustness also remain which will be discussed in the following sections. 
			Bayesian approaches provide a way how to train robust, uncertainty-aware models that generalize well. 

		\subsection{Variational Inference}
			Instead of maximizing the likelihood as in the frequentist setting, one can take a Bayesian approach and place a prior $p$ on the weights $\matr{w}$ and maximize a posterior $p(\matr{w} | \set{D})$ instead:
			\begin{equation}
				p(\matr{w} | \set{D}) = \frac{p(\set{D} | \matr{w}) p(\matr{w})}{p(\set{D})} = \frac{p(\set{D} | \matr{w}) p(\matr{w})}{\int p(\set{D} | \matr{w}) p(\matr{w}) \text{d}\matr{w}} \propto p(\set{D} | \matr{w}) p(\matr{w}) = \text{likelihood} \times \text{prior},
				\label{eqn:BayesRule}
			\end{equation}
			where $\set{D}$ the data, giving the Maximum A Posteriori (MAP) estimate
			\begin{equation}
				\matr{\hat{w}}_{MAP} := \argmax_{\matr{w} \in \Real^d} p(\matr{w} | \set{D})
				\label{eqn:MAP}
			\end{equation}
			However, this distribution is much more difficult to sample from and optimize over than in the case of MLE due to the difficulty of sampling from the posterior and the nonconjugacy of the product $p(\set{D} | \matr{w}) p(\matr{w})$ \parencite{KNT+18}.
			Thus, one can simplify the problem in Eqn. (\ref{eqn:MAP}) by introducing a simpler posterior called the variational posterior $q(\matr{w})$ such that $q(\matr{w}) \approx p(\matr{w} | \set{D})$ which is done by minimizing the KL divergence between the two distributions 
			$$\min_{q \in \set{Q}} \KL(q(\matr{w}) || p(\matr{w} | \set{D})),$$
			where $\set{Q}$ the set of all possible distributions and 
			$$\KL(q(\matr{w}) || p(\matr{w} | \set{D})) = \int q(\matr{z}) \log \frac{q(\matr{z})}{p(\matr{z} | \set{D})} d\matr{z}$$
			the Kullback-Leibler Divergence. 

			However, optimizing over this set can become intractable such that one chooses a subset of distributions parametrized by $\lambda \in \Real^{\tilde{d}}, \tilde{d} \in \Natural$ and then maximize the ELBO 
			\begin{equation}
				\max_{\lambda \in \Real^n} -\Expect_{q_{\lambda}(\matr{w})}[\ell(\matr{w})] + \gamma \KL(q_{\lambda}(\matr{w}) || p(\matr)) := \text{ELBO}(\lambda),
				\label{eqn:ELBO}
			\end{equation}
			where $\gamma \ge 0$ a regularization parameter \parencite{ZSD+17, LSK20, LNK+21}.
			$\gamma = 1$ corresponds to classic Variational Inference. 
			However, in keeping with the traditional optimization setting of minimization, we will minimize over the negative ELBO instead,
			\begin{equation*}
				\min_{\matr{\lambda} \in \Real^n} \Expect_{q_{\lambda}(\matr{z})}[\ell(\matr{z})] + \gamma \KL(q_{\lambda}(\matr{z}) || p(\matr{z})) =: \set{L}(\matr{\lambda})
				\label{eqn:NegELBO}
			\end{equation*}
			Note the different number of dimensions $d$ and $n$ in the optimization problems (\ref{eqn:MLE_NLL}) and (\ref{eqn:NegELBO}).
			Usually, we have $d \ll n$ in Variational Inference (VI) which for today's large models becomes infeasible very quickly. 
			For example, in practice we usually choose 
			\begin{equation}
				p(\matr{w}) := \Normal(\matr{0}, \eta^{-1} \matr{I}), \quad q_{\lambda}(\matr{w}) := \Normal(\matr{w} | \matr{\mu}, \matr{\Sigma}),
				\label{eqn:VI_Normal}
			\end{equation}
			where $\eta > 0$ a spherical precision parameter, $\matr{\lambda} := \{\matr{\mu}, \matr{\Sigma}\}$ the parameterization for the variational posterior, and $\matr{\mu} \in \Real^d, \matr{\Sigma} \in \Real^{d \times d}$ leading to quadratic storage cost $d + d^2 = \BigO(d^2)$ for VI \parencite{ZSD+17, KNT+18, LSK20}.
			To reduce complexity, one can take the covariance matrix $\matr{\Sigma}$ to be a diagonal matrix or a vector, respectively, as $\matr{\Sigma} := \diag(\matr{\sigma^2})$ reducing the storage cost from quadratic to linear, $d + d = \BigO(d)$. 
			However, this approach assumes zero correlation between the weights and only models the variances of each individual weight in the diagonal of the covariance $\matr{\Sigma}$. 
			Thus, finding a way of parametrizing the covariance matrix to be more informative than a diagonal matrix but less costly than a full square matrix is a desirable direction which will be presented in this thesis.
			One has to strike a balance between a diagonal covariance matrix $\matr{\sigma} \in \Real^d$, and a fully factorized matrix $\matr{\Sigma} \in \Real^{d \times d}$.

			Furthermore, in this thesis, imposed structures on the covariance matrix with respect to the neural architecture structure are of special interest.
			To illustrate this approach, consider the covariance $\matr{\Sigma} \in Real^{d \times d}$ over the weights $\matr{w} \in \Real^d$ for a neural network $f$ with $L$ layers.
			Denote the number of parameters for layer $i$ as $d_i$ such $d = \sum_{i=1}^L d_i$.
			Now, divide $\matr{\Sigma}$ into $L$ blocks with each block $\matr{\Sigma}^{(i, j)} \in \Real^{d_i \times d_j}$ the covariance of layer $i$ and $j$, where $i, j \in [L]$.
			Obviously, a fully parametrized matrix $\matr{\Sigma}$ is not feasible so we can approach it as diagonal $\matr{\Sigma} = \Diag(\matr{\sigma})$, block-diagonal where 
			\begin{equation*}
				\matr{\Sigma}^{(i, j)} = 
					\begin{cases}	
						\matr{\Sigma}^{(i, i)},& \text{ if } i = j \\
						\matr{0},& \text{ otherwise }
					\end{cases}
			\end{equation*}
			or block-tridiagonal
			\begin{equation*}
				\matr{\Sigma}^{(i, j)} = 
					\begin{cases}	
						\matr{\Sigma}^{(i, j)},& \text{ if } |i - j| \le 1 \\
						\matr{0},& \text{ otherwise.}
					\end{cases}
			\end{equation*}
			Each of the subblocks might have a certain substructure as well. 
			One can image a more general structure where only the neighboring $k$ layers are assumed to be correlated or correlations between specific layers $i, j$ are accounted for. 
			We will keep it simple however and only investigate diagonal, block-diagonal and block-tridiagonal structures. 

			As mentioned before, the fully diagonal approach neglects correlations of the weights and makes the rather strong assumption of weight independence.
			A block-diagonal covariance can model intra-layer dependencies, i.e. correlations within a single layer but assumes independence across layer. 
			A block-tridiagonal covariance on the other hand can model these intra-layer as well as inter-layer dependencies for neighboring layers. 
			Diagonal and block-diagonal structures have also been investigated in \parencite{ZSD+17, MG15} but inter-layer dependencies were neglected.
			We will introduce an approach how to introduce these structures for a modern NN.

		\subsection{Optimization} % First Order, Second Order, Newton Method, Newton-like Methods, Damping, Trust Region?
			An optimization problem 
			\begin{equation}
				\min_{x \in \set{X}} \mathcal{L} (x)
				\label{eqn:OptimizationProblem}
			\end{equation}
			is a minimization problem involving a target function $\set{L}: \set{X} \to \Real$ in an admissible region $\set{X} \subset \Real^d$ where Problem (\ref{eqn:OptimizationProblem}) is called an unconstrained optimization problem if $\set{X} = \Real^d$ and a constrained optimization problem if $\set{X} \subsetneq \Real^d$.
			The goal is then finding a minimizer $x^{*} \in \set{X}$ such that $\mathcal{L} (x^*) \le \mathcal{L} (x) \quad \forall x \in \set{X}$.
			In rare cases such as linear or quadratic problems, this solution can be found analytically but in general, the minimizer is found through iteratively improving an initial starting solution $x_0$ until a stopping criterion is reached. 
			Furthermore, when $f$ is not strongly convex, such a minimizer may not be unique and many local regions exist where these methods can get stuck in local minima. 
			Generally speaking, there are two types of optimization methods, first-order methods and second-order methods. 
			First-order methods improve the initial solution using local information of the gradient and stepping along this direction with some step length $\alpha$. 
			Probably the most famous of such algorithms is Gradient Descent (GD) which steps along the direction of maximum local descent, the negative gradient, with some step length $\alpha_t > 0$ which can be determined by some step size rule such as the Armijo rule or the Powell-Wolfe rule. 
			In modern, high-dimensional scenarios such as Deep Learning, a constant step size or so-called learning rate $\alpha_t = \alpha > 0$ is used with potentially a heuristic, decreasing schedule.
			\begin{equation}
				\matr{w}_{t+1} \leftarrow \matr{w}_t - \alpha_k \grad \mathcal{L} (\matr{w}_t)
				\label{eqn:GradientDescent}
			\end{equation}
			However, in DL scenarios, the loss function landscape contains many local minima or saddle points which GD algorithm are prone to get stuck in. 
			Thus, considerable amount has been done to improve the convergence abilities of these algorithms such as SGD with momentum, RMSprop, Adam \parencite{KB14, R16}. 
			RMSProp and Adam scale the update in Eqn: (\ref{eqn:GradientDescent}) by some diagonal vector that is estimated during training.

			These methods are first-order methods and only take local gradient information into account for their updates. 
			However, while their update can be scaled by some diagonal vector, it cannot be rotated into a more beneficial direction.
			Second-order methods incorporate curvature information in their updates through the Hessian which can alleviate this issue.
			The most basic variant of this is the update rule
			\begin{equation}
				\matr{w}_{t+1} \leftarrow \matr{w}_t - \alpha \left( \matr{H}_t \right)^{-1} \matr{g}_t,
				\label{eqn:NewtonMethod}
			\end{equation}
			where Eqn. (\ref{eqn:NewtonMethod}) is called Newton's Method for $\matr{H}_t = \Hessian \mathcal{L} (\matr{w}_t)$ and $\matr{g}_t = \grad \mathcal{L} (\matr{w}_t)$ the Hessian and gradient, respectively. 
			However, the Hessian might be hard to compute, store or invert especially for high-dimensional problems. 
			Thus, one can take a diagonal estimate of the true Hessian or replace it with an approximation $\matr{H}_t \approx \Hessian \mathcal{L} (\matr{w}_t)$ that is easier to compute and invert. 
			This is then called a Newton-like method.
			
			The theory presented in this section covers only a small portion and the interested reader is encouraged to study \parencite{WN99} for more details.

		\subsection{Natural Gradient Descent}
			The previous section presented a small overview of ``classic'' optimization methods in the literature. 
			However, recall that our goal is minimizing the negative ELBO in Eqn. (\ref{eqn:NegELBO}), i.e. the loss function is 
			\begin{equation*}
				\set{L}(\matr{\lambda}) = \Expect_{q_{\lambda}(\matr{w})}[\ell(\matr{w})] + \gamma \KL(q_{\lambda}(\matr{z}) || p(\matr{z}).
			\end{equation*}
			One could naively apply these methods to minimize this function.
			However, while the standard gradient descent in Eqn. (\ref{eqn:GradientDescent}) is the steepest descent direction in Euclidean space, variational parameters $\matr{\lambda}$ are situated on a Riemannian manifold with the Fisher Information metric induced by $\matr{F}$, the Fisher Information matrix. 
			That is, instead of Euclidean distance induced by the canonical scalar product, we have the metric
			\begin{equation*}
				\langle u, v \rangle_F = u^T F v \quad \Rightarrow \quad \|u\|_F = \sqrt{\langle u, u \rangle_F} \quad \Rightarrow \quad d(u, v)_F = \|u - v\|_F
			\end{equation*}			

			\begin{definition}[Fisher Information Matrix (FIM)]
				\label{def:FIM}
				The FIM is defined as 
				\begin{align*}
					F &:= \Expect_{q_{\lambda}}\left[(\grad \log q_{\lambda}) (\nabla \log q_{\lambda})^T\right] &\\
				  	  &= -\Expect_{q_{\lambda}}\left[ \Hessian \log q_{\lambda} \right]
				\end{align*}
			\end{definition}
			\cite{KR21} note that the GD update (\ref{eqn:GradientDescent}) solves
			\begin{align*}
				\matr{\lambda}_{t+1} &= \argmin_{\matr{\lambda}} (\grad \mathcal{L}(\matr{\lambda}_t))^T \matr{\lambda} + \frac{1}{2\alpha_t} \|\matr{\lambda} - \matr{\lambda}_t\|^2.
			\end{align*}
			However, since the natural parameters lie on a Riemannian manifold with a local metric $\KL$, the update must solve
			\begin{align*}
				\matr{\lambda}_{t+1} &= \argmin_{\matr{\lambda}} (\grad \mathcal{L}(\matr{\lambda}_t))^T \matr{\lambda} + \frac{1}{\alpha_t} \KL (q_{\lambda} \mid\mid p) &\\
				&\approx \argmin_{\matr{\lambda}} \mathcal{L}(\matr{\lambda}_t) + (\nabla \mathcal{L}(\matr{\lambda}_t))^T \matr{\lambda} + \frac{1}{\matr{\alpha}_t} \matr{\lambda}^T F \matr{\lambda} &\\
				&= \matr{\lambda}_t -\alpha_t F^{-1} \nabla \mathcal{L}(\matr{\lambda}_t) = \matr{\lambda}_t + \alpha_k g_k.
			\end{align*}
			Here, we used $\matr{g}_t = -\matr{F}^{-1} \grad \mathcal{L}(\matr{\lambda}_t) = -\ngrad \mathcal{L}(\matr{\lambda}_t)$, where the latter term is the so-called natural gradient.
			Note the similarity to the Newton step when observing Definition \ref{def:FIM} where the FIM is not the Hessian of the ELBO but the log variational posterior.
			This is precisely the definition of the Natural Gradient.
			\begin{definition}[Natural Gradient]
				\label{def:NaturalGradient}
				The Natural Gradient \parencite{A16} is defined as 
				\begin{equation}
					\ngrad \mathcal{L} := F^{-1} \grad \mathcal{L}.
					\label{eqn:NaturalGradient}
				\end{equation}
			\end{definition}

			There is some similarity between Newton's Method and Natural Gradient Descent as the former involves the Hessian of the loss function and the latter the expected Hessian of some log-probability but generally, they are different. 
			This allows us to define Natural Gradient Descent (NGD) \parencite{A16} as 
			\begin{equation}
				\matr{\lambda}_{t+1} \leftarrow \matr{\lambda}_t - \alpha \ngrad \mathcal{L} (\matr{\lambda}_t).
				\label{eqn:NGD}
			\end{equation}
			It is a second-order method that scales and rotates the gradient within the Riemannian manifold. 

		\subsection{Calibration}
			More sophisticated models have been able to achieve increasingly better results in classification accuracy over the years.
			These models output a vector from the $m$-dimensional probability simplex $\set{S} = \{\matr{x} \in \Real^m\ | \sum_{i=1}^m x_i = 1, \quad x_i \ge 0 \forall i \in [m]\}$ where $m$ is the number of classes.
			The final prediction is then taken as the argmax of this vector, $\hat{y} = \argmax f(\matr{x})$, with an associated confidence $\hat{p} = \max f(\matr{x})$.
			This formulation suggests that the model's confidence is equal to the probability of being of this label.
			However, this was shown to be false for modern neural networks \parencite{GPS17}.
			Formally, one would like a model to be well-calibrated such that its prediction probability agrees with the likelihood of the prediction,
			\begin{equation}
				\Prob (\hat{y} = y | \hat{p} = p) = p, \quad \forall p \in [0, 1].
				\label{eqn:calibration}
			\end{equation}
			This is a continuous notion of calibration and cannot be computed from finitely many samples in practice.
			Thus, one chooses to approximate this term over $M$ discrete bins of equal width where the $m$-th  $B_m$ the bin of all predictions with confidences falling into the interval $(\frac{m-1}{M}, \frac{m}{M}]$.
			We can then compute the average accuracy of this bin as 
			\begin{equation*}
				\text{acc}(B_m) = \frac{1}{B_m} \sum_{i \in B_m} \mathbbm{1}(\hat{y}_i = y_i)
			\end{equation*}
			and its average confidence as 
			\begin{equation*}
				\text{conf}(B_m) = \frac{1}{B_m} \sum_{i \in B_m} \hat{p}_i.
			\end{equation*}
			We can visualize a model's calibration in a reliability diagram with the binned confidence on the $x$-axis and the expected accuracy on the $y$-axis.
			For a perfectly calibrated model, we would expect these two terms to agree over all the bins $B_m$, i.e. the diagram shows the identity function.
			There exist many metrics to measure calibration but the most popular metrics are Expected Calbration Error (ECE) and Maximum Calibration Error (MCE):
			\begin{align*}
				\text{ECE} &= \sum_{m=1}^M \frac{|B_m|}{N} \left| \text{acc}(B_m) - \text{conf}(B_m) \right| \\
				\text{MCE} &= \max_{m \in \{1, \dots, M\}} \left| \text{acc}(B_m) - \text{conf}(B_m) \right|.
			\end{align*}
			However, these measure calibration at the top-most level and ignores the calibration of lower confidences. 
			Furthermore, in modern neural networks trained on the NLL loss, networks have sharp confidence outputs so the the majority of confidences land in the right-most bin which renders ECE ineffective. 
			Thus, other calibration metrics such as Static Calibration Error (SCE), Adaptive Calibration Error (ACE) \parencite{NDZ+19}, or Top-$k$ ECE \parencite{KOH22} have been proposed.

			There exist many techniques to improve calibration such as histogram binning, Platt scaling, matrix/vector scaling, temperature scaling, Bayesian Binning into Quartiles (BBQ), and isotonic regression \parencite{GPS17}.
			However, results by \parencite{ZSD+17} show the improvement noisy optimizers can have for calibration.
			In the experiments, we will investigate the calibration improvements of Structured NGD methods.

		\subsection{Robustness}
			There has been extensive research in the field of robustness of Neural Networks which refers to the ability to be robust against perturbations in their inputs.
			The research focusses mainly on artificial worst-case adversarial examples and certification radii \parencite{CRK19, GSS14}.
			Adversarial examples are characterized by adding a small, for the human eye indistinguishable amount of noise to an input that drastically changes the model's output and can be easily constructed via Gradient Ascent with respect to an input $\matr{x}$, i.e. one iteratively computes
			\begin{equation*}
				\matr{\tilde{x}}_{t+1} = \matr{\tilde{x}}_t + \alpha_t \grad_{\matr{x}} \ell(\matr{w}; \matr{\tilde{x}}_t),
			\end{equation*}
			where the initial adversarial example is set as $\matr{\tilde{x}}_0 = \matr{x}$.

			However, following the work of \parencite{HD19, HBM21}, more common corruptions such as Gaussian noise, different weather and lighting conditions are more representative for real-life, safety-critical situations since these can easily occur due to sensor uncertainty or an underrepresented scene in the data.
			Furthermore, it allows one to measure the model's generalization ability to out-of-distribution data. 

			While certain network architectures and data augmentation during training are beneficial for increasing robustness, we will investigate how an uncertainty-aware Bayesian Neural Network fares in this regard.

	\section{Related Work}
		Uncertainty Quantification is of particular interest in Bayesian Deep Learning.
		Bayesian Neural Networks (BNNs) allow decomposing uncertainty into aleatoric and epistemic uncertainty, i.e., uncertainty in the observations and uncertainty in the model \parencite{APH+21, KG17, W22}. 
		BNNs have also been studied in the domain of Active Learning \parencite{DHD+17} and Reinforcement Learning \parencite{KNT+18}.
		Noisy versions of existing optimizers have been an interest in the field of Variational Inference as well with variational versions of popular optimizers such as VOGN, Vadam or Vprop \parencite{KNT+18}.
		Natural Gradient Descent can also be seen as a second-order Newton-like update rule.
		Such rules also allow for the design of variational optimization where one chooses a tampering parameter $\gamma = 0$, removing the $\KL$-term in the negative ELBO loss function. 
		Imposing additional structures on the Fisher matrix or covariance matrix results in structured second-order optimization \parencite{LNK+21} which allows the update to take into account local curvature information and possibly significantly better convergence properties. However, one needs to balance out the higher quality of the descent direction with the difficulty in computation which constitutes a difficult tradeoff.

	\section{Structured Natural Gradient Descent}
		Recall the Bayesian formulation of the learning algorithm
		\begin{equation}
			\min_{\matr{\lambda} \in \Real^n} \Expect_{q_{\matr{\lambda}} (\matr{z})} [\ell (\matr{z})] - \gamma \KL (q_{\matr{\lambda}} (\matr{z}) || p(\matr{z})),
			\label{eqn:BayesianLearningAlgorithmUnconstrained}
		\end{equation}
		where $\ell(\matr{z})$ the Negative Log-Likelihood (NLL) and 
		\begin{equation*}
			\KL (q_{\matr{\lambda}} (\matr{z}) || p(\matr{z})) = \int q_{\matr{\lambda}} (\matr{x}) \log \left( \frac{q_{\matr{\lambda}} (\matr{x})}{p (\matr{x})} \right) d \matr{x}
		\end{equation*}
		the Kullback-Leibler Divergence. 
		Our aim is solving Problem (\ref{eqn:BayesianLearningAlgorithmUnconstrained}) for variational posteriors $q_{\matr{\lambda}} (\matr{z}) := \Normal(\matr{z} | \matr{\mu}, \matr{S}^{-1})$ a multivariate Gaussian distribution via the Bayesian Learning Rule \parencite{LSK20}
		\begin{equation}
			\matr{\lambda} \leftarrow \matr{\lambda} - \alpha \matr{\hat{g}}
			\label{eqn:BLR}
		\end{equation}
		on the variational parameters $\matr{\lambda} = (\matr{\mu}, \matr{S})$, where $\alpha > 0$ the fixed learning rate which can be decreased according to some schedule during training and $\matr{\hat{g}} = \ngrad \ell (\matr{w}_k)$ the Natural Gradient.
		This results in an update rule 
		\begin{align*}
			\matr{\mu} &\leftarrow \matr{\mu} - \alpha \matr{S}^{-1} \Expect_{q(\matr{z} | \matr{\lambda})} [\grad_{\matr{z}} \ell (\matr{z})] \\
			\matr{S} &\leftarrow (1 - \alpha) \matr{S} + \alpha \Expect_{q(\matr{z} | \matr{\lambda})} [\Hessian_{\matr{z}} \ell (\matr{z})].
		\end{align*}

		However, the variational parameter $\matr{\Sigma}$ does not lie in the unconstrained set $\Real^{d \times}$ but rather in the set of symmetric positive definite matrices
		\begin{equation}
			\set{S}_+ = \{\matr{X} | \matr{X} = \matr{X}^T, \quad \matr{X} \succ 0\} \subset \Real^{d \times d},
			\label{eqn:psdMatrices}
		\end{equation}
		where the notation $\matr{X} \succ 0$ denotes a positive definite matrix, i.e., a matrix whose eigenvalues are all greater than zero. 
		Instead of optimizing the unconstrained problem (\ref{eqn:BayesianLearningAlgorithmUnconstrained}), we will minimize the constrained problem
		\begin{equation}
			\min_{\matr{\lambda = (\matr{\mu}, \matr{S}) \in \Real^d \times \set{S}_+}} \Expect_{q_{\matr{\lambda}} (\matr{z})} [\ell (\matr{z})] + \gamma \KL (q_{\matr{\lambda}} (\matr{z}) || p (\matr{z}))
		\end{equation}
		With the iterative update rule in Eqn. (\ref{eqn:BLR}), this constraint can be violated. 
		One can remedy this issue via a projection step onto the constraint set
		\begin{equation}
			\matr{\lambda} \leftarrow \set{P}_{\set{S}_+} (\matr{\lambda} - \alpha \matr{\hat{g}})
		\end{equation}
		or a line search \parencite{SEH18} which are both time consuming in practice for high-dimensional problems.
		\parencite{LSK20} introduce a new ``improved Bayesian Learning Rule'' (iBLR) which does not require such a line search since their update naturally respects this constraint. 
		\begin{equation}
			\begin{aligned}
				\matr{\mu} &\leftarrow \matr{\mu} - \alpha \matr{S}^{-1} \Expect_{q_{\matr{\lambda}} (\matr{z})} [\grad_{\matr{z}} \ell (\matr{z})] \color{red} + \matr{0} \color{black} \\
				\matr{S} &\leftarrow (1 - \alpha) \matr{S} + \alpha \Expect_{q_{\matr{\lambda}} (\matr{z})} [\Hessian_{\matr{z}} \ell (\matr{z})] \color{red} + \frac{\alpha^2}{2} \matr{\hat{G}} \matr{S}^{-1} \matr{\hat{G}} \color{black},
			\end{aligned}
			\label{eqn:iBLR}
		\end{equation}
		where $\matr{\hat{G}} := \matr{S} - \Expect_{q_{\matr{\lambda}} (\matr{z})} [\Hessian_{\matr{z}} \ell (\matr{z})]$

		Using this improved update rule, \parencite{LSK20} approach the precision matrix $\matr{S} = \diag{\matr{s}}$ as diagonal and propose an Adam-like Update by incorporating momentum in the update rule which is presented in Algorithm \ref{alg:AdamLikeOptimizer}.
		This diagonal definition significantly reduces the number of parameters from a fully factorized Gaussian from $d + d^2 = \BigO(d^2)$ to $d + d = \BigO(d)$, i.e., quadratic to linear. 
		However, this decomposes the variational posterior into 
		\begin{equation*}
			q_{\matr{\lambda}} (\matr{z}) = \prod_{i=1}^n q_{\lambda_i} (z_i)
		\end{equation*}
		and assumes total independence of the network weights which is certainly not true in practice.

		\parencite{LNK+21} expanded on this idea by parametrizing the precision matrix via its square root, $\matr{S} = \matr{B} \matr{B}^T$, which naturally respects the positive definite constraint on the precision and covariance matrix.
		Furthermore, depending on the choice of the square-root precision $\matr{B}$, $\matr{S}$ can take on certain structures.
		Instead of computing the Fisher matrix and performing an update step in a global space $\Omega_{\tau}$, they propose to compute the Natural Gradient in a local space which is then mapped onto the global space. 
		Namely, through local $\matr{\eta} \in \Omega_{\eta}$, auxiliary $\matr{\lambda} \in \Omega_{\lambda}$, and global $\matr{\tau} \in \Omega_{\tau}$, connected via the mapping $\matr{\tau} = \matr{\psi}(\matr{\lambda})$ and $\matr{\lambda} = \matr{\phi}_{\matr{\lambda}_t} (\matr{\eta})$, they perform Structured Natural Gradient Descent 
		\begin{equation}
			\matr{\lambda}_{t+1} \leftarrow \matr{\phi}_{\matr{\lambda}_t} \left( \matr{\eta}_0 - \alpha \matr{\hat{g}}_{\matr{\eta}_0} \right), \quad \matr{\tau}_{t+1} \leftarrow \matr{\phi} (\matr{\lambda}_{t+1})
		\end{equation}
		Namely, they introduce the update rule 
		\begin{equation}
			\begin{aligned}
				\matr{\mu}_{t+1} &\leftarrow \matr{\mu}_t - \alpha \matr{S}_t^{-1} \matr{g}_{\matr{\mu}} \\
				\matr{B}_{t+1} &\leftarrow \matr{B}_t \matr{h} \left( \alpha \matr{C_{\text{up}}} \odot \kappa_{\text{up}} (2 \matr{B}_t^{-1} \matr{G}_{\matr{\Sigma}} \matr{B}_t^{-T}) \right),
			\end{aligned}
			\label{eqn:SNGD_Rule}
		\end{equation}
		where $\kappa_{\text{up}}$ a function the projection to structured group, $\matr{G}_{\matr{\Sigma}} = \frac{1}{2} \Expect_{q_{\matr{\lambda}}} [\Hessian \ell (\matr{z})]$ the Hessian of the NLL loss which can be approximated as diagonal or subsampled through Monte Carlo samples, $\matr{C}_{\text{up}}$ the Fisher matrix of the structured matrix group, and $\matr{h} (\matr{M}) := \matr{I} + \matr{M} + \frac{1}{2} \matr{M}^2$.
		More details on these will follow in the next section. 
		With the update rule iBLR in Eqn. (\ref{eqn:iBLR}), an update to the precision matrix $\matr{S}$ can violate this structure while the update rule in Eqn. (\ref{eqn:SNGD_Rule}) maintains the positive definite constraint as well as the structure constraint. 

		Their approach allows to represent a wide range of structures for the precision matrix for general Variational Inference.
		The interested reader is encouraged to consult the original work \parencite{LNK+21} for a description of these structures.
		In this thesis, we will specifically investigate a diagonal plus low-rank matrix and block arrowhead covariance structure.
		However, the structured update rule as introduced in the original paper has a ``global'' structure with respect to the underlying neural network and is layer-agnostic.
		That is, we would like to adapt the covariance structure in such a way that it allows one to model weight dependencies within and across model layers.
		We will derive such algorithms for structured covariance matrices which model intra and inter-layer correlations. 
		\begin{algorithm}[!htbp]
			\DontPrintSemicolon
			\KwInput{$\text{Initial parameters } \matr{\mu}_0, \matr{s}_0$}
			\myinput{$\text{Learning rate } \alpha > 0, \text{ prior precision } \eta > 0$}
			\myinput{$\text{Regularization } \gamma \ge 0, \text{ damping } \xi \ge 0, \text{ Number of MC samples } M \ge 1$}
			\myinput{$\beta_1, \beta_2 \in (0, 1]$}
			% \KwOutput{}

			Set $\matr{m}_0 = 0$

			\For{$t = 0, 1, \dots$}{
				$\matr{z}_m = \matr{\mu_t} + (N s_t)^{-1/2} \odot \varepsilon_m \text{ , where }\matr{\varepsilon}_m \sim \Normal(\matr{0}, \matr{I}_{d\times d}), \quad \forall m \in \{1, \dots, M\}$

				Sample a minibatch $\set{M}$ from the training set.

				Compute sampled mini-batch gradients $\matr{\bar{g}}_m = \frac{1}{|\set{M}|} \sum_{i \in \set{M}} \grad \ell_i (\matr{z}_m) \quad \forall m \in \{1, \dots, M\}$.

				$\matr{g_{\mu}} = \frac{\eta}{N} \matr{\mu}_t + \frac{1}{M} \sum_{m=1}^M \matr{\bar{g}}_m$

				$\matr{m}_{t+1} = \beta_1 \matr{m}_t + (1 - \beta_1) \matr{g_{\mu}}$

				Debias the update and add damping to the precision vector
				$\matr{\bar{m}} = \matr{m}_{t+1} / (1 - \beta_1^t), \quad \matr{\bar{s}} = \matr{s}_{t+1} / (1 - \beta_2^t) + \xi$

				$\matr{\hat{m}} = \matr{\bar{m}} / \matr{\bar{s}}$

				$\matr{g_s} = \frac{\gamma \eta}{N} - \gamma \matr{s}_t + \frac{1}{M} \sum_{m=1}^M \left[ (N \matr{s}_t) \odot (\matr{z}_m - \matr{\mu}_t) \right] \odot \matr{\bar{g}}_m$

				$\matr{\mu}_{t+1} = \matr{\mu}_t - \alpha \matr{\hat{m}}$

				$\matr{s}_{t+1} = \matr{s}_t + (1 - \beta_2) \matr{g_s} \color{red} + \frac{1}{2} (1 - \beta_2)^2 \matr{g_s} \odot \matr{s}_t^{-1} \odot \matr{g_s} \color{black}$
				}
			\caption{Adam-like Optimizer using the iBayesLRule \parencite[Fig.~1]{LSK20}}
			\label{alg:AdamLikeOptimizer}
		\end{algorithm}

		\subsection{Adaptive Damping}
			Recall the second-order update scheme 
			\begin{equation}
				\matr{w}_{t+1} = \matr{w}_t - (\matr{H}_t)^{-1} \matr{g}_t,
			\end{equation}
			where the update rule is called Newton's Method if $\matr{H}_t = \Hessian \ell(\matr{w}_t)$ the exact (or subsampled) Hessian, $\matr{g}_t = \grad \ell (\matr{w}_t)$ the exact (or subsampled) gradient, and Natural Gradient Descent if $\matr{H}_t = \matr{F}_t$ the Fisher Matrix.
			However, in practice, one cannot make any assumption on the conditioning and eigenvalues of $\matr{H}_t$ and adds a diagonal damping term $\xi \matr{I}$ to this matrix in a Levenberg-Marquardt style update.
			As \parencite{MG15} argue, it is beneficial to perform adaptive damping as well and vary the damping $\xi$ in a trust-region like approach based on a local quadratic approximation
			\begin{equation*}
				m(\matr{p}) := \ell(\matr{w}_t) + (\matr{g}_t)^T \matr{p} + \frac{1}{2} \matr{p}^T \matr{H}_t \matr{p}.
				\label{eqn:TrustRegionFunction}
			\end{equation*}
			around the target function $\ell$. 
			One would then solve 
			\begin{equation}
				\min_{\matr{p}} m(\matr{p}), \quad \text{subject to } \|\matr{p}\| \le \xi
				\label{eqn:TrustRegion}
			\end{equation}

			\begin{equation}
				\rho_t := \frac{\ell (\matr{w}_t) - \ell (\matr{w}_t + \matr{p})}{m(\matr{0}) - m(\matr{p})}
				\label{eqn:TrustRegionApproximationQuality}
			\end{equation}
			\begin{equation}
				\matr{\mu}_{t+1} \leftarrow 
			\end{equation}

			There are a number of ways we can interpret damping in the context of update rule (\ref{eqn:SNGD_Rule}). 
			One way is in calculating the local Natural Gradient $\matr{\hat{g}}_{\matr{\eta}_0}^{(t)} = (\matr{F}_{\matr{\eta}} (\matr{\eta}_0) + \xi \matr{I})^{-1} \matr{g}_{\matr{\eta}_0}$, in the update for $\matr{\mu}_{t+1} \leftarrow \matr{\mu}_t - \alpha (\matr{S} + \xi \matr{I})^{-1} \matr{g}_t$, or $\matr{\mu}_{t+1} \leftarrow \matr{\mu}_t - \alpha (\matr{B} + \sqrt{\xi} \matr{I})^{-T} (\matr{B} + \sqrt{\xi} \matr{I})^{-1} \matr{g}_t$

			\begin{algorithm}[!htbp]
				\DontPrintSemicolon
				\KwInput{$\text{Decay Constant } \omega \in (0, 1)$}

				\For{$t = 0, 1, \dots$}{
					Calculate $\rho_t$ as in Eqn. (\ref{eqn:TrustRegionApproximationQuality}).

					\uIf {$\rho_t < \frac{1}{4}$}{
						$\xi \leftarrow \frac{1}{\omega} \xi $
					}\uElseIf {$\rho_t > \frac{3}{4}$}{
						$\xi \leftarrow \omega \xi $
					}

					Update the parameter $\matr{w}_{t+1} \leftarrow \matr{w}_t - (\matr{H}_t + \xi \matr{I})^{-1} \matr{g}_t$
				}
				\caption{Trust Region like Adaptive Damping \cite[Alg.~4.1]{WN99}, \cite{MG15}}
				\label{alg:AdaptiveDamping}
			\end{algorithm}
		\subsection{Momentum}
			\parencite{P64}
		\subsection{Implicitly Defined Structure}
			\subsubsection{Low-Rank Structure}
				In this section, we will investigate a low-rank structure on the covariance $\matr{\Sigma}$ for the variational posterior $q(\matr{z} | \matr{\mu}, \matr{\Sigma}) = \Normal(\matr{\mu}, \matr{\Sigma})$, i.e., 
				\begin{equation*}
					\matr{\Sigma} = \matr{U}_k \matr{U}_k^T + \Diag(\matr{\sigma}^2),
				\end{equation*}
				where $\matr{U}_k \in \Real^{d \times k}, k \in \{0, \dots, d\}$ a matrix of rank $k$ and $\matr{\sigma} \in \Real^d$.
				The special case $k=0$ corresponds to a diagonal covariance matrix $\matr{\Sigma} = \Diag (\matr{\sigma}^2)$.
				However, when we optimize the ELBO with parametrization $\matr{\lambda} = \{\matr{\mu}, \matr{\alpha}\}$ with $\matr{\alpha} = \{\matr{U}_k, \matr{\sigma}\}$, this can lead to a non-invertible Fisher matrix \parencite[Sec. J.1.6]{LNK+21}. 
				Thus, we will instead follow the work of \parencite{LNK+21} who proposed a parametrization $\matr{\lambda} = \{\matr{\mu}, \matr{B}\}$ such that $\matr{S} = \matr{B} \matr{B}^T$, where $\matr{B}$ is from the set of block upper triangular matrices
				\begin{equation}
					\set{B}_{\text{up}}(k) = \left\{\begin{pmatrix} \matr{B}_A & \matr{B}_B \\
											 	\matr{0}  & \matr{B}_D
										     \end{pmatrix} \mid \matr{B}_A \in \text{GL}^{k \times k}, \matr{B}_D \in \set{D}_+^{d-k}\right\}.
					\label{eqn:B_up}
				\end{equation}

				% Upper block-triangular
				\begin{figure}[H]
					\centering
					\begin{tikzpicture}[scale=0.5]
						\block[white](0,0) (2,5)
						\block[white](2,0) (5,5)

						\block[red](0,6) (1,1)
						\block[red](0,5) (1,1)
						\block[red](1,6) (1,1)
						\block[red](1,5) (1,1)

						\block[blue](2,6) (1,1)
						\block[blue](2,5) (1,1)
						\block[blue](3,6) (1,1)
						\block[blue](3,5) (1,1)
						\block[blue](4,6) (1,1)
						\block[blue](4,5) (1,1)
						\block[blue](5,6) (1,1)
						\block[blue](5,5) (1,1)
						\block[blue](6,6) (1,1)
						\block[blue](6,5) (1,1)

						\block[gray](2,4) (1,1)
						\block[gray](3,3) (1,1)
						\block[gray](4,2) (1,1)
						\block[gray](5,1) (1,1)
						\block[gray](6,0) (1,1)
					\end{tikzpicture}
					\caption{Upper triangular Matrix Group $\set{B}_{\text{up}}(k)$}
				\end{figure}

				This, in turn induces a low-rank structure on the covariance $\matr{\Sigma}$, as 
				$$\matr{\Sigma} = \matr{S}^{-1} = (\matr{B} \matr{B}^T)^{-1} = \matr{U}_k \matr{U}_k^T + \begin{pmatrix} \matr{0}_k & \matr{0} \\ \matr{0} & \matr{B}_D^{-2}\end{pmatrix},$$
				where 
				$$\matr{U}_k = \begin{pmatrix} -\matr{B}_A^{-T} \\ \matr{B}_D^{-1} \matr{B}_B^T \matr{B}_A^{-T}\end{pmatrix} \in \Real^{d \times k}$$
				a matrix of rank $k$ and a block arrowhead structure on $\matr{S}$:
				$$\matr{S} = \matr{B} \matr{B}^T = \begin{pmatrix} \matr{B}_A \matr{B}_A^T + \matr{B}_B \matr{B}_B^T & \matr{B}_B \matr{B}_D \\ \matr{B}_D \matr{B}_B^T & \matr{B}_D^2 \end{pmatrix}.$$
				Note that the dimensionality of $\set{B}_{\text{up}}(k)$ is
				\begin{align*}
					\dim(\set{B}_{\text{up}}(k)) &= \dim (\matr{B}_A) + \dim (\matr{B}_B) + \dim (\matr{B}_D) \\
					&= k^2 + k (d - k) + (d - k) = d + k (d - 1) \in \{d, \dots, d^2\}
				\end{align*}
				for $k \in \{0, \dots, d\}$ such that this approach interpolates between a diagonal approximation to the covariance matrix $\matr{\Sigma} = \Diag(\matr{\sigma}^2)$ when $k = 0$ and the full covariance matrix when $k = d$.
				Note that $\matr{B}_D$ is a diagonal matrix.
				That is, we can linearly interpolate from linear to quadratic storage cost via the rank parameter $k$,
				\begin{equation*}
					\dim(\set{B}_{\text{up}}(k)) = (k + 1) d - k = \BigO ((k + 1) d).
				\end{equation*}
				Their method uses two maps $\matr{\psi}$ and $\matr{\phi}$ which map from a local space to an auxiliary and a global space where the global space has some structure on the parameters $\matr{\lambda}$ and a possibly non-invertible Fisher matrix and the FIM in the local space invertible. 
				$$\matr{\lambda}_{t+1} = \matr{\phi}_{\matr{\lambda}_t}(-\alpha \matr{\hat{g}}_{\matr{\eta}_0}^{(t)}), \quad \matr{\tau}_{t+1} = \matr{\psi} (\matr{\lambda}_{t+1}),$$
				where $\matr{\eta}_0 = \matr{0}$.

				\parencite{LSK20} propose adding an additional term to the Bayesian Learning Rule (BLR) to ensure positive definite constraints which involves the Christoffel symbol. 
				In doing so, they adapt their new ``improved Bayesian Learning Rule'' (iBLR) which ensures the updates remain in the constraint set without performing a costly line search at each step.
				\begin{align*}
					\lambda^{c_i} \leftarrow \lambda^{c_i} - \alpha \hat{g}^{c_i} \color{red} - \frac{\alpha^2}{2} \sum_{a_i, b_i} \Gamma_{a_i, b_i}^{c_i} \hat{g}^{a_i} \hat{g}^{b_i} \color{black}
				\end{align*}
				Specifically, for a Gaussian posterior with precision matrix $\matr{S}$, i.e., $\matr{\lambda} = \{\matr{\mu}, \matr{S}\}$, this update rule becomes
				\begin{align*}
					\matr{\mu} &\leftarrow \matr{\mu} - \alpha \matr{S}^{-1} \Expect_q[\grad_z \bar{l}(\matr{z})] \color{red} + 0 \color{black}\\
					\matr{S} &\leftarrow (1 - \alpha) \matr{S} + \alpha \Expect_q[\grad_z^2 \bar{l}(\matr{z})] \color{red} + \frac{\alpha^2}{2} \matr{G} \matr{S}^{-1} \matr{G} \color{black},
				\end{align*}
				where $\matr{G} := \matr{S} - \Expect_q[\grad_z^2 \bar{l}(\matr{z})]$.

				Using a multivariate Gaussian with diagonal precision, i.e., $\matr{\lambda} = \{\matr{\mu}, \matr{s}\}$, with $\matr{\Sigma} = \Diag (\matr{s}^{-1})$, \parencite{LSK20} added a natural momentum term to the mean parameter $\matr{\mu}$ which gives an Adam-like optimizer as follows
				\begin{align*}
					\matr{\mu} &\leftarrow \matr{\mu} - \alpha \frac{1 - \beta_2^t}{1 - \beta_1^t} \matr{m} / \matr{s} \\
					\matr{s} &\leftarrow \matr{s} + (1 - \beta_2) \matr{g_s} \color{red} + \frac{1}{2} (1 - \beta_2)^2 \matr{g_s} \odot \matr{s}^{-1} \odot \matr{g_s} \color{black}
				\end{align*}
				The full method is summarized in Algorithm \ref{alg:iBayesLRule}.

				\begin{algorithm}[!htbp]
					\DontPrintSemicolon
					\KwInput{$\text{Initial parameters } \matr{\mu}_0, \matr{s}_0$}
					\myinput{$\text{Learning rate } \alpha > 0, \text{ prior precision } \eta > 0$}
					\myinput{$\text{Regularization } \gamma \ge 0, \text{ damping } \xi \ge 0, \text{ Number of MC samples } M \ge 1$}
					\myinput{$\beta_1, \beta_2 \in (0, 1]$}
					% \KwOutput{}

					Set $\matr{m}_0 = 0$

					\For{$t = 0, 1, \dots$}{
						$\matr{z}_m = \matr{\mu_t} + (N s_t)^{-1/2} \odot \varepsilon_m \text{ , where }\matr{\varepsilon}_m \sim \Normal(\matr{0}, \matr{I}_{d\times d}), \quad \forall m \in \{1, \dots, M\}$

						Sample a minibatch $\set{M}$ from the training set.

						Compute sampled mini-batch gradients $\matr{\bar{g}}_m = \frac{1}{|\set{M}|} \sum_{i \in \set{M}} \grad \ell_i (\matr{z}_m) \quad \forall m \in \{1, \dots, M\}$.

						$\matr{g_{\mu}} = \frac{\eta}{N} \matr{\mu}_t + \frac{1}{M} \sum_{m=1}^M \matr{\bar{g}}_m$

						$\matr{m}_{t+1} = \beta_1 \matr{m}_t + (1 - \beta_1) \matr{g_{\mu}}$

						Debias the update and add damping to the precision vector
						$\matr{\bar{m}} = \matr{m}_{t+1} / (1 - \beta_1^t), \quad \matr{\bar{s}} = \matr{s}_{t+1} / (1 - \beta_2^t) + \xi$

						$\matr{\hat{m}} = \matr{\bar{m}} / \matr{\bar{s}}$

						$\matr{g_s} = \frac{\gamma \eta}{N} - \gamma \matr{s}_t + \frac{1}{M} \sum_{m=1}^M \left[ (N \matr{s}_t) \odot (\matr{z}_m - \matr{\mu}_t) \right] \odot \matr{\bar{g}}_m$

						$\matr{\mu}_{t+1} = \matr{\mu}_t - \alpha \matr{\hat{m}}$

						$\matr{s}_{t+1} = \matr{s}_t + (1 - \beta_2) \matr{g_s} \color{red} + \frac{1}{2} (1 - \beta_2)^2 \matr{g_s} \odot \matr{s}_t^{-1} \odot \matr{g_s} \color{black}$
						}
					\caption{Adam-like Optimizer using the iBayesLRule \parencite[Fig.~1]{LSK20}}
					\label{alg:iBayesLRule}
				\end{algorithm}

				They make the following assumptions on their parametrization and mapping.
				\begin{assumption}
					\label{assumption:FIM}
					The Fisher matrix $\matr{F}_{\matr{\eta}} (\matr{\eta}_0)$ is invertible.
				\end{assumption}

				\begin{assumption}
					\label{assumption:Diffeomorphism}
					The map $\matr{\eta} \mapsto \matr{\psi} (\matr{\phi}_{\matr{\lambda}_t} (\matr{\eta}))$ is locally $C^1$-diffeomorphic in an open neighborhood of $\matr{\eta}_0$ for all $\matr{\lambda}_t \in \Omega_{\lambda}$.
				\end{assumption}
				However, their method cannot impose special structures beyond a diagonal covariance matrix as easily since this can make the FIM non-invertible and thus computing a Natural Gradient step impossible. 
				We will thus attempt to combine both works and provide an Adam-like optimizer by adding a momentum term in the auxiliary space to the work of \parencite{LNK+21}:
				$$\matr{\lambda}_{t+1} \leftarrow \matr{\phi}_{\matr{\lambda}_t}(-\alpha \matr{\hat{g}}_{\matr{\eta}_0}^{(t)}), \quad \matr{m}_{t+1} \leftarrow (1-\beta) \matr{m}_t + \beta \matr{\lambda}_{t+1}, \quad \matr{\tau}_{t+1} \leftarrow \matr{\psi} (\matr{m}_{t+1}).$$
				One could also consider adding a momentum term in the global space:
				$$\matr{\lambda}_{t+1} \leftarrow \matr{\phi}_{\matr{\lambda}_t}(-\alpha \matr{\hat{g}}_{\matr{\eta}_0}^{(t)}), \quad \matr{\tau}_{t+1} \leftarrow (1-\beta) \matr{\tau}_t + \beta \matr{\psi}(\matr{\lambda}_{t+1}).$$

				For completeness, we present the derivation for the Adam-like optimizer using momentum in the auxiliary or global space, respectively, on the mean parameter $\matr{\mu}$ \parencite[Sec. E.3]{LSK20}. 
				Remember that we would like to minimize the function given in Eqn. (\ref{eqn:ELBO}):
				\begin{equation}
					\min_{\matr{\mu}, \matr{B}} \ELBO(\matr{\mu}, \matr{B}) = \Expect_{q(\matr{z} | \matr{\mu}, \matr{B})} \left[ \left( \sum_{i=1}^N \ell_i(z) \right) - \gamma \log \Normal(\matr{z} | \matr{0}, \eta^{-1} \matr{I}) + \gamma \log q(\matr{z} | \matr{\mu}, \matr{B})\right],
					\label{eqn:ELBO_Objective}
				\end{equation}
				where $q(\matr{z} | \matr{\mu}, \matr{B}) = \Normal(\matr{z} | \matr{\mu}, (\matr{B} \matr{B}^T)^{-1})$ since we parametrize $\matr{S} = \matr{B} \matr{B}^T$.
				Using the reparametrization trick and an MC sample, we can approximate the gradients
				\begin{align*}
					\grad_{\matr{\mu}} \ELBO(\matr{\mu}, \matr{B}) &= \sum_{i=1}^N \grad_{\matr{\mu}}\Expect_q[\ell_i (\matr{z})] + \eta \matr{\mu} \approx \frac{1}{M} \sum_{m=1}^M \sum_{i=1}^N \grad_{\matr{z}} \ell_i (\matr{z}_m) + \gamma \eta \matr{\mu} \\
					\grad_{\matr{\Sigma}} \ELBO(\matr{\mu}, \matr{B}) &= \sum_{i=1}^N \grad_{\matr{\Sigma}} \Expect_q[\ell_i (\matr{z})] + \gamma \left( \frac{\eta}{2} \matr{I} - \frac{1}{2} \matr{S} \right) \\
					&= \frac{1}{2} \left( \sum_{i=1}^N \Expect_q[\grad_{\matr{\Sigma}}^2 \ell_i (\matr{z})] + \gamma \eta \matr{I} - \gamma \matr{S} \right) \\
					&\approx \frac{1}{2} \left( \sum_{i=1}^N \frac{1}{M} \sum_{m=1}^M (\matr{K}_i(\matr{z_m}) + \matr{K}_i(\matr{z_m})^T) + \gamma \eta \matr{I} - \gamma \matr{S} \right) \\
					&= \frac{N}{2} \left( \frac{1}{2 M} \sum_{m=1}^M (\matr{K}(\matr{z_m}) + \matr{K}(\matr{z_m})^T) + \frac{\gamma \eta}{N} \matr{I} - \frac{\gamma}{N} \matr{S} \right),
				\end{align*}
				where 
				\begin{align*}
					\matr{K}_i(\matr{z}_m) &:= \matr{S} (\matr{z}_m - \matr{\mu}) (\grad_{\matr{z}} \ell_i (\matr{z}_m) )^T \\
					\matr{K}(\matr{z}_m) &:= \frac{1}{N} \sum_{i=1}^N \matr{K}_i(\matr{z}_m) = \matr{S} (\matr{z}_m - \matr{\mu}) (\grad_{\matr{z}} \bar{\ell} (\matr{z}_m) )^T,
				\end{align*}
				and $\matr{z}_m \sim q(\matr{z} | \matr{\mu}, \matr{B}) = \Normal(\matr{z} | \matr{\mu}, (\matr{B} \matr{B}^T)^{-1})$ for $M$ MC samples, $m \in \{1, \dots, M\}$.
				\parencite{KNT+18, LSK20} propose adding a natural momentum term to the mean parameter
				\begin{align*}
					\matr{\mu}_{t+1} &= \matr{\mu}_t - \alpha_1 \matr{S}_t^{-1} \grad_{\matr{\mu}} \ELBO (\matr{\lambda}_t) + \alpha_2 \matr{S}_t^{-1} \matr{S}_{t-1} (\matr{\mu}_t - \matr{\mu}_{t-1}) \\
					&= \matr{\mu}_t - \alpha_1 \matr{B}_t^{-T} \matr{B}_t^{-1} \grad_{\matr{\mu}} \ELBO (\matr{\lambda}_t) + \alpha_2 \matr{B}_t^{-T} \matr{B}_t^{-1} \matr{B}_{t-1} \matr{B}_{t-1}^T (\matr{\mu}_t - \matr{\mu}_{t-1}).
				\end{align*}
				% Full derivation better here!
				Now, setting $\matr{B}_{t+1} = \sqrt{N} \matr{\hat{B}}_{t+1}, \matr{\hat{S}}_{t+1} = \matr{\hat{B}}_{t+1} \matr{\hat{B}}_{t+1}^T$, one can directly follow the derivation of \parencite[Sec. E.3]{LSK20} to arrive at the method described in Algorithm \ref{alg:RC} for low-rank structure on the covariance.

				While taking the inverse of the block triangular matrix $\matr{B} \in \set{B}_{\text{up}}(k)$ can be computed easily, when updating the mean parameter with the inverse precision matrix $\matr{S}^{-1} = (\matr{B} \matr{B}^T)^{-1}$, this can lead to numerical instabilities when $\matr{S}$ is ill-conditioned and nearly singular. 
				Thus, in the literature, one often adds a damping term $\xi \matr{I}$ to the matrix where $\xi > 0$ and then performs inversion similar to a Levenberg-Marquardt update \parencite{L44, M63}.
				However, $\matr{S}$ is a dense matrix so computing its inverse is not feasible in practice computationally.
				Thus, we can follow the approach in \parencite[Sec. 6.3]{MG15} and approximate it as 
				\begin{align*}
					\matr{S} + \xi \matr{I} &= \matr{B} \matr{B}^T + \xi \matr{I} \\
					&\approx (\matr{B} + \pi \sqrt{\xi} \matr{I}) (\matr{B} + \frac{1}{\pi} \sqrt{\xi} \matr{I})^T \\
					&= (\matr{S} + \xi \matr{I}) + \sqrt{\xi} \left(\pi \matr{B}^T + \frac{1}{\pi} \matr{B} \right),
				\end{align*}
				where $\pi$ is a scalar chosen such that the difference $\sqrt{\xi}(\pi \matr{B}^T + \frac{1}{\pi} \matr{B})$ is minimized which it is for $\pi = 1$.
				Thus, we can approximate
				\begin{equation*}
					(\matr{S} + \xi \matr{I})^{-1} \approx (\matr{B} + \sqrt{\xi} \matr{I})^{-t} (\matr{B} + \sqrt{\xi} \matr{I})^{-1},
				\end{equation*}
				where the inversion on the right hand side involves a block triangular matrix which can be done very efficiently. 
				We can perform a natural gradient step to update $\matr{B}$
				\begin{equation}
					\matr{B} \leftarrow \matr{B} \matr{h}((1 - \beta_2) \matr{C_{up}} \odot \kappa_{up}(\matr{B}^{-1} \matr{G_{\Sigma}} \matr{B}^{-T})),
				\end{equation}
				where
				\begin{align}
					\matr{h}(\matr{M}) &:= \matr{I} + \matr{M} + \frac{1}{2} \matr{M}^2 \label{eqn:h_func} \\
					\matr{C_{up}} &:= \begin{pmatrix} \frac{1}{2} \matr{J_A} & \matr{J_B} \\ \matr{0} & \frac{1}{2} \matr{J_D} \end{pmatrix} \label{eqn:C_up} \\
					\kappa_{up} \begin{pmatrix} \matr{M_A} & \matr{M_B} \\ \matr{M_C} & \matr{M_D} \end{pmatrix} &:= \begin{pmatrix} \matr{M_A} & \matr{M_B} \\ \matr{0} & \diag(\matr{M_D}) \end{pmatrix}
					\label{eqn:kappa_up}
				\end{align}

				\begin{algorithm}[!htbp]
					\DontPrintSemicolon
					% \KwInput{$\text{Initial parameters } \matr{\mu}, \matr{B}$}
					\KwInput{$\text{Learning rate } \alpha > 0, \text{ rank } k \in \{0, \dots, d\}, \text{ prior precision } \eta > 0$}
					\myinput{$\text{Regularization } \gamma \ge 0, \text{ damping } \xi \ge 0, \text{ Number of MC samples } M \ge 1$}
					\myinput{$\beta_1, \beta_2 \in (0, 1]$}
					Set $\matr{m} \leftarrow \matr{0}, \matr{\mu} \leftarrow \matr{0}, \matr{\hat{B}} \leftarrow \sqrt{\frac{\gamma \eta}{N} + \xi} \matr{I} \quad \color{red} \text{(1)} \color{black}$

					\For{$t = 0, 1, 2, \dots $}{
						\For{$\ell = 1, 2, \dots, L$}{
							Randomly sample a minibatch $\set{M}$ from the training set

							Draw $M$ MC samples from the posterior $q(\matr{z} | \matr{\lambda}_{\ell}) = \Normal(\matr{z} | \matr{\mu}_{\ell}, N (\matr{\hat{B}}_{\ell} \matr{\hat{B}}_{\ell}^T)^{-1})$
							$$\matr{z}_m \leftarrow \matr{\mu}_{\ell} + N^{-1/2} \matr{\hat{B}}_{\ell}^{-T} \matr{\varepsilon}_m, \quad \text{ where } \matr{\varepsilon}_m \sim \Normal(\matr{0}, \matr{I}_{d\times d}), \quad \forall m \in \{1, \dots, M\}$$
							% $\matr{z} \leftarrow \matr{\mu} + N^{-1/2} \left[\matr{\hat{U_k}} \matr{\varepsilon}_{rank} + \diag{\matr{\hat{B}}_D^{-1}} \otimes \matr{\varepsilon}_{diag}\right]$, where $\matr{\varepsilon}_{rank} \sim \Normal(\matr{0}, \matr{I}_{k\times k}), \matr{\varepsilon}_{diag} \sim \mathcal{N}(\matr{0}, \matr{I}_{(d-k) \times (d-k)})$

							Sample $M$ gradient samples using $\matr{z}_m$
							$$\matr{\bar{g}}_m \leftarrow \frac{1}{|\set{M}|} \sum_{i \in \set{M}} \grad_z \ell_i (\matr{z}_m), \quad \forall m \in \{1, \dots, M\}$$

							Compute the gradient for the mean parameter $\matr{\mu}_{\ell}$
							$$\matr{g}_{\matr{\mu}} \leftarrow \frac{\gamma \eta}{N} \matr{\mu}_{\ell} + \frac{1}{M} \sum_{m=1}^M \matr{\bar{g}}_m $$

							Update the momentum term
							$$\matr{m}_{\ell} \leftarrow \beta_1 \matr{m}_{\ell} + (1 - \beta_1) \matr{g}_{\matr{\mu}} $$

							Debias momentum and precision
							$$\matr{\bar{m}}_{\ell} \leftarrow \frac{\matr{m}_{\ell}}{1 - \beta_1^{t + 1}}, \matr{\bar{B}}_{\ell} = \frac{\matr{\hat{B}}_{\ell} - \sqrt{\frac{\gamma \eta}{N} + \xi} \matr{I}}{\sqrt{1 - \beta_2^t + \varepsilon}} + \sqrt{\frac{\gamma \eta}{N} + \xi} \matr{I} $$

							Compute the gradient with respect to $\matr{\Sigma}_{\ell}$
							$$\matr{G_{\Sigma}} \leftarrow \frac{\gamma \eta}{N} \matr{I} - \gamma\matr{\hat{B}} \matr{\hat{B}}^T + \frac{N}{2M} \sum_{m=1}^M \left( \matr{K}(\matr{z}_m) + \matr{K}(\matr{z}_m)^T \right), \color{red} \text{(6)} \color{black}$$
							where $\matr{K}(\matr{z}_m) = \matr{\hat{B}} \matr{\hat{B}}^T (\matr{z}_m - \matr{\mu}) \matr{\bar{g}}_m^T$

							Update mean parameter scaled by dampened precision estimate
							$$\matr{\mu}_{\ell} \leftarrow \matr{\mu}_{\ell} - \alpha \matr{\bar{B}}^{-T} \matr{\bar{B}}^{-1} \matr{\bar{m}} $$

							Update square root precision
							$$\matr{\hat{B}}_{\ell} \leftarrow \matr{\hat{B}}_{\ell} \matr{h}((1 - \beta_2) \matr{C_{up}} \odot \kappa_{up}(\matr{\hat{B}}_{\ell}^{-1} \matr{G_{\Sigma}} \matr{\hat{B}}_{\ell}^{-T})),$$
							where $\matr{h}, \matr{C}_{\text{up}}, \kappa_{up} \text{ given in (\ref{eqn:h_func}), (\ref{eqn:C_up}), \ref{eqn:kappa_up}}$
						}
					}
					\caption{Rank-$k$ Update Rule using Momentum in the Auxiliary/Global Space}
					\label{alg:RC}
				\end{algorithm}

				We can impose a block-diagonal covariance structure according to a neural network architecture by instead using the block-diagonal group
				\begin{equation*}
					\set{B}_{\text{diag-up}}(k) = \left\{ \Diag(\matr{B}_1, \dots, \matr{B}_L) \mid \matr{B}_{\ell} \in \set{B}_{\text{up}} (k), \quad \forall \ell \in \{1, \dots, L\} \right\}.
				\end{equation*}
				as the proofs in \parencite{LNK+21} are identical except for indices with respect to the diagonal blocks. 

				Note that this algorithm is an extension to the Adam-like optimizer of \parencite{LSK20} in the diagonal case, $k = 0$.
				We can set $\matr{b} = \diag(\matr{B}) \in \Real^d$ and note that $\matr{c_{up}} = \frac{1}{2} \matr{\mathbbm{1}} \in \Real^d, \kappa_{up}(2\matr{b}^{-1} \odot \matr{g_s} \odot \matr{b}^{-1})) = 2 \matr{b}^{-1} \odot \matr{g_s} \odot \matr{b}^{-1}$.
				\begin{align*}
					\matr{s}_{t+1} & = (\matr{b}_{t+1})^2 = (\matr{b}_t \odot \matr{h}(\beta \matr{c_{up}} \odot \kappa_{up} (2 \matr{b}_t^{-1} \odot \matr{g_s} \odot \matr{b}_t^{-1})))^2 \\
					&= (\matr{b}_t \odot \matr{h}(\beta \matr{b}_t^{-2} \odot \matr{g_s}))^2 \\
					&= (\matr{b}_t + \beta \matr{b}_t^{-1} \odot \matr{g_s} + \frac{1}{2} \beta^2 \matr{b}_t^{-3} \odot \matr{g_s}^2)^2 \\
					&= \matr{b}_t^2 + 2\beta \matr{g_s} + 2 \beta^2 \matr{b}_t^{-2} \odot \matr{g_s}^2 + \beta^3 \matr{b}_t^{-4} \odot \matr{g_s}^3 + \frac{1}{4} \beta^4 \matr{b}_t^{-6} \odot \matr{g_s}^4 \\
					&= \matr{s}_t + \tilde{\beta} \matr{g_s} \color{red} + \frac{1}{2} \tilde{\beta}^2 \matr{s}_t^{-1} \odot \matr{g_s}^2 \color{blue} + \frac{1}{8} \tilde{\beta}^3 \matr{s}_t^{-2} \odot \matr{g_s}^3 + \frac{1}{64} \tilde{\beta}^4 \matr{s}_t^{-3} \odot \matr{g_s}^4 \color{black},
				\end{align*}
				where $\tilde{\beta} = 2 \beta$ and $\matr{s}_t = \matr{b}_t^2$.
				This is thus an extension of the Adam-like optimizer from a quadratic update to a polynomial of order $4$ in terms of the gradient vector $\matr{g_s}$, i.e., fourth-order information is being used to update $\matr{s}$. 
				Note, that we do not explicitly compute this update in terms of $\matr{s}$ but implicitly since we update its square root, namely $\matr{b}$ using only second-order information!

			% \subsubsection{Implementation Details}
				Note that in Algorithm \ref{alg:RC}, the term $\matr{B}^{-1} \matr{G_{\Sigma}} \matr{B}^{-1}$ is generally a dense matrix that is passed as an argument to $\kappa_{up}$ which then zeroes out the entries in the bottom left block as well as the off-diagonals in the bottom right block:
				$$\kappa_{up}(X) = \kappa_{up} \begin{pmatrix} \matr{A} & \matr{B} \\ \matr{C} & \matr{D} \end{pmatrix} = \begin{pmatrix} \matr{A} & \matr{B} \\ \matr{0} & \Diag(\diag (\matr{D}))\end{pmatrix},$$
				where $\diag$ returns a vector of the diagonal elements of its arguments and $\Diag$ returns a diagonal matrix.
				Since most of the entries in the bottom right block will be discarded (usually in practice, we will set $k \ll d$ such that the bottom block will hold the majority of entries, namely $(d - k)^2$), we need to find an efficient way of computing $\kappa_{up}(\matr{B}^{-1} \matr{G_{\Sigma}} \matr{B}^{-T})$.
				\begin{lemma}
					For a multivariate Gaussian posterior $q(\matr{z} | \matr{\lambda}) = \Normal(\matr{z} | \matr{\mu}, \matr{S}^{-1})$, $M$ MC samples $\matr{z} \sim q(\matr{z} | \matr{\lambda})$, and prior $p(z) = \Normal(\matr{z} | \matr{0}, \eta^{-1} \matr{I}), \eta > 0$, with parametrization $\matr{S} = \matr{B} \matr{B}^T, \matr{B} \in \set{B}_{\text{up}}(k)$, we have
					\begin{align*}
						\kappa_{up}(\matr{B}^{-1} \matr{G_{\Sigma}} \matr{B}^{-T}) &= \frac{\gamma \eta}{N}\begin{pmatrix} \matr{B}_A^{-T} \matr{B}_A^{-1} & -\matr{B}_A^{-T} \matr{B}_A^{-1} \matr{B}_B \matr{B}_D^{-1} \\ \matr{0} & \diag(\matr{B}_D^{-2}) \odot (1 + \sum_{j=1}^k ((\matr{B}_A^{-1} \matr{B}_B) \odot (\matr{B}_A^{-1} \matr{B}_B))_{ji})_{i \in [d-k]} \end{pmatrix} \\
						&\quad - \gamma \matr{I} \\
						&\quad + \frac{N}{2 M} \sum_{m=1}^M \begin{pmatrix} \matr{M}_{A, m} + \matr{M}_{A, m}^T & \matr{M}_{B, m} + \matr{M}_{C, m}^T \\ \matr{0} & 2\matr{M}_{D, m} \end{pmatrix},
					\end{align*}
					where
					% Definition for notation z_{:k}, z_{k:} beforehand!
					\begin{align*}
						\matr{M}_{A, m} &:= \matr{B}_A^T (\matr{z}_{m, :k} - \matr{\mu}) (\matr{\bar{g}}_{m, :k}^T \matr{B}_A^{-T} - \matr{\bar{g}}_{m, k:}^T \matr{B}_D^{-1} \matr{B}_B^T \matr{B}_A^{-1} ) \\
						\matr{M}_{B, m} &:= \matr{B}_A^T (\matr{z}_{m, :k} - \matr{\mu}) \matr{\bar{g}}_{m, k:}^T \matr{B}_D^{-1} \\
						\matr{M}_{C, m} &:= (\matr{B}_B^T \matr{v}_{m, :k} + \matr{B}_D \matr{v}_{m, k:}) (\matr{\bar{g}}_{m, :k}^T \matr{B}_A^{-T} - \matr{\bar{g}}_{m, k:}^T \matr{B}_D^{-1} \matr{B}_B^T \matr{B}_A^{-T}) \\
						\matr{M}_{D, m} &:= (\matr{z}_{m, k:} - \matr{\mu} + \diag(\matr{B}_D^{-1}) \odot (\matr{B}_B^T (\matr{z}_{m, :k} - \matr{\mu}))) \odot \matr{\bar{g}}_{m, k:}
					\end{align*}
					\label{lemma:kappa_up}
				\end{lemma}

				\begin{proof}
					Note that
					$$\matr{G_{\Sigma}} = \frac{\gamma \eta}{N} \matr{I} - \gamma \matr{S} + \frac{N}{2 M} \sum_{i=1}^M (\matr{K}(\matr{z}_m) + \matr{K}(\matr{z}_m)^T),$$
					where $\matr{K}(\matr{z}_m) := \matr{S} (\matr{z}_m - \matr{\mu}) \matr{\bar{g}}_m^T$.
					Denote $\matr{v}_m := \matr{z}_m - \matr{\mu}$.
					With parametrization $\matr{S} = \matr{B} \matr{B}^T$, we have
					% Add lemma for block arrowhead matrix Sigma?
					\begin{align*}
						\matr{B}^{-1} \matr{G_{\Sigma}} \matr{B}^{-T} &= \frac{\gamma \eta}{N} \matr{B}^{-T} \matr{B}^{-1} - \gamma \matr{I} + \frac{N}{2 M} \sum_{m=1}^M (\matr{B}^{-1} \matr{K}(\matr{z}_m) \matr{B}^{-T} + \matr{B}^{-1} \matr{K}(\matr{z}_m)^T \matr{B}^{-T}) \\
						&= \frac{\gamma \eta}{N} \begin{pmatrix} \matr{B}_A^{-T} \matr{B}_A^{-1} & -\matr{B}_A^{-T} \matr{B}_A^{-1} \matr{B}_B \matr{B}_D^{-1} \\ * & \matr{B}_D^{-1} \matr{B}_B^T \matr{B}_A^{-T} \matr{B}_A^{-1} \matr{B}_B \matr{B}_D^{-1} + \matr{B}_D^{-2} \end{pmatrix} - \gamma \matr{I} \\
						&\quad + \frac{N}{2 M} \sum_{m=1}^M (\matr{B}^{-1} \matr{K}(\matr{z}_m) \matr{B}^{-T} + (\matr{B}^{-1} \matr{K}(\matr{z}_m) \matr{B}^{-T})^T) \numberthis{\label{eqn:RankMatrix}}
					\end{align*}
					Now, for the last term (\ref{eqn:RankMatrix}), we can decompose the vectors
					$$\matr{v}_m = (\underbrace{v_{m, 1}, \dots, v_{m, k}}_{=: \matr{v}_{m, :k}}, \underbrace{v_{m, k+1}, \dots, v_{m, d}}_{=:\matr{v}_{m, k:}})^T, \text{ and } \matr{\bar{g}}_{m} = (\underbrace{\bar{g}_{m, k}, \dots, \bar{g}_{m, k}}_{=: \matr{\bar{g}}_{m, :k}}, \underbrace{\bar{g}_{m, k+1}, \dots, \bar{g}_{m, d}}_{=:\matr{\bar{g}}_{m, k:}})^T$$
					and write
					\begin{align*}
						\matr{B}^{-1} \matr{K}(\matr{z}_m) \matr{B}^{-T} &= \matr{B}^T \matr{v}_m \matr{\bar{g}}_m^T \matr{B}^{-T} \\
						&= \begin{pmatrix} \matr{B}_A^T & \matr{0} \\ \matr{B}_B^T & \matr{B}_D \end{pmatrix} \begin{pmatrix} \matr{v}_{m, :k} \\ \matr{v}_{m, k:} \end{pmatrix} \begin{pmatrix} \matr{\bar{g}}_{m, :k}^T, & \matr{\bar{g}}_{m, k:}^T \end{pmatrix} \begin{pmatrix} \matr{B}_A^{-T} & \matr{0} \\ -\matr{B}_D^{-1} \matr{B}_B^T \matr{B}_A^{-T} & \matr{B}_D^{-1} \end{pmatrix} \\
						&= \begin{pmatrix} \matr{B}_A^T \matr{v}_{m, :k} \\ \matr{B}_B^T \matr{v}_{m, :k} + \matr{B}_D \matr{v}_{m, k:} \end{pmatrix} \begin{pmatrix} \matr{\bar{g}}_{m, :k}^T \matr{B}_A^{-T} - \matr{\bar{g}}_{m, k:}^T \matr{B}_D^{-1} \matr{B}_B^T \matr{B}_A^{-T}, & \matr{\bar{g}}_{m, k:}^T \matr{B}_D^{-1} \end{pmatrix} \\
						&= \begin{pmatrix} \matr{\tilde{M}}_{A, m} & \matr{\tilde{M}}_{B, m} \\ \matr{\tilde{M}}_{C, m} & \matr{\tilde{M}}_{D, m} \end{pmatrix},
					\end{align*}
					where
					\begin{align*}
						\matr{\tilde{M}}_{A, m} &= \matr{B}_A^T \matr{v}_{m, :k} (\matr{\bar{g}}_{m, :k}^T \matr{B}_A^{-T} - \matr{\bar{g}}_{m, k:}^T \matr{B}_D^{-1} \matr{B}_B^T \matr{B}_A^{-1}) \\
						\matr{\tilde{M}}_{B, m} &= \matr{B}_A^T \matr{v}_{m, :k} \matr{\bar{g}}_{m, k:}^T \matr{B}_D^{-1} \\
						\matr{\tilde{M}}_{C, m} &= (\matr{B}_B^T \matr{v}_{m, :k} + \matr{B}_D \matr{v}_{m, k:}) (\matr{\bar{g}}_{m, :k}^T \matr{B}_A^{-T} - \matr{\bar{g}}_{m, k:}^T \matr{B}_D^{-1} \matr{B}_B^T \matr{B}_A^{-T}) \\
						\matr{\tilde{M}}_{D, m} &= (\matr{B}_B^T \matr{v}_{m, :k} + \matr{B}_D \matr{v}_{m, k:}) \matr{\bar{g}}_{m, k:}^T \matr{B}_D^{-1}.
					\end{align*}
					Combining terms, we arrive at
					\begin{align*}
						\matr{B}^{-1} \matr{G_{\Sigma}} \matr{B}^{-T} &= \frac{\gamma \eta}{N} \begin{pmatrix} \matr{B}_A^{-T} \matr{B}_A^{-1} & -\matr{B}_A^{-T} \matr{B}_A^{-1} \matr{B}_B \matr{B}_D^{-1} \\ * & \matr{B}_D^{-1} \matr{B}_B^T \matr{B}_A^{-T} \matr{B}_A^{-1} \matr{B}_B \matr{B}_D^{-1} + \matr{B}_D^{-2} \end{pmatrix} - \gamma \matr{I} \\
						&\quad + \frac{N}{2 M} \sum_{m=1}^M \begin{pmatrix} \matr{\tilde{M}}_{A, m} + \matr{\tilde{M}}_{A, m}^T & \matr{\tilde{M}}_{B, m} + \matr{\tilde{M}}_{C, m}^T \\ * & \matr{\tilde{M}}_{D, m} + \matr{\tilde{M}}_{D, m}^T \end{pmatrix}.
					\end{align*}
					Note that $\kappa_{up}$ is linear such that
					\begin{align*}
						\kappa_{up}(\matr{B}^{-1} \matr{G_{\Sigma}} \matr{B}^{-T}) &= \frac{\gamma \eta}{N} \kappa_{up} \begin{pmatrix} \matr{B}_A^{-T} \matr{B}_A^{-1} & -\matr{B}_A^{-T} \matr{B}_A^{-1} \matr{B}_B \matr{B}_D^{-1} \\ * & \matr{B}_D^{-1} \matr{B}_B^T \matr{B}_A^{-T} \matr{B}_A^{-1} \matr{B}_B \matr{B}_D^{-1} + \matr{B}_D^{-2} \end{pmatrix} - \gamma \kappa_{up}(\matr{I}) \\
						&\quad + \frac{N}{2 M} \sum_{m=1}^M \kappa_{up} \begin{pmatrix} \matr{\tilde{M}}_{A, m} + \matr{\tilde{M}}_{A, m}^T & \matr{\tilde{M}}_{B, m} + \matr{\tilde{M}}_{C, m}^T \\ * & \matr{\tilde{M}}_{D, m} + \matr{\tilde{M}}_{D, m}^T \end{pmatrix} \\
						&= \frac{\gamma \eta}{N} \begin{pmatrix} \matr{B}_A^{-T} \matr{B}_A^{-1} & -\matr{B}_A^{-T} \matr{B}_A^{-1} \matr{B}_B \matr{B}_D^{-1} \\ \matr{0} & \Diag(\matr{B}_D^{-1} \matr{B}_B^T \matr{B}_A^{-T} \matr{B}_A^{-1} \matr{B}_B \matr{B}_D^{-1} + \matr{B}_D^{-2}) \end{pmatrix} - \gamma \matr{I} \\
						&\quad + \frac{N}{2 M} \sum_{m=1}^M \begin{pmatrix} \matr{\tilde{M}}_{A, m} + \matr{\tilde{M}}_{A, m}^T & \matr{\tilde{M}}_{B, m} + \matr{\tilde{M}}_{C, m}^T \\ \matr{0} & \Diag(\matr{\tilde{M}}_{D, m} + \matr{\tilde{M}}_{D, m}^T) \end{pmatrix}.
					\end{align*}
					For the diagonal terms, note that $\Diag$ is also a linear function and for diagonal matrices $D_1, D_2$, we have
					$$\diag(D_1 X D_2) = \diag(D_1) \odot \diag(X) \odot \diag(D_2).$$
					For non-diagonal matrices, we have
					$$\diag(A B)_i = \sum_j (A \odot B^T)_{ji}$$
					Thus,
					\begin{align*}
						\diag(\matr{B}_D^{-1} \matr{B}_B^T \matr{B}_A^{-T} \matr{B}_A^{-1} \matr{B}_B \matr{B}_D^{-1} + \matr{B}_D^{-2}) &= \diag(\matr{B}_D)^{-2} \odot (1 + \diag((\matr{B}_A^{-1} \matr{B}_B)^T (\matr{B}_A^{-1} \matr{B}_B))_i) \\
						&= \diag(\matr{B}_D)^{-2} \odot \left(1 + \sum_{j=1}^k ((\matr{B}_A^{-1} \matr{B}_B) \odot (\matr{B}_A^{-1} \matr{B}_B))_{ji} \right)
					\end{align*}
					\begin{align*}
						\diag(\matr{\tilde{M}}_{D, m} + \matr{\tilde{M}}_{D, m}^T) &= 2 \diag(\matr{\tilde{M}}_{D, m}) \\
						&= 2 (\diag(\matr{B}_D)^{-1} \odot \diag(\matr{B}_B^T \matr{v}_{m, :k} \matr{\bar{g}}_{m, k:}^T) + \diag(\matr{v}_{m, k:} \matr{\bar{g}}_{m, k:}^T)) \\
						&= 2 (\diag(\matr{B}_D)^{-1} \odot (\matr{B}_B^T \matr{v}_{m, :k}) + \matr{v}_{m, k:}) \odot \matr{\bar{g}}_{m, k:}
					\end{align*}
					which completes the proof.
				\end{proof}

			\subsubsection{Block Arrowhead Structure}
				Similarly to the previous section where we used a block upper triangular square-root parametrization $\matr{B} \in \set{B}_{\text{up}}(k)$ of the precision matrix that induced a low-rank structure on the covariance, one can take 
				\begin{equation*}
					\matr{B} = \begin{pmatrix} \matr{B}_A & \matr{0} \\ \matr{B}_C & \matr{B}_D \end{pmatrix} \in \set{B}_{\text{low}}(k)
				\end{equation*}
				a block lower triangular matrix which induces a block arrowhead structure on the covariance,
				\begin{align*}
					\matr{S} &= \matr{B} \matr{B}^T = \begin{pmatrix} \matr{B}_A \matr{B}_A^T & \matr{B}_A \matr{B}_C^T \\ \matr{B}_C \matr{B}_A^T & \matr{B}_C \matr{B}_C^T + \matr{B}_D^2 \end{pmatrix} = \matr{U}_k \matr{U}_k^T + \begin{pmatrix} \matr{0} & \matr{0} \\ \matr{0} & \matr{B}_D^2 \end{pmatrix}, \\
					\matr{\Sigma} &= \matr{B}^{-T} \matr{B}^{-1} = \begin{pmatrix} \matr{B}_A^{-T} (\matr{I} + \matr{B}_C^T \matr{B}_D^{-2} \matr{B}_C) \matr{B}_A^{-1} & -\matr{B}_A^{-T} \matr{B}_C^T \matr{B}_D^{-2} \\ -\matr{B}_D^{-2} \matr{B}_C \matr{B}_A^{-1} & \matr{B}_D^{-2} \end{pmatrix},
				\end{align*}
				where
				\begin{equation*}
					\matr{U}_k := \begin{pmatrix} \matr{B}_A \\ \matr{B}_C \end{pmatrix}.
				\end{equation*}

				% Lower block-triangular
				\begin{figure}[H]
					\centering
					\begin{tikzpicture}[scale=0.5]
						\block[white](0,0) (7,7)
						\block[white](2,0) (5,5)

						\block[red](0,6) (1,1)
						\block[red](0,5) (1,1)
						\block[red](1,6) (1,1)
						\block[red](1,5) (1,1)

						\block[blue](0,4) (1,1)
						\block[blue](1,4) (1,1)
						\block[blue](0,3) (1,1)
						\block[blue](1,3) (1,1)
						\block[blue](0,2) (1,1)
						\block[blue](1,2) (1,1)
						\block[blue](0,1) (1,1)
						\block[blue](1,1) (1,1)
						\block[blue](0,0) (1,1)
						\block[blue](1,0) (1,1)

						\block[gray](2,4) (1,1)
						\block[gray](3,3) (1,1)
						\block[gray](4,2) (1,1)
						\block[gray](5,1) (1,1)
						\block[gray](6,0) (1,1)
					\end{tikzpicture}
					\caption{Lower triangular Matrix Group $\set{B}_{\text{low}}(k)$}
				\end{figure}

				Algorithm \ref{alg:RC} can simply be transferred to this domain using 
				\begin{equation}
					\matr{C_{low}} = \begin{pmatrix} \frac{1}{2} \matr{J}_A & \matr{0} \\ \matr{J}_C & \frac{1}{2} \matr{J}_D \end{pmatrix},
					\label{eqn:C_low}
				\end{equation}
				and
				\begin{equation}
					\kappa_{low}(\matr{M}) = \kappa_{low} \begin{pmatrix} \matr{M}_A & \matr{M}_B \\ \matr{M}_C & \matr{M}_D \end{pmatrix} = \begin{pmatrix} \matr{M}_A & \matr{0} \\ \matr{M}_C & \diag(\matr{M}_D) \end{pmatrix}.
					\label{eqn:kappa_low}
				\end{equation}
				The resulting update rule is presented in Algorithm \ref{alg:arrowhead}.

				\begin{algorithm}[!htbp]
					\DontPrintSemicolon
					% \KwInput{$\text{Initial parameters } \matr{\mu}, \matr{B}$}
					\KwInput{$\text{Learning rate } \alpha > 0, \text{ rank } k \in \{0, \dots, d\}, \text{ prior precision } \eta > 0$}
					\myinput{$\text{Regularization } \gamma \ge 0, \text{ damping } \xi \ge 0, \text{ Number of MC samples } M \ge 1$}
					\myinput{$\beta_1, \beta_2 \in (0, 1]$}
					Set $\matr{m} \leftarrow \matr{0}, \matr{\mu} \leftarrow \matr{0}, \matr{\hat{B}} \leftarrow \sqrt{\frac{\gamma \eta}{N} + \xi} \matr{I}$

					\For{$t = 0, 1, 2, \dots $}{

						Randomly sample a minibatch $\set{M}$ from the training set

						Draw $M$ MC samples from the posterior $q(\matr{z} | \matr{\lambda}) = \Normal(\matr{z} | \matr{\mu}, N (\matr{\hat{B}} \matr{\hat{B}}^T)^{-1})$
						$$\matr{z}_m \leftarrow \matr{\mu} + N^{-1/2} \matr{\hat{B}}^{-T} \matr{\varepsilon}_m, \quad \text{ where } \matr{\varepsilon}_m \sim \Normal(\matr{0}, \matr{I}_{d\times d}), \quad \forall m \in \{1, \dots, M\}$$
						% $\matr{z} \leftarrow \matr{\mu} + N^{-1/2} \left[\matr{\hat{U_k}} \matr{\varepsilon}_{rank} + \diag{\matr{\hat{B}}_D^{-1}} \otimes \matr{\varepsilon}_{diag}\right]$, where $\matr{\varepsilon}_{rank} \sim \Normal(\matr{0}, \matr{I}_{k\times k}), \matr{\varepsilon}_{diag} \sim \mathcal{N}(\matr{0}, \matr{I}_{(d-k) \times (d-k)})$

						Sample $M$ gradient samples using $\matr{z}_m$
						$$\matr{\bar{g}}_m \leftarrow \frac{1}{|\set{M}|} \sum_{i \in \set{M}} \grad_z \ell_i (\matr{z}_m), \quad \forall m \in \{1, \dots, M\}$$

						Compute the gradient for the mean parameter
						$$\matr{g}_{\matr{\mu}} \leftarrow \frac{\gamma \eta}{N} \matr{\mu} + \frac{1}{M} \sum_{m=1}^M \matr{\bar{g}}_m $$

						Update the momentum term
						$$\matr{m} \leftarrow \beta_1 \matr{m} + (1 - \beta_1) \matr{g}_{\matr{\mu}} $$

						Debias momentum and precision
						$$\matr{\bar{m}} \leftarrow \frac{\matr{m}}{1 - \beta_1^{t + 1}}, \matr{\bar{B}} = \frac{\matr{\hat{B}} - \sqrt{\frac{\gamma \eta}{N} + \xi} \matr{I}}{\sqrt{1 - \beta_2^t + \varepsilon}} + \sqrt{\frac{\gamma \eta}{N} + \xi} \matr{I} $$

						Compute the gradient with respect to $\matr{\Sigma}$
						$$\matr{G_{\Sigma}} \leftarrow \frac{\gamma \eta}{N} \matr{I} - \gamma\matr{\hat{B}} \matr{\hat{B}}^T + \frac{N}{2M} \sum_{m=1}^M \left( \matr{K}(\matr{z}_m) + \matr{K}(\matr{z}_m)^T \right), $$
						where $\matr{K}(\matr{z}_m) = \matr{\hat{B}} \matr{\hat{B}}^T (\matr{z}_m - \matr{\mu}) \matr{\bar{g}}_m^T$

						Update mean parameter scaled by dampened precision estimate
						$$\matr{\mu} \leftarrow \matr{\mu} - \alpha \matr{\bar{B}}^{-T} \matr{\bar{B}}^{-1} \matr{\bar{m}}, \color{red} \text{(7)} \color{black}$$

						Update square root precision
						$$\matr{\hat{B}} \leftarrow \matr{\hat{B}} \matr{h}((1 - \beta_2) \matr{C_{low}} \odot \kappa_{low}(\matr{\hat{B}}^{-1} \matr{G_{\Sigma}} \matr{\hat{B}}^{-T})),$$
						where $\matr{h}, \matr{C}_{\text{low}}, \kappa_{\text{low}}$ given in (\ref{eqn:h_func}), (\ref{eqn:C_low}), (\ref{eqn:kappa_low})
					}
					\caption{Block AH Covariance Structure Update Rule using Momentum in the Auxiliary/Global Space}
					\label{alg:arrowhead}
				\end{algorithm}

				Once again, an implementation issue arises when trying to compute $\kappa_{\text{low}}(\matr{\hat{B}}^{-1} \matr{G_{\Sigma}} \matr{\hat{B}}^{-T}))$ which involves a dense matrix. 
				Similarly to Lemma \ref{lemma:kappa_up}, we can show that this computation can be done more efficiently in memory.
				\begin{lemma}
					For a multivariate Gaussian posterior $q(\matr{z} | \matr{\lambda}) = \Normal(\matr{z} | \matr{\mu}, \matr{S}^{-1})$, $M$ MC samples $\matr{z} \sim q(\matr{z} | \matr{\lambda})$, and prior $p(z) = \Normal(\matr{z} | \matr{0}, \eta^{-1} \matr{I}), \eta > 0$, with parametrization $\matr{S} = \matr{B} \matr{B}^T, \matr{B} \in \set{B}_{low}(k)$, we have
						\begin{align*}
							\kappa_{low}(\matr{B}^{-1} \matr{G_{\Sigma}} \matr{B}^{-T}) &= \frac{\gamma \eta}{N}\begin{pmatrix} \matr{B}_A^{-T} (\matr{I} + \matr{B}_C^T \matr{B}_D^{-2} \matr{B}_C) \matr{B}_A^{-1} & \matr{0} \\ -\matr{B}_D^{-2} \matr{B}_C \matr{B}_A^{-1} & \matr{B}_D^{-2} \end{pmatrix} \\
							&\quad - \gamma \matr{I} \\
							&\quad + \frac{N}{2 M} \sum_{m=1}^M \begin{pmatrix} \matr{M}_{A, m} + \matr{M}_{A, m}^T & \matr{0} \\ \matr{M}_{B, m}^T + \matr{M}_{C, m} & 2\matr{M}_{D, m} \end{pmatrix},
						\end{align*}
						where 
						\begin{align*}
							\matr{M}_{A, m} &:= (\matr{B}_A^T \matr{v}_{m, :k} + \matr{B}_C \matr{v}_{m, k:}) (\matr{\bar{g}}_{:k}^T \matr{B}_A + \matr{\bar{g}}_{k:}^T \matr{B}_C) \\
							\matr{M}_{B, m} &:= (\matr{B}_A^T \matr{v}_{m, :k} + \matr{B}_C \matr{v}_{m, k:}) \matr{\bar{g}}_{k:}^T \matr{B}_D \\
							\matr{M}_{C, m} &:= \matr{B}_D \matr{v}_{k:} (\matr{\bar{g}}_{:k}^T \matr{B}_A + \matr{\bar{g}}_{k:}^T \matr{B}_C ) \\
							\matr{M}_{D, m} &:= \matr{B}_D^2 \odot \matr{v}_{k:} \odot \matr{\bar{g}}_{k:}
						\end{align*}
						\label{lemma:kappa_low}
				\end{lemma}

			\subsubsection{Beyond Block-Diagonal Structure}
				Previously, we have only considered intra-layer weight correlations within the same layer for a block-diagonal covariance.
				This is equivalent to a variational posterior which could be composed into 
				\begin{equation*}
					q_{\matr{\lambda}} (\matr{z}) = \prod_{\ell=1}^L q_{\matr{\lambda}_{\ell}} (\matr{z}_{\ell}).
				\end{equation*}
				However, this assumed independence across layers and in practice, this assumption is often unjustified and there exists a high level of inter-layer correlation.
				At the same time, modelling all correlations is prohibitively expensive and leads to a quadratic covariance $\matr{\Sigma} \in \Real^{d \times d}$, i.e. $\matr{\Sigma} \in \BigO (d^2)$ which does not fit into memory for large neural networks.
				Thus, one again needs to strike a balance between a limited diagonal parametrization $\matr{\Sigma} = \diag (\matr{\sigma})$, where $\matr{\sigma} \in \Real^d$ and a fully factorized covariance matrix with $d^2$ parameters.
				Furthermore, the implicit square-root structure makes it difficult to formulate this structure easily.
				Furthermore, the off-diagonal blocks $\matr{B}^{(i, i+p)} \in \Real^{d_i \times d_{i+p}}, p \ge 1$ will not be able to be represented in fully factorized form either since they will be in $\BigO (d^2)$.
				There are a number of possibilities to parametrize this non-square matrix differently to reduce the number of parameters significantly such as a Kronecker factorization
					\[
						\matr{B}^{(i, i+p)} = \matr{X} \otimes \matr{Y}, \quad \text{ where } \matr{X}, \matr{Y} \in \Real^{\sqrt{d_i} \times \sqrt{d_{i+p}}},
					\]
				which reduces the parameter dimensions from quadratic to linear.
				Another option is to parametrize the off-diagonal blocks as matrices of rank $k$
					\[
						\matr{B}^{(i, i+p)} = \matr{X} \otimes \matr{Y}, \quad \text{ where } \matr{X} \in \Real^{d_i \times k}, \matr{Y} \in \Real^{d_{i+p} \times k}.
					\]
				The diagonal blocks $\matr{B}^{(i, i)}$ can be chosen from the upper triangular matrix group $\set{B}_{\text{up}}$ or the lower triangular group $\set{B}_{\text{low}}$, respectively.
				For simplicity, we will present and implement the square-root precision $\matr{B}$ with diagonal blocks from the upper triangular group and low-rank matrices as off-diagonal blocks in this thesis.

				\begin{definition}
					Define the block upper triangular matrix group with off-diagonal blocks $\set{B}_{\text{tri-up}}$ to be
					\begin{equation}
						\set{B}_{\text{tri-up}}^{p_{\text{max}}}(k_0, k_1) = \left \{ \sum_{i=0}^{p_{\text{max}}} \matr{B}_{\text{up}}^i \mid \matr{B}_{\text{up}}^0 \in \set{B}_{\text{up}}(k_0), \matr{B}_{\text{up}}^i \in \set{R}(k_1) \forall i > 0 \right \},
					\end{equation}
					where
					\begin{equation}
						\set{R}(k) = \left \{\matr{X} \matr{Y}^T \mid \matr{X} \in \Real^{d_i \times k}, \matr{Y} \in \Real^{d_{i+1} \times k} \right\}
					\end{equation}
				\end{definition}

				\begin{lemma}[{\parencite[Thm. 1]{FS16}}]
					Let $k_0, k_1 \in \Natural$ the rank parameters for the diagonal and off-diagonal blocks, respectively, $p_{\text{max}}$ the maximum block bandwidth of the block upper triangular matrix $\matr{B} \in \set{B}_{\text{tri-up}}^{p_{\text{max}}}(k_0, k_1)$.
					Then, for the full inverse $\matr{B}^{-1}$, we have
					\begin{equation}
						(\matr{B}^{-1})^{(i, j)} =
						\begin{cases}
							(\matr{B}^{(i, i)})^{-1}, & \text{ if } i = j \\
							(\matr{B}^{(i, i)})^{-1} \sum_{i < q_1 < \ldots < q_m < j} (-1)^{m+1} \matr{B}^{(i, q_1)} \matr{B}^{(q_1, q_2)}  \cdots \matr{B}^{(q_m, j)} & \text{ if } i < j \le p_{\text{max}} \\
							\matr{0}, & \text{ otherwise}.
						\end{cases}
					\end{equation}
					Furthermore, we have that the set $\set{B}_{\text{tri-up}}^{p_{\text{max}}}(k_0, k_1)$ is closed under inversion, i.e.,
					\begin{equation}
						\matr{B}^{-1} \in \set{B}_{\text{tri-up}}^{p_{\text{max}}}(k_0, k_1).
					\end{equation}
				\end{lemma}

				\begin{corollary}
					Let $k_0 \in \Natural, k_1 = 1$ the rank parameters for the diagonal and off-diagonal blocks, respectively, $p_{\text{max}}$ the maximum block bandwidth of the block upper triangular matrix $\matr{B} \in \set{B}_{\text{tri-up}}^{p_{\text{max}}}(k_0, k_1)$.
					Then, for the full inverse $\matr{B}^{-1}$, we have
					\begin{equation}
					(\matr{B}^{-1})^{(i, j)} =
						\begin{cases}
							(\matr{B}^{(i, i)})^{-1}, & \text{ if } i = j \\
							-(\matr{B}^{(i, i)})^{-1} \matr{B}^{(i, i+1)}, & \text{ if } j = i + 1  \\
							\matr{0}, & \text{ otherwise}.
						\end{cases}
					\end{equation}
					Furthermore, we have that the set $\set{B}_{\text{tri-up}}^{p_{\text{max}}}(k_0, k_1)$ is closed under inversion, i.e.,
					\begin{equation}
						\matr{B}^{-1} \in \set{B}_{\text{tri-up}}^{p_{\text{max}}}(k_0, k_1).
					\end{equation}
				\end{corollary}

				\begin{theorem}
					Let $\matr{B} \in \set{B}_{\text{tri-up}}^{p_{\text{max}}}(k_0, k_1)$.
					For the parametrization $\matr{S} = \matr{B} \matr{B}^T$, we have that the precision $\matr{S}$ and covariance $\matr{\Sigma} = \matr{S}^{-1}$ have block-offdiagonals of band width $2 p_{\text{max}}$, i.e.
					\begin{align}
						\matr{S} &= \sum_{i=-p_{\text{max}}}^{p_\text{max}} \matr{S}^i, \\
						\matr{\Sigma} &= \sum_{i=-p_{\text{max}}}^{p_\text{max}} \matr{\Sigma}^i,
					\end{align}
					where the index $i$ corresponds to the block position of the diagonals and off-diagonals, respectively.
				\end{theorem}

				\begin{corollary}
					Let $\matr{B} \in \set{B}_{\text{tri-up}}^{p_{\text{max}}}(k_0, k_1), p_{\text{max}} = 1$.
					Then, we have that for parametrization $\matr{S} = \matr{B} \matr{B}^T$, the precision $\matr{S}$ and covariance $\matr{\Sigma} = \matr{S}^{-1}$ are block-tridiagonal.
				\end{corollary}

				For an illustration of the set $\set{B}_{\text{tri-up}}^{p_{\text{max}}}(k_0, k_1)$ and the induced block-tridiagonal covariance matrix, see Figures \ref{fig:B_TriUp} and \ref{fig:BlockTridiagonal}.

				% B_{tri-up}
				\begin{figure}[H]
					\centering
					\begin{tikzpicture}[scale=0.5]
						\block[white](0,0) (7,7)

						% Diagonals
						\node[] at (-1, 7.5) {Layer};
						\node[] at (-1, 6.5) {1};
						\block[red](0, 6) (1,1)
						\node[] at (-1, 5.5) {2};
						\block[red](1, 5) (1,1)
						\node[] at (-1, 4.5) {\vdots};
						\block[red](2, 4)     (1,1)
						\node[] at (-1, 3.5) {$\ell$};
						\block[red](3, 3) (1,1)
						\block[red](4, 2)     (1,1)
						\node[] at (-1, 2.5) {\vdots};
						\node[] at (-1, 1.5) {$L-1$};
						\block[red](5,1) (1,1)
						\node[] at (-1, 0.5) {$L$};
						\block[red](6, 0) (1,1)

						% Off-Diagonals
						\block[blue](1,6) (1,1)
						\block[blue](2,5) (1,1)
						\block[blue](3,4)  (1,1)
						\block[blue](4,3) (1,1)
						\block[blue](5,2)  (1,1)
						\block[blue](6,1) (1,1)

						% % Diagonals
						% \node[] at (-1, 7.5) {Layer};
						% \node[] at (-1, 6.5) {1};
						% \block[red](0, 6) B^{(1, 1)} (1,1)
						% \node[] at (-1, 5.5) {2};
						% \block[red](1, 5) B^{(2, 2)} (1,1)
						% \node[] at (-1, 4.5) {\vdots};
						% \block[white](2, 4) \ddots     (1,1)
						% \node[] at (-1, 3.5) {$\ell$};
						% \block[red](3, 3) B^{(\ell, \ell)} (1,1)
						% \block[white](4, 2) \ddots     (1,1)
						% \node[] at (-1, 2.5) {\vdots};
						% \node[] at (-1, 1.5) {$L-1$};
						% \block[red](5,1) B^{(L-1, L-1)} (1,1)
						% \node[] at (-1, 0.5) {$L$};
						% \block[red](6, 0) B^{(L, L)} (1,1)

						% % Off-Diagonals
						% \block[blue](1,6) B^{(1, 2)} (1,1)
						% \block[blue](2,5) B^{(2, 3)} (1,1)
						% \block[white](3,4) \ddots (1,1)
						% \block[blue](4,3) B^{(l, l+1)} (1,1)
						% \block[white](5,2) \ddots (1,1)
						% \block[blue](6,1) B^{(L-1, L)} (1,1)
					\end{tikzpicture}
					\caption{Upper Block-Tridiagonal set $\set{B}_{\text{tri-up}}^{p_{\text{max}}}(k_0, k_1)$}
					\label{fig:B_TriUp}
				\end{figure}

				% Block Tridiagonal Covariance/Precision Matrix
				\begin{figure}[H]
					\centering
					\begin{tikzpicture}[scale=0.5]
						\block[white](0,0) (7,7)

						% Diagonals
						\block[red](0,6)  (1,1)
						\block[red](1,5)  (1,1)
						\block[red](2,4)  (1,1)
						\block[red](3,3)  (1,1)
						\block[red](4,2)  (1,1)
						\block[red](5,1)  (1,1)
						\block[red](6,0) (1,1)

						% Off-Diagonals
						\block[blue](1,6)  (1,1)
						\block[blue](2,5)  (1,1)
						\block[blue](3,4)  (1,1)
						\block[blue](4,3)  (1,1)
						\block[blue](5,2)  (1,1)
						\block[blue](6,1)  (1,1)

						\block[blue](0,5)  (1,1)
						\block[blue](1,4)  (1,1)
						\block[blue](2,3)  (1,1)
						\block[blue](3,2)  (1,1)
						\block[blue](4,1)  (1,1)
						\block[blue](5,0)  (1,1)

						% % Diagonals
						% \block[red](0,6) S^{(1  , 1  )} (1,1)
						% \block[red](1,5) S^{(2  , 2  )} (1,1)
						% \block[red](2,4) S^{(l-1, l-1)} (1,1)
						% \block[red](3,3) S^{(l  , l  )} (1,1)
						% \block[red](4,2) S^{(l+1, l+1)} (1,1)
						% \block[red](5,1) S^{(L-1, L-1)} (1,1)
						% \block[red](6,0) S^{(L  , L  )} (1,1)

						% % Off-Diagonals
						% \block[blue](1,6) S^{(1  , 2  )} (1,1)
						% \block[blue](2,5) S^{(2  , 3  )} (1,1)
						% \block[blue](3,4) S^{(l-1, l  )} (1,1)
						% \block[blue](4,3) S^{(l  , l+1)} (1,1)
						% \block[blue](5,2) S^{(L-2, L-1)} (1,1)
						% \block[blue](6,1) S^{(L-1, L  )} (1,1)

						% \block[blue](0,5) S^{(2  , 1  )} (1,1)
						% \block[blue](1,4) S^{(3  , 2  )} (1,1)
						% \block[blue](2,3) S^{(l  , l-1)} (1,1)
						% \block[blue](3,2) S^{(l+1, l  )} (1,1)
						% \block[blue](4,1) S^{(L-1, L-2)} (1,1)
						% \block[blue](5,0) S^{(L  , L-1)} (1,1)
					\end{tikzpicture}
					\caption{Block-Tridiagonal Covariance/Precision Matrix with respect to the Neural Network Architecture}
					\label{fig:BlockTridiagonal}
				\end{figure}

				For simplicity, from now on, we will only consider the one off-diagonal block with rank-1 matrices, i.e. $p_{\text{max}} = 1, k_1 = 1$.

				An issue that arises when going beyond diagonal blocks is that this set does no longer form a group under matrix multiplication and one needs to project the resulting matrices.
				In the implementation however, one cannot simply compute these intermediate results and then project down since the former matrix cannot fit into memory.
				We will now present a way how to perform the update steps efficiently in memory.

				\begin{align}
					\matr{\mu}_{t} & \leftarrow \matr{\mu}_t - \alpha \matr{B}_t^{-T} \matr{B}_t^{-1} \matr{g_{\mu}}, \\
					\matr{B}_{t+1} & \leftarrow \kappa_{\text{tri-up}} (\matr{B}_t \matr{h}(2 \alpha \matr{C_{\text{tri-up}}} \odot \matr{B}_t^{-1} \matr{G_{\Sigma}} \matr{B}_t^{-T})),
				\end{align}

				\begin{lemma}
					For the first term, we have for the diagonal blocks,
					\begin{equation}
						\left( \matr{B}^{-1} \matr{B}^{-T} \right)^{(i, i)} = 
						\begin{cases}
							\left( \matr{B}^{(i, i)} \right)^{-1} \left[ \matr{I}^{(i, i)} + \matr{B}^{(i, i+1)} \left( \matr{B}^{i, i+1} \right)^T \right] \left( \matr{B}^{(i, i)} \right)^{-T}, &\text{ if } i \le L - 1 \\
							\left( \matr{B}^{(L, L)} \right)^{-1} \left( \matr{B}^{(L, L)} \right)^{-T}, &\text{ if } i = L,
						\end{cases}
					\end{equation}
					and for the off-diagonal blocks
					\begin{equation}
						\left( \matr{B}^{-1} \matr{B}^{-T} \right)^{(i, i+1)} = \left( \matr{B}^{(i, i)} \right)^{-1} \matr{B}^{(i, i+1)} \left(\matr{B}^{(i+1, i+1)} \right)^{-T}, \quad \forall i \in [L-1].
					\end{equation}
				\end{lemma}

				\begin{proof}
					Decompose the upper triangular matrix $\matr{B}^{-1}$ into its diagonal blocks and first off-diagonal blocks as 
					\begin{align}
						\matr{B}^{-1} & = \Diag \left( \{ (\matr{B}^{-1})^{(i, i)} | i \in [L] \} \right) \\
						& \quad + \OffDiag \left( \{ (\matr{B}^{-1})^{(i, i+1)} | i \in [L-1] \} \right).
					\end{align}
					Expanding $(\matr{B}^{-1} \matr{B}^{-T})$, we obtain the block-tridiagonal matrix with diagonal blocks
					\begin{align*}
						(\matr{B}^{-1} \matr{B}^{-T})^{(i, i)} &= \sum_{k=1}^L (\matr{B}^{-1})^{(i, k)} (\matr{B}^{-T})^{(k, i)} \\
						&= \sum_{k=i}^L (\matr{B}^{-1})^{(i, k)} (\matr{B}^{-T})^{(k, i)} \\
						&= \sum_{p=0}^{p_{\text{max}}} (\matr{B}^{-1})^{(i, i+p)} (\matr{B}^{-T})^{(i+p, i)} \\
						&= (\matr{B}^{-1})^{(i, i)} (\matr{B}^{-T})^{(i, i)} + (\matr{B}^{-1})^{(i, i+1)} (\matr{B}^{-T})^{(i+1, i)} \\
						&= (\matr{B}^{(i, i)})^{-1} (\matr{B}^{(i, i)})^{-T} + [-\matr{B}^{(i, i)})^{-1} \matr{B}^{(i, i+1)}] [-(\matr{B}^{(i, i)})^{-1}\matr{B}^{(i, i+1)}]^T \\
						&= (\matr{B}^{(i, i)})^{-1} \left[\matr{I} + \matr{B}^{(i, i+1)} (\matr{B}^{(i, i+1)})^T \right] (\matr{B}^{(i, i)})^{-T}
					\end{align*}
					for all $i \in [L-1]$, and for the last remaining diagonal block, we have
					\begin{equation*}
						(\matr{B}^{-1} \matr{B}^{-T})^{(L, L)} = (\matr{B}^{(i, i)})^{-1} (\matr{B}^{(i, i)})^{-T}
					\end{equation*}

					For the off-diagonal blocks, we obtain
					\begin{align*}
						(\matr{B}^{-1} \matr{B}^{-T})^{(i, i+1)} &= \sum_{k=1}^L (\matr{B}^{-1})^{(i, k)} (\matr{B}^{-T})^{(k, i+1)} \\
						&= \sum_{k=i+1}^L (\matr{B}^{-1})^{(i, k)} (\matr{B}^{-T})^{(k, i+1)} \\
						&= \sum_{p=1}^{p_{\text{max}}} (\matr{B}^{-1})^{(i, i+p)} (\matr{B}^{-T})^{(i+p, i+1)} \\
						&= (\matr{B}^{-1})^{(i, i+1)} (\matr{B}^{-T})^{(i+1, i+1)} + \sum_{p=2}^{p_{\text{max}}} (\matr{B}^{-1})^{(i, i+p)} (\matr{B}^{-T})^{(i+p, i+1)} \\
						&= (\matr{B}^{(i, i)})^{-1} \matr{B}^{(i, i+1)} (\matr{B}^{(i+1, i+1)})^{-T}
					\end{align*}
					for all $i \in [L-1]$.
				\end{proof}

				\begin{lemma}
					Let $\matr{x} := \matr{B}^T \matr{v}_m, \matr{y} := \matr{B}^{-1} \matr{g}_m$.
					For the last term, we have $\matr{K(\matr{z_m})} = \matr{B}^T \matr{v}_m (\matr{g}_m)^T \matr{B}^{-T} = \matr{x} \matr{y}^T$, and for the individual blocks,
					\begin{equation}
						\left( \matr{K(z_m)} + \matr{K(z_m)}^{-T} \right)^{(i, j)} = \matr{x}^{(i)} (\matr{y}^{(j)})^T + \matr{y}^{(i)} (\matr{x}^{(j)})^T,
					\end{equation}
					where 
					\begin{align}
						\matr{x}^{(j)} &= 
						\begin{cases}
							\left( \matr{B}^{(1, 1)} \right)^{T} \matr{v}_m^{(1)}, & \text{ if } j = 1. \\
							\left( \matr{B}^{(j, j)} \right)^{T} \matr{v}_m^{(j)} + \matr{B}^{(j-1, j)} \matr{v}_m^{(j-1)}, & \text{ otherwise},
						\end{cases} \\
						\matr{y}^{(j)} &= 
						\begin{cases}
							\left( \matr{B}^{(1, 1)} \right)^{T} \matr{v}_m^{(1)}, & \text{ if } j = 1. \\
							\left( \matr{B}^{(j, j)} \right)^{-1} \left( \matr{g}_m^{(j)} + \matr{B}^{(j, j+1)} \matr{g}_m^{(j+1)} \right), & \text{ otherwise}.
						\end{cases}
					\end{align}
				\end{lemma}

				\begin{proof}
					Recall the definition of $\matr{K}(\matr{z}_m) := \matr{B}^T \matr{v}_m (\matr{g}_m)^T \matr{B}^{-T}$. 
					For the $j$-th block of the vector $\matr{x}$, we have
					\begin{align*}
						\matr{x}^{(j)} &= (\matr{B}^T \matr{v}_m)^{(j)} = \sum_{k=1}^L (\matr{B}^T)^{(j, k)} (\matr{v}_m)^{(k)} \\
						&= \sum_{k=1}^L (\matr{B}^{(k, j)})^T (\matr{v}_m)^{(k)} \\
						&= \sum_{p=0}^{p_{\text{max}}} (\matr{B}^{(j-p, j)})^T (\matr{v}_m)^{(j-p)} \\
						&= (\matr{B}^{(j, j)})^T (\matr{v}_m)^{(j)} + (\matr{B}^{(j-1, j)})^T (\matr{v}_m)^{(j-1)},
					\end{align*}
					when $j > 1$
					and 
					\begin{equation*}
						\matr{x}^{(1)} = (\matr{B}^{(1, 1)})^T (\matr{v}_m)^{(1)})
					\end{equation*}
					for $j = 1$.
					For the $j$-th block of the vector $\matr{y}$, we have
					\begin{align*}
						\matr{y}^{(j)} &= (\matr{B}^{-1} \matr{g}_m)^{(j)} = \sum_{k=1}^L (\matr{B}^{-1})^{(j, k)} (\matr{g}_m)^{(k)} \\
						&= \sum_{k=j}^L (\matr{B}^{-1})^{(j, k)} (\matr{g}_m)^{(k)} \\
						&= \sum_{p=0}^{p_{\text{max}}} (\matr{B}^{-1})^{(j, j+p)} (\matr{g}_m)^{(j+p)} \\
						&= (\matr{B}^{-1})^{(j, j)} (\matr{g}_m)^{(j)} + (\matr{B}^{-1})^{(j, j+1)} (\matr{g}_m)^{(j+1)} \\
						&= (\matr{B}^{(j, j)})^{-1} (\matr{g}_m)^{(j)} + (\matr{B}^{-1})^{(j, j)} \matr{B}^{(j, j+1)} (\matr{g}_m)^{(j+1)} \\
						&= (\matr{B}^{(j, j)})^{-1} \left[ (\matr{g}_m)^{(j)} + \matr{B}^{(j, j+1)} (\matr{g}_m)^{(j+1)} \right]
					\end{align*}
					for all $j < L$ and 
					\begin{equation*}
						\matr{y}^{(L)} = (\matr{B}^{(L, L)})^{-1} (\matr{g}_m)^{(L)}
					\end{equation*}
					for $j = L$.
				\end{proof}

				\begin{theorem}
					Let $\matr{M} \in \Real^{d \times d}$,
					\begin{align}
						\matr{M} = \matr{B}_t^{-1} \matr{G_{\Sigma}} \matr{B}^{-T} = \frac{\gamma \eta}{N} \matr{B}^{-1} \matr{B}^{-T} - \gamma \matr{I} + \frac{N}{2 M} \sum_{m=1}^M (\matr{K(z_m)} + \matr{K(z_m)}^T),
					\end{align}
					where $\matr{K}(z_m) = \matr{B}^T \matr{v}_m \matr{g}_m^T \matr{B}^{-T}, \matr{z}_m \sim \Normal(\matr{\mu}_t, \matr{\Sigma}_t)$.
					Then, we have for the individual blocks
					\begin{equation*}
						\matr{M}^{(i, j)} = \frac{\gamma \eta}{N} \left( \matr{B}^{-1} \matr{B}^{-T} \right)^{(i, j)} - \gamma \matr{I}^{(i, j)} + \frac{N}{2M} \sum_{m=1}^M \left( \matr{x}^{(i)} (\matr{y}^{(j)})^T + \matr{y}^{(i)} (\matr{x}^{(j)})^T \right), \quad i \le j \le p_\text{max},
					\end{equation*}
					where index $(i, i)$ corresponds to diagonal blocks and $(i, i+1)$ to off-diagonal blocks, respectively. 
				\end{theorem}

				Unfortunately, the formulation of \parencite{LNK+21} does not allow for sparse parametrizations in the off-diagonal blocks $\matr{B}^{(i, i+1)} = \matr{x} \matr{y}^T$ and even worse, adding two different rank matrices, $\matr{x}_1 \matr{y}_1^T + \matr{x}_2 \matr{y}_2^T \approx \matr{\tilde{x}} \matr{\tilde{y}}^T$, does not result in a matrix of the same rank that can be easily factored.
				However, we can employ a trick such as in \parencite{MG15} to approximate the left-hand side as a factorization of rank-k matrices. 
				Thus, whenever we add these off-diagonal blocks, we will approximate the right-hand side by its closest rank-k matrix as in Def. \ref{def:RankMatrixAddition}.

				\begin{lemma}
					\label{lemma:RankMatrixAddition}
					Let $\matr{x}_1, \matr{x}_2 \in \Real^{m} \setminus \{\matr{0}\}, \matr{y}_1, \matr{y}_2 \in \Real^{n} \setminus \{\matr{0}\}$, then the scalar $\pi > 0$ that minimizes the distance
					\begin{equation}
						(\matr{x}_1 + \pi \matr{x}_2) (\matr{y}_1 + \pi^{-1} \matr{y}_2)^T - (\matr{x}_1 \matr{y}_1^T + \matr{x}_2 \matr{y}_2^T)
					\end{equation}
					is given by
					\begin{equation}
						\pi := \sqrt{\frac{\|\matr{x}_1 \matr{y}_2^T\|}{\|\matr{x}_2 \matr{y}_1^T\|}} = \sqrt{\frac{\|\matr{x}_1 \|_2 \|\matr{y}_2^T\|_2}{\|\matr{x}_2\|_2 \|\matr{y}_1^T\|_2}}
						\label{eqn:piScalarDefinition}
					\end{equation}
				\end{lemma}

				\begin{proof}
					\begin{align*}
						\| (\matr{x}_1 + \pi \matr{x}_2) (\matr{y}_1 + \pi^{-1} \matr{y}_2)^T - (\matr{x}_1 \matr{y}_1^T + \matr{x}_2 \matr{y}_2^T) \| 
						&= \| \pi \matr{x}_2 (\matr{y}_1)^T + \pi^{-1} \matr{x}_1 (\matr{y}_2)^T \| \\
						&= \pi \| \matr{x}_2 (\matr{y}_1)^T \| + \pi^{-1} \| \matr{x}_1 (\matr{y}_2)^T \| \\
					\end{align*}
					which is minimized by choosing $\pi$ as in Eqn. (\ref{eqn:piScalarDefinition})
				\end{proof}

				We are now ready to derive the Structured Natural Gradient Descent algorithm with block-tridiagonal covariance and precision.

				\begin{lemma}
					Assumption \ref{assumption:Diffeomorphism} is satisfied for the parametrization 
					\begin{align*}
						\matr{\tau} &:= \{\matr{\mu} \in \Real^d, \matr{S} = \matr{B}\matr{B}^T \in \set{S}_+^{d \times d} | \matr{B}  \in \set{B}_{\text{tri-up}}^{(1)}(k) \} \\
						\matr{\lambda} &:= \{\matr{\mu} \in \Real^d, \matr{B} \in \set{B}_{\text{tri-up}}^{(1)}(k) \} \\
						\matr{\eta} &:= \{\matr{\delta} \in \Real^d, \matr{M} \in \set{M}_{\text{tri-up}}^{(1)}(k) \}.
					\end{align*}
					That is, the Fisher matrix $\matr{F}_{\matr{\eta}} (\matr{\eta}_0)$ is non-singular and the mapping $\matr{\eta} \mapsto \matr{\psi} (\matr{\phi}_{\matr{\lambda}_t} (\matr{\eta}))$ is $C^1$-diffeomorphic.
				\end{lemma}

				\begin{proof}
					We can decompose $\matr{S} = \matr{B} \matr{B}^T$ into its individual blocks as 
					\begin{align*}
						(\matr{B} \matr{B}^T)^{(i, i)} &=  \matr{B}^{(i, i)} (\matr{B}^{(i, i)})^T + \matr{B}^{(i, i+1)} (\matr{B}^{(i+1, i+1)})^T \\
						(\matr{B} \matr{B}^T)^{(i, i+1)} &=  \matr{B}^{(i, i+1)} (\matr{B}^{(i+1, i+1)})^T \\
						(\matr{B} \matr{B}^T)^{(i+1, i)} &=  \matr{B}^{(i+1, i+1)} (\matr{B}^{(i, i+1)})^T
					\end{align*}
					and apply the same logic as in \parencite[App.~J.1.3]{LNK+21} to arrive at the conclusion.
				\end{proof}

				\begin{theorem}
					The natural gradients with respect to the diagonal blocks is $\matr{C}_{\text{tri-up}}$, where
					\begin{align*}
						\matr{C}_{\text{tri-up}}^{(i, i)} &= \matr{C}_{i, \text{up}}, \quad \forall i \in [L] \\
						\matr{C}_{\text{tri-up}}^{(i, i+1)} &= \matr{J}_{i, i+1}, \quad \forall i \in [L-1],
					\end{align*}
					where $\matr{C}_{i, up}$ is defined as before such that 
					\begin{align*}
						\matr{C}_{i, up} = \begin{pmatrix} \matr{J}_A & \matr{J}_B \\ \matr{0} & \matr{J}_D \end{pmatrix},
					\end{align*}
					and $\matr{J}$ the matrix filled with ones. 
					Assumption \ref{assumption:FIM} is satisfied.
				\end{theorem}

				\begin{proof}
					Similarly to \parencite{LNK+21}, we can decompose $\matr{M} \in \set{M}_{\text{tri-up}} (k)$ into $\matr{M} = \matr{M}_{Diag} + \matr{M}_{OffDiag}$ where we can again write for the diagonal blocks $\matr{M}_{Diag, A} = \matr{M}_{Diag, A_{up}} + \matr{M}_{Diag, A_{up}}^T + \matr{M}_{Diag, A_{diag}}$, recalling that for the diagonal blocks $\matr{M}_{Diag} \in \set{M}_{up} (k)$. 
					That is, overall we can decompose $\matr{M}$ as
					\begin{equation*}
						\matr{M} = \matr{M}_{Diag} + \matr{M}_{OffDiag} = \matr{M}_{diag} + \matr{M}_{Diag, up} + \matr{M}_{Diag, up}^T + \matr{M}_{asym} + \matr{M}_{OffDiag}.
					\end{equation*}
					We will now have to show that any cross-term of the FIM between these is zero. 
					As the original proof shows, we have
					\begin{align*}
						-\grad_{M_{Diag, asym}} &= -Asym (\grad_M \log q_{\matr{\lambda}} (\matr{z})]) \\
						-\grad_{M_{diag}} &= -Diag (\grad_M \log q_{\matr{\lambda}} (\matr{z})]),
					\end{align*}
					where $\text{Diag}$ returns the diagonal entries and $\text{Asym}$ the asymmetric entries of their arguments. 
					For the off-diagonal blocks, we can show
					\begin{align*}
						-\grad_{M_{\OffDiag_{(i, j)}}} = -\tr ([\grad_{M_{\OffDiag}} \matr{M}] [\log q_{\matr{\lambda}} (\matr{z})]) = -\tr (\matr{I}_{i, j} [\log q_{\matr{\lambda}} (\matr{z})]),
					\end{align*}
					such that we obtain similarly to the asymmetric blocks
					\begin{align*}
						-\grad_{M_{\OffDiag}} &= -\OffDiag (\grad_M \log q_{\matr{\lambda}} (\matr{z})]).
					\end{align*}
					Since all indices $\set{I}_{up}, \set{I}_{asym}, \set{I}_{diag}$ do not coincide with $\set{I}_{\text{off-diag}}$, i.e. $(i, j) \notin \set{I}_{\text{off-diag}}$ for all $(i, j) \in \set{I}_{\text{up}} \cup \set{I}_{\text{asym}} \cup \set{I}_{\text{diag}}$, the cross terms of the FIM are zero. 
					\parencite{LNK+21} show that the remaining terms are zero. 

					For the FIM, the original proof shows 
					\begin{align*}
						-\Expect_{q_{\matr{\lambda}}} [\grad_{M_{up_{i, j}}} \grad_{M_{up}} \log q_{\matr{\lambda}} (\matr{z})] |_{\matr{\eta} = \matr{0}} &= 4 \matr{I}_{i, j} \\
						-\Expect_{q_{\matr{\lambda}}} [\grad_{M_{diag_{i, j}}} \grad_{M_{diag}} \log q_{\matr{\lambda}} (\matr{z})] |_{\matr{\eta} = \matr{0}} &= 2 \matr{I}_{i, j} \\
						-\Expect_{q_{\matr{\lambda}}} [\grad_{M_{asym_{i, j}}} \grad_{M_{asym}} \log q_{\matr{\lambda}} (\matr{z})] |_{\matr{\eta} = \matr{0}} &= \matr{I}_{i, j}.
					\end{align*}
					Now, for the FIM of the off-diagonal blocks, we can show
					\begin{align*}
						&-\Expect_{q_{\matr{\lambda}}} \left[\grad_{M_{\text{off-diag}_{i, j}}} \grad_{M_{\text{off-diag}}} \log q_{\matr{\lambda}} (\matr{z}) \right] |_{\matr{\eta} = \matr{0}} \\
						\qquad&= -\Expect_{q_{\matr{\lambda}}} \left[\grad_{M_{\text{off-diag}_{i, j}}} \OffDiag ( \log q_{\matr{\lambda}} (\matr{z}) ) \right] |_{\matr{\eta} = \matr{0}} \\
						&= -\Expect_{q_{\matr{\lambda}}} \left[ \sum_{m, n} [\grad_{M_{\text{off-diag}_{i, j}}} M_{m, n}] \grad_{M_{m, n}} \OffDiag ( \log q_{\matr{\lambda}} (\matr{z}) ) \right] |_{\matr{\eta} = \matr{0}} \\
						&= -\Expect_{q_{\matr{\lambda}}} \left[ [\grad_{M_{\text{off-diag}_{i, j}}} M_{i, j}] \grad_{M_{i, j}} \OffDiag ( \log q_{\matr{\lambda}} (\matr{z}) ) \right] |_{\matr{\eta} = \matr{0}} \\
						&= -\OffDiag (\Expect_{q_{\matr{\lambda}}} \left[\grad_{M_{i, j}} \grad_M \log q_{\matr{\lambda}} (\matr{z}) \right] ) |_{\matr{\eta} = \matr{0}} \\
						&= -\OffDiag (\grad_{M_{i, j}} [\matr{M} + \matr{M}^T]) \\
						&= -\OffDiag (\matr{I}_{i, j} + \matr{I}_{j, i}) = \matr{I}_{i, j},
					\end{align*}
					where the second to last step followed from \parencite[Lemma 11]{LNK+21} and the last step from the fact that for $i < j$, when $(i, j) \in \set{I}_{\text{off-diag}}$, we have $(i, j) \notin \set{I}_{\text{off-diag}}$, exactly like in the proof for the asymmetric case. 

					Therefore, we find that for gradients evaluate at $\matr{\eta}_0 = \{ \matr{\delta}_0, \matr{M}_0 \} = \matr{0}$
					\begin{align*}
						\grad_{\matr{\delta}_i} \set{L} |_{\matr{\eta} = \matr{0}} &= [\grad_{\delta_i} \matr{\delta}]^T \matr{B}_t^{-1} \grad_{\mu} \set{L} \\
						\grad_{\matr{M}_{i, j}} \set{L} |_{\matr{\eta} = \matr{0}} &= -\tr ( [ \matr{M} + \matr{M}^T) \matr{B}_t^{-1} [ \grad_{\matr{\Sigma}} \set{L} ] \matr{B}_t^{-T})
					\end{align*}
					We have for the Euclidean gradient with respect to local parameters $\matr{\delta}$ and $\matr{M}$
					\begin{align*}
						\matr{g}_{\delta} &= \matr{B}_t^{-1} \grad_{\mu} \set{L} \\
						\matr{G}_{M_{diag}} &= \Diag(\matr{G}_M) \\
						\matr{G}_{M_{up}} &= \text{Up}(\matr{G}_M + \matr{G}_M^T) = 2\text{Up} (\matr{G}_M) \\
						\matr{G}_{M_{asym}} &= \text{Asym}(\matr{G}_M) \\
						\matr{G}_{M_{off-diag}} &= \OffDiag (\matr{G}_M),
					\end{align*}
					where $\matr{G}_M = -2 \matr{B}_t^{-1} [\grad_{\Sigma}] \matr{B}_t^{-T}$.
					Just as in the original proof, we arrive at the natural gradients $\frac{1}{2} \Diag(\matr{G}), \frac{1}{2} \text{Up}(\matr{G}), \text{Asym}(\matr{G})$, and $\OffDiag(\matr{G})$, for the blocks $\matr{M}_{diag}, \matr{M}_{up}, \matr{M}_{asym}$, and $\matr{M}_{off-diag}$, respectively. 
					We have thus proved the update rule 
					\begin{align*}
						\matr{\mu}_{t+1} &\leftarrow \matr{\mu}_t - \alpha \matr{B}_t^{-1} \matr{B}_t^{-T} \matr{g}_{\matr{\mu}} \\
						\matr{B}_{t+1} &\leftarrow \kappa_{\text{tri-up}} (\matr{B}_t \matr{h} (\alpha \matr{C}_{\text{tri-up}} \otimes \kappa_{\text{tri-up}} (2 \matr{B}_t^{-1} \matr{G}_{\matr{\Sigma}} \matr{B}_t^{-T})))
					\end{align*}
					with 
					\begin{align*}
						\matr{C}_{\text{tri-up}}^{(i, i)} &= \matr{C}_{i, \text{up}} \\
						\matr{C}_{\text{tri-up}}^{(i, i+1)} &= \matr{J}_{i, i+1},
					\end{align*}
					$\matr{C}_{\text{tri-up}} \in \set{M}_{\text{tri-up}} (k)$
				\end{proof}

				\begin{algorithm}[!htbp]
					\DontPrintSemicolon
					\KwInput{$\text{Learning rate } \alpha > 0, \text{ rank } k \in \{0, \dots, d\}, \text{ prior precision } \eta > 0$}
					\myinput{$\text{Regularization } \gamma \ge 0, \text{ damping } \xi \ge 0, \text{ Number of MC samples } M \ge 1$}
					\myinput{$\beta_1, \beta_2 \in (0, 1]$}
					Set $\matr{m} \leftarrow \matr{0}, \matr{\mu} \leftarrow \matr{0}, \matr{\hat{B}} \leftarrow \sqrt{\frac{\gamma \eta}{N} + \xi} \matr{I} $

					\For{$t = 0, 1, 2, \dots $}{

						Randomly sample a minibatch $\set{M}$ from the training set

						Draw $M$ MC samples from the posterior $q(\matr{z} | \matr{\lambda}) = \Normal(\matr{z} | \matr{\mu}, N (\matr{\hat{B}} \matr{\hat{B}}^T)^{-1})$
						$$\matr{z}_m \leftarrow \matr{\mu} + N^{-1/2} \matr{\hat{B}}^{-T} \matr{\varepsilon}_m, \quad \text{ where } \matr{\varepsilon}_m \sim \Normal(\matr{0}, \matr{I}_{d\times d}), \quad \forall m \in \{1, \dots, M\}$$

						Sample $M$ gradient samples using $\matr{z}_m$
						$$\matr{\bar{g}}_m \leftarrow \frac{1}{|\set{M}|} \sum_{i \in \set{M}} \grad_z \ell_i (\matr{z}_m), \quad \forall m \in \{1, \dots, M\} $$

						Compute the gradient for the mean parameter
						$$\matr{g}_{\matr{\mu}} \leftarrow \frac{\gamma \eta}{N} \matr{\mu} + \frac{1}{M} \sum_{m=1}^M \matr{\bar{g}}_m $$

						Update the momentum term
						$$\matr{m} \leftarrow \beta_1 \matr{m} + (1 - \beta_1) \matr{g}_{\matr{\mu}} $$

						Debias momentum and precision
						$$\matr{\bar{m}} \leftarrow \frac{\matr{m}}{1 - \beta_1^{t + 1}}, \quad \matr{\bar{B}} = \frac{\matr{\hat{B}} - \sqrt{\frac{\gamma \eta}{N} + \xi} \matr{I}}{\sqrt{1 - \beta_2^t + \varepsilon}} + \sqrt{\frac{\gamma \eta}{N} + \xi} \matr{I} $$

						Compute the gradient with respect to $\matr{\Sigma}$
						$$\matr{G_{\Sigma}} \leftarrow \frac{\gamma \eta}{N} \matr{I} - \gamma\matr{\hat{B}} \matr{\hat{B}}^T + \frac{N}{2M} \sum_{m=1}^M \left( \matr{K}(\matr{z}_m) + \matr{K}(\matr{z}_m)^T \right), \color{red} \text{(6)} \color{black}$$
						where $\matr{K}(\matr{z}_m) = \matr{\hat{B}} \matr{\hat{B}}^T (\matr{z}_m - \matr{\mu}) \matr{\bar{g}}_m^T$

						Update mean parameter scaled by dampened precision estimate using backwards substitution, then forwards substituation
						$$\matr{\mu} \leftarrow \matr{\mu} - \alpha \matr{\bar{B}}^{-T} \matr{\bar{B}}^{-1} \matr{\bar{m}} $$

						Update square root precision
						$$\matr{\hat{B}} \leftarrow \kappa_{tri-up} (\matr{\hat{B}} \matr{h}((1 - \beta_2) \matr{C}_{\text{tri-up}} \odot \kappa_{\text{tri-up}}(\matr{\hat{B}}^{-1} \matr{G_{\Sigma}} \matr{\hat{B}}^{-T}))) $$
					}
					\caption{Block-Tridiagonal Covariance/Precision Structure Update Rule using Momentum in the Auxiliary/Global Space}
					\label{alg:tridiagonal}
				\end{algorithm}

			\subsubsection{Structured Square-Root Covariance}
				While the proposed Structured NGD update rule of \parencite{LNK+21} in Eqn. (\ref{eqn:SNGD_Rule}) is parametrized in terms of the square-root precision $\matr{B}$ such that $\matr{S} = \matr{B} \matr{B}^T$, they only provide an update for general square-root covariance without any special structure as $\matr{\Sigma} = \matr{A} \matr{A}^T$ with the parametrizations 
				\begin{align*}
					\matr{\tau} &:= \{\matr{\mu}_k \in \Real^d, \matr{\Sigma} \in \set{S}_+^{d \times d}\} \\
					\matr{\lambda} &:= \{\matr{\mu}_k \in \Real^d, \set{A}_k \in \text{GL}^{d \times d} \} \\
					\matr{\eta} &:= \{\matr{\delta}_k \in \Real^d, \set{M}_k \in \set{S}^{d \times d} \},
				\end{align*}
				with auxiliary parameters $\matr{\lambda}_t := (\matr{\mu}_t, \matr{A}_t)$ and mappings 
				\begin{align*}
					\begin{pmatrix} \matr{\mu} \\ \matr{\Sigma} \end{pmatrix} &= \matr{\psi}(\matr{\lambda}) := \begin{pmatrix} \matr{\mu} \\ \matr{A} \matr{A}^T \end{pmatrix} \\
					\begin{pmatrix} \matr{\mu} \\ \matr{A} \end{pmatrix} = \matr{\phi}_{\matr{\lambda}} (\matr{\eta}) := \begin{pmatrix} \matr{\mu} + \matr{A}_t \matr{\delta} \\ \matr{A}_t \text{Exp}(\frac{1}{2} \matr{M})) \end{pmatrix}
				\end{align*}
				where the $\text{Exp} ( \cdot )$ the matrix exponential. 
				However, this mapping is a lot more expensive to compute than the quadratic approximation $\matr{h} (\matr{M})$ used in the previous update rule. 
				We will thus adapt the proofs of the authors to write the update rule in Eqn. (\ref{eqn:SNGD_Rule}) in terms of $\matr{\Sigma}$ as 
				\begin{equation}
					\begin{aligned}
						\matr{\mu}_{t+1} &\leftarrow \matr{\mu}_t - \alpha \matr{\Sigma} \matr{g}_{\mu} \\
						\matr{A}_{t+1} &\leftarrow \matr{A}_t \matr{h} (\alpha \matr{C}_{\text{up}} \odot \kappa_{\text{up}} (2 \matr{A}_t^T \matr{G}_{\matr{\Sigma}} \matr{A}_t)).
					\end{aligned}
				\end{equation}

				\begin{lemma}
					The inverse of the quadratic approximation to the matrix exponential in an open neighborhood around $\matr{M} = \matr{0}$ is
					\begin{equation*}
						\matr{h}(\matr{M})^{-1} \approx \matr{h}(-\matr{M})
					\end{equation*}
				\end{lemma}

				\begin{proof}
					We have 
					\begin{align*}
						\matr{h}(-\matr{M}) \matr{h}(\matr{M}) &= (\matr{I} - \matr{M} + \frac{1}{2} \matr{M}^2) (\matr{I} + \matr{M} + \frac{1}{2} \matr{M}^2) \\
						\quad &+ \matr{I} + \frac{1}{2} \matr{M}^4 = \matr{I} + \BigO(\matr{M}^4)
					\end{align*}
					which finishes the proof.
				\end{proof}

				\begin{lemma}
					\label{lemma:Quadratic_Gradient}
					We have the following expressions:
					\begin{align*}
						(\grad_{M_{ij}} \matr{h}(-\matr{M}))^T \matr{h}(-\matr{M}) &= (-\grad_{M_{ij}} \matr{M}^T + \frac{1}{2} (\grad_{M_{ij}} \matr{M}^T) \matr{M}^T + \frac{1}{2} \matr{M}^T (\grad_{M_{ij}} \matr{M}^T ) + (\grad_{M_{ij}} \matr{M}^T) \matr{M}) + \BigO(\matr{M}^2) (\grad_{M_{ij}} \matr{M}) \\
						(\matr{h}(-\matr{M}))^T (\grad_{M_{ij}} \matr{h}(-\matr{M})) &= (-\grad_{M_{ij}} \matr{M} + \frac{1}{2} \matr{M} (\grad_{M_{ij}} \matr{M}) + \frac{1}{2} (\grad_{M_{ij}} \matr{M}) \matr{M} + \matr{M}^T (\grad_{M_{ij}} \matr{M})) + \BigO(\matr{M}^2) (\grad_{M_{ij}} \matr{M})
					\end{align*}
				\end{lemma}

				\begin{proof}
					We have for the first term
					\begin{align*}
						(\grad_{M_{ij}} \matr{h}(-\matr{M}))^T \matr{h}(-\matr{M}) &= (-\grad_{M_{ij}} \matr{M} + \frac{1}{2} \matr{M} (\grad_{M_{ij}} \matr{M}) + \frac{1}{2} (\grad_{M_{ij}} \matr{M}) \matr{M})^T \matr{h}(\matr{M}) \\
						&= (-\grad_{M_{ij}} \matr{M} + \frac{1}{2} \matr{M} (\grad_{M_{ij}} \matr{M}) + \frac{1}{2} (\grad_{M_{ij}} \matr{M}) \matr{M})^T (\matr{I} + \matr{M} + \BigO(\matr{M}^2)) \\
						&= (-\grad_{M_{ij}} \matr{M}^T + \frac{1}{2} (\grad_{M_{ij}} \matr{M}^T) \matr{M}^T + \frac{1}{2} \matr{M}^T (\grad_{M_{ij}} \matr{M}^T ) + (\grad_{M_{ij}} \matr{M}^T) \matr{M}) + \BigO(\matr{M}^2) (\grad_{M_{ij}} \matr{M}).
					\end{align*}
					We can obtain the second term in an identical fashion. 
				\end{proof}

				\begin{theorem}
					We have for the Fisher matrix with respect to $\matr{M}$:
					\begin{align*}
						\Expect_{q_{\matr{\lambda}}} [\grad_{M_{ij}} \grad_M \log q_{\matr{\lambda}} (\matr{z})] = \grad_{M_{ij}} (\matr{M} + \matr{M}^T)
					\end{align*}
				\end{theorem}

				\begin{proof}
					We have for the log of the variational posterior with parametrization $\matr{\Sigma} = \matr{A} \matr{A}^T$ and mappings $\matr{\psi}, \matr{\phi}$ as above:
					\begin{align*}
						-\log q_{\matr{\lambda}} (\matr{z}) &= -\log q (|\matr{A}_t \matr{h} (\matr{M})|) \\
						&\qquad + \frac{1}{2} (\matr{\mu}_t + \matr{A}_t \matr{\delta} - \matr{z})^T \matr{A}_t^{-T} \matr{h}(\matr{M})^{-T} \matr{h}(\matr{M})^{-1} \matr{A}_t^{-1} (\matr{\mu}_t + \matr{A}_t \matr{\delta} - \matr{z}) + C \\
						&\approx -\log q (|\matr{A}_t \matr{h} (\matr{M})|) \\
						&\qquad +  \frac{1}{2} (\matr{\mu}_t + \matr{A}_t \matr{\delta} - \matr{z})^T \matr{A}_t^{-T} \matr{h}(-\matr{M}) \matr{h}(-\matr{M}) \matr{A}_t^{-1} (\matr{\mu}_t + \matr{A}_t \matr{\delta} - \matr{z}) + C.
					\end{align*}
					Now, set $\matr{Z} = \matr{A}_t^{-1} (\matr{\mu}_t + \matr{A}_t \matr{\delta} - \matr{z}) (\matr{\mu}_t + \matr{A}_t \matr{\delta} - \matr{z})^T \matr{A}_t^{-T}$.
					We can now write
					\begin{align*}
						\quad& \frac{1}{2} \grad_{M_{ij}} [ (\matr{\mu}_t + \matr{A}_t \matr{\delta} - \matr{z})^T \matr{A}_t^{-T} \matr{h}(-\matr{M})^T \matr{h}(-\matr{M}) \matr{A}_t^{-1} (\matr{\mu}_t + \matr{A}_t \matr{\delta} - \matr{z}) ] \\
						&= \frac{1}{2} \grad_{M_{ij}} \tr (\matr{Z} \grad_{M_{ij}} \matr{h}(-\matr{M})^T \matr{h}(-\matr{M})) \\
						&= \frac{1}{2} \grad_{M_{ij}} \tr (\matr{Z} [(\grad_{M_{ij}} \matr{h}(-\matr{M})^T) \matr{h}(-\matr{M}) + \matr{h}(-\matr{M})^T (\grad_{M_{ij}} \matr{h}(-\matr{M}))])
					\end{align*}
					and thus, with Lemma \ref{lemma:Quadratic_Gradient}:
					\begin{align*}
						\quad& \frac{1}{2} \grad_M [ (\matr{\mu}_t + \matr{A}_t \matr{\delta} - \matr{z})^T \matr{A}_t^{-T} \matr{h}(-\matr{M})^T \matr{h}(-\matr{M}) \matr{A}_t^{-1} (\matr{\mu}_t + \matr{A}_t \matr{\delta} - \matr{z}) ] \\
						&= \frac{1}{2} [2 \matr{Q} + \matr{Q} \matr{M}^T + \matr{M}^T \matr{Q} + 2 \matr{M} \matr{Q} + \BigO(\matr{M}^2) \matr{Z} \\
						&= \matr{Z} + (\matr{Z} \matr{M}^T + \matr{M}^T \matr{Z}) / 2 + \matr{M} \matr{Z} + \BigO(\matr{M}^2) \matr{Z},
					\end{align*}
					where $\matr{Q} = (\matr{Z}^T + \matr{Z})/2 = \matr{Z}$.
					With \parencite[Lemma 11]{LNK+21}, one arrives at the result.
				\end{proof}

				For the remainder of the derivation, please consult the original paper \parencite{LNK+21} as the proof are identical.
				With this parametrization in terms of the covariance, one avoids computing an expensive matrix exponential and it becomes much easier to induce a certain structure on the covariance since one is not forced to invert the precision matrix $\matr{S} = \matr{B} \matr{B}^T$ and deduce the implicitly induced structure for $\matr{\Sigma} = \matr{S}^{-1} = \matr{B}^{-T} \matr{B}^{-1}$.

			\subsubsection{Gaussian Mixtures with Structured Covariances}
				Similarly to \parencite{LNK+21} who introduce general, layer-agnostic structure which did not take into account dependencies of specific neural network layers, we can apply our derived algorithms to a Mixture of Gaussians of $K$ components with covariances of potentially different structures,
				\begin{align*}
					q_{\matr{\tau}} (\matr{z}) = \frac{1}{K} \sum_{k=1}^K \Normal (\matr{z} | \matr{\mu}_k, \matr{S}_k^{-1})
				\end{align*}
				with $\matr{\tau} = \{\matr{\mu}_k, \matr{S}_k\}_{k=1}^K$.
				To this end, we will introduce the parametrizations
				\begin{align*}
					\matr{\tau} &:= \{\matr{\mu}_k \in \Real^d, \matr{S} \in \set{S}_+^{d \times d}\}_{k=1}^K \\
					\matr{\lambda} &:= \{\matr{\mu}_k \in \Real^d, \matr{B}_k \in \set{B} \}_{k=1}^K \\
					\matr{\eta} &:= \{\matr{\delta}_k \in \Real^d, \matr{M}_k \in \set{M} \}_{k=1}^K,
				\end{align*}
				where $\set{M}, \set{B}$ can be chosen arbitrarily from the introduced matrix sets defined in this thesis. 
				For different $i$ these can be chosen to be from different structured sets as well. 
				The mappings remain the same as before for a single covariance or precision matrix, respectively, and possibly with a projection $\kappa_{\text{tri-up}}$ for block-tridiagonal square-root precision $\matr{B}$.
				\begin{align*}
					\begin{pmatrix} \matr{\mu}_k \\ \matr{S}_k \end{pmatrix} &= \matr{\psi}_k(\matr{\lambda}_k) := \begin{pmatrix} \matr{\mu}_k \\ \matr{B}_k \matr{B}_k^T \end{pmatrix} \\
					\begin{pmatrix} \matr{\mu}_k \\ \matr{B}_k \end{pmatrix} &= \matr{\phi}_{k, \matr{\lambda}_k} (\matr{\eta}_k) := \begin{pmatrix} \matr{\mu}_k + \matr{B}_{k, t}^{-T} \matr{\delta}_k \\ \kappa_{\text{tri-up}} (\matr{B}_{k, t} \matr{h} (\matr{M}_k)) \end{pmatrix}
				\end{align*}
				for the collection of mappings
				\begin{align*}
					\matr{\psi} (\matr{\lambda}) &= \{ \matr{\psi}_k (\matr{\lambda}_k) \}_{k=1}^K \\
					\matr{\phi}_{\matr{\lambda}_t} (\matr{\eta}) &= \{ \matr{\phi}_{k, \matr{\lambda}_t} (\matr{\eta}_k) \}_{k=1}^K 
				\end{align*}
				As before, natural gradients with respect to the $k$-th Gaussian components $\matr{\delta}_k$ and $\matr{M}_k$ can be derived as 
				\begin{align*}
					\matr{\hat{g}}_{\matr{\delta}_k} = \frac{1}{\pi_k} \matr{B}_{k, t}^{-1} \grad_{\matr{\mu}_k} \mathcal{L}, \quad \matr{\hat{g}}_{\matr{M}_k} = \frac{1}{\pi_k} \matr{C}_{\text{tri-up}} \odot \kappa_{\text{tri-up}} (-2 \matr{B}_t^{-1} \matr{G}_{\matr{\Sigma}_t} \matr{B}_t^{-T} ),
				\end{align*}
				where $\pi_k = \frac{1}{K}$.
				The structured update then becomes 
				\begin{align*}
					\matr{\mu}_{k, t+1} &\leftarrow \matr{\mu}_{k, t} - \frac{\alpha}{\pi_k} \matr{B}_{k, t}^{-1} \matr{B}_{k, t}^{-T} \matr{g}_{\matr{\mu}_k} \\
					\matr{B}_{k, t+1} &\leftarrow \kappa_{\text{tri-up}} \left( \matr{B}_{k, t} \matr{h} \left(\frac{\alpha}{\pi_k} \matr{C}_{\text{tri-up}} \odot \kappa_{\text{tri-up}} (2 \matr{B}_{k, t}^{-1} \matr{G}_{\matr{\Sigma}_k} \matr{B}_{k, t}^{-T}) \right) \right).
				\end{align*}
				Structured NGD with parametrization $\matr{\tau} = \{\matr{\mu}_k, \matr{\Sigma}_k \}_{k=1}^K$ can be obtained by replacing $\matr{B}_{k, t}$ with $\matr{A}_{k, t}$ above and 
				\begin{equation*}
					\matr{\phi}_{k, \matr{\lambda}_t} (\matr{\eta}_k) := \begin{pmatrix} \matr{\mu}_{k, t} + \matr{A}_{k, t} \matr{\delta}_k \\ \matr{A}_{k, t} \matr{h} ( \matr{M}_k )\end{pmatrix}
				\end{equation*}
				as shown in the previous section.

		\subsection{Explicitly Defined Structure}
			As we have argued in this thesis, the structured update rule introduced by \parencite{LNK+21} naturally respects the positive definite constraint on the precision $\matr{S}$ and covariance $\Sigma$, respectively, and the update retains the desired structure which is achieved by the separation into local, auxiliary and global parameter spaces. 
			For explicitly defined structures such that one performs updates only in the global space $\matr{\tau}$, one is presented with multiple challenges that need to be addressed. 
			\begin{enumerate}
				\item
					How can the structure be efficiently represented?
				\item
					How can one sample from the Normal distribution $\Normal(\matr{z} | \matr{\mu}, \matr{\Sigma})$ efficiently?
				\item
					How can the positive definiteness and structure be guaranteed to hold after the update step?
				\item 
					How can the NGD step be efficiently computed in such a way that the Fisher matrix is non-singular?
			\end{enumerate}
			First of all, the update 
			\begin{equation*}
				\matr{S} \leftarrow \matr{S} - \alpha \ngrad_{\matr{S}} \mathcal{L}
			\end{equation*}
			can obviously violate the positive definite constraint or the structure of $\matr{S}$.
			Furthermore, the Fisher matrix $\matr{F}_{\matr{\tau}}$ might be non-singular for two reasons.
			First of all, as shown in \parencite[J.1.6]{LNK+21}, the FIM might be overparametrized but even so, they present an example where the FIM can become singular. 
			Furthermore, it is not clear how one can efficiently solve the linear system $\matr{\hat{g}} = (\matr{F}( \matr{\tau} ) )^{-1} \matr{g}$.
			The first issue can be remedied by limiting the degrees of freedom and adding a sufficiently large damping term $\xi \matr{I}$ to the FIM but the structure needs to be chosen such that the resulting FIM has an easily computable inverse. 
			There does exist a promising approach for block-diagonal and block-tridiagonal covariance matrices.

			\parencite{ZSD+17} propose an update rule 
			\begin{equation*}
				\begin{aligned}
					\matr{\mu}_{t+1} &\leftarrow \matr{\mu}_t - \alpha \left( \matr{F} + \frac{\gamma \eta}{N} \matr{I} \right)^{-1} \left[ \grad \ell(\matr{z}) - \frac{\gamma \eta}{N} \matr{z} \right] \\
					\matr{F} &\leftarrow (1 - \beta) \matr{F} + \beta (\grad \ell(\matr{z})) (\grad \ell(\matr{z}))^T
				\end{aligned}
			\end{equation*}
			where $\matr{F}$ is the approximation to the Fisher matrix which they write as a Kronecker product $\matr{A} \otimes \matr{B}$. 
			Note the inverse matrix in the update for the mean parameter $\matr{\mu}$ is the same as the inverse of the precision matrix $\matr{S}$ as before. 
			\parencite{MG15} present a technique how to approximate this inverse as block-diagonal or block-tridiagonal. 
			We can thus unify both approaches and obtain an explicit structured update. 
			Refer to the original papers for details.

	\section{Experiments}
		We will now evaluate a selection of the presented Structured Natural Gradient methods on CIFAR10 \parencite{KH+09} and Tiny-ImageNet \parencite{LY15}. 
		CIFAR10 is a standard Computer Vision image dataset of $C = 10$ with $N = $\num[group-separator={,}]{50000} RGB images of sizes $32 \times 32$.
		Tiny-ImageNet is a variant of the the larger ImageNet dataset \parencite{DDS+09}.
		While the much larger original consists of 14M $256 \times 256$ RGB images with $C = 1000$ classes, we will limit ourselves to the smaller version which consists of $N = $ \num[group-separator={,}]{100000} images of size $64 \times 64$ and $C = 200$ classes.
		We train a ResNet32 architecture \parencite{HZR+15, I18}on each of these datasets using different optimizers.
		We split the original training data using a 80 -- 20 split for CIFAR10 and a 90 -- 10 split for Tiny-ImageNet and use the smaller set as a holdout validation set for hyperparameter tuning. 
		Our baselines are Vanilla MLE Stochastic Gradient Descent, Temperature Scaling, a Deep Ensemble of $K = 5$ models, Bayes by Backprop (BBB) \parencite{BCK+15, KES22}, and MC Dropout \parencite{GG15}.
		We distinguish between Dropout where Dropout is applied at every layer connection and Last Layer Dropout (LL Dropout) where Dropout is only applied before the very last linear layer. 
		All baselines were trained using SGD with a learning rate $\alpha = 10^{-2}$ which we reduce once the monitored validation loss plateaus. 
		BBB however was trained using Adam and a lower tampering parameter $\gamma = 0.1$.
		Furthermore, we applied Early Stopping to avoid to the training data. 

		For the Structured NGD optimizers, we will present results with block-diagonal covariance with each block being a diagonal matrix plus a rank-$k$ matrix (``RankCov'') and a block arrowhead structure (``Arrowhead'').
		Each of these will be presented for different (rank) parameters $k \in \{0, 1, 3, 5\}$ where the special case $k = 0$ corresponds to a strictly diagonal covariance matrix. 
		We also present results on a block-tridiagonal covariance and precision structure (``Tridiagonal'') with the parameter of the diagonal blocks as $k \in \{0, 1, 3, 5\}$.
		These optimizers are trained with a higher learning rate $\alpha = 10^{-1}$ and damping $\xi = 0.1$ except for the block-tridiagonal models which due to the approximations in the addition of rank matrices $\matr{x} \matr{y}^T$ become much more instable and required a significantly higher damping of $\xi = 0.4$.
		The prior precision was set to $\eta = 0.4$ for all NGD optimizers with a weight decay of $\frac{eta}{N}$ for the baselines since these terms agree in their respective update rules.
		For NGD, the gradient for the mean parameter $\matr{\mu}$ with prior precision $\eta$ is 
		\[
			\matr{g}_{\matr{\mu}} = \frac{\eta}{N} \matr{\mu} + \sum_{m=1}^M \matr{\bar{g}}_m
		\]
		and 
		\[
			\matr{\tilde{g}} = \frac{\eta}{N} \matr{w} + \matr{g}
		\]
		for the baselines with weight decay $\frac{\eta}{N}$.

		For NGD, we also combine the independently trained models into a Gaussian Mixture Model grouped by their structure, i.e., for the rank covariance structure, we take a Gaussian mixture of $K = 4$ components $q_{\matr{\lambda}} (\matr{z}) = \frac{1}{K} \sum_{i=1}^K \Normal(\matr{z} | \matr{\mu}_i, \matr{\Sigma}_i)$ where each component has a covariance of differing parameters $k \in \{0, 1, 3, 5\}$ but the same structure.
		Note that we do not train these models jointly but independently.
		This can be interpreted as a Bayesian Hyper-Deep Ensemble \parencite{WST+20}.

		First, we will show results on ``clean'', uncorrupted data with respect to multiple metrics such as accuracy and calibration. 
		In a second iteration, we will investigate the robustness of these models to common corruptions on their corresponding corrupted version, CIFAR10-C or Tiny-ImageNet-C, respectively. 
		Introduced as a benchmark for model robustness, \parencite{HD19} provide 15 common and 4 extra corruptions to the original uncorrupted datasets of the types \emph{Blur}, \emph{Digital}, \emph{Noise}, \emph{Weather}.
		For an example of these corruptions to a single image from the original ImageNet dataset, see Figure \ref{fig:corruption_types}.
		Each of these comes in 5 different severity levels ranging from minor to severe, see Figure \ref{fig:CIFAR10_corruption_severities} and Figure \ref{fig:ImageNet_corruption_severities} for an example for the corruption \emph{``Impulse Noise''} on CIFAR10 and TinyImageNet for all severities.

		\begin{figure}[H]
			\centering
			\includegraphics[width=0.9\linewidth]{Graphics/corruption_types.pdf}
			\caption{Different Types of Corruption on ImageNet-C \parencite{HD19}}
			\label{fig:corruption_types}
		\end{figure}

		\begin{figure}[H]
			\centering
			\hfill
			\begin{subfigure}{.16\textwidth}
				\centering
				Clean
				\frame{\includegraphics[width=0.99\linewidth]{../code/plots/CIFAR10-C/impulse_noise_0_horse.png}}
			\end{subfigure}%
			\hfill
			\begin{subfigure}{.16\textwidth}
				\centering
				severity = 1
				\frame{\includegraphics[width=0.99\linewidth]{../code/plots/CIFAR10-C/impulse_noise_1_horse.png}}
			\end{subfigure}%
			\hfill
			\begin{subfigure}{.16\textwidth}
				\centering
				severity = 2
				\frame{\includegraphics[width=0.99\linewidth]{../code/plots/CIFAR10-C/impulse_noise_2_horse.png}}
			\end{subfigure}%
			\hfill
			\begin{subfigure}{.16\textwidth}
				\centering
				severity = 3
				\frame{\includegraphics[width=0.99\linewidth]{../code/plots/CIFAR10-C/impulse_noise_3_horse.png}}
			\end{subfigure}%
			\hfill
			\begin{subfigure}{.16\textwidth}
				\centering
				severity = 4
				\frame{\includegraphics[width=0.99\linewidth]{../code/plots/CIFAR10-C/impulse_noise_4_horse.png}}
			\end{subfigure}%
			\hfill
			\begin{subfigure}{.16\textwidth}
				\centering
				severity = 5
				\frame{\includegraphics[width=0.99\linewidth]{../code/plots/CIFAR10-C/impulse_noise_5_horse.png}}
			\end{subfigure}%
			\caption{CIFAR10-C example with \emph{``Impulse Noise''} for different severities \parencite{HD19}}
			\label{fig:CIFAR10_corruption_severities}
		\end{figure}

		\begin{figure}[H]
			\centering
			\hfill
			\begin{subfigure}{.16\textwidth}
				\centering
				Clean
				\frame{\includegraphics[width=0.99\linewidth]{../code/plots/Tiny-ImageNet-C/impulse_noise_0_val_1551.png}}
			\end{subfigure}%
			\hfill
			\begin{subfigure}{.16\textwidth}
				\centering
				severity = 1
				\frame{\includegraphics[width=0.99\linewidth]{../code/plots/Tiny-ImageNet-C/impulse_noise_1_val_1551.png}}
			\end{subfigure}%
			\hfill
			\begin{subfigure}{.16\textwidth}
				\centering
				severity = 2
				\frame{\includegraphics[width=0.99\linewidth]{../code/plots/Tiny-ImageNet-C/impulse_noise_2_val_1551.png}}
			\end{subfigure}%
			\hfill
			\begin{subfigure}{.16\textwidth}
				\centering
				severity = 3
				\frame{\includegraphics[width=0.99\linewidth]{../code/plots/Tiny-ImageNet-C/impulse_noise_3_val_1551.png}}
			\end{subfigure}%
			\hfill
			\begin{subfigure}{.16\textwidth}
				\centering
				severity = 4
				\frame{\includegraphics[width=0.99\linewidth]{../code/plots/Tiny-ImageNet-C/impulse_noise_4_val_1551.png}}
			\end{subfigure}%
			\hfill
			\begin{subfigure}{.16\textwidth}
				\centering
				severity = 5
				\frame{\includegraphics[width=0.99\linewidth]{../code/plots/Tiny-ImageNet-C/impulse_noise_5_val_1551.png}}
			\end{subfigure}%
			\caption{Tiny-ImageNet-C example with \emph{``Impulse Noise''} for different severities \parencite{HD19}}
			\label{fig:CIFAR10_corruption_severities}
		\end{figure}

		The results on the in-distribution, uncorrupted test data are shown in Table \ref{table:CIFAR10_clean} and Table \ref{table:ImageNet_clean}.
		NGD with diagonal and block-diagonal covariance structure is approximately 10\% slower than Vanilla SGD and 40\% slower for the block-tridiagonal covariance structure.
		% The Brier Score \parencite{B50} is defined as 
		% \begin{equation}
		% 	BS = \frac{1}{N} \sum_{i=1}^N \sum_{j=1}^C (f(\matr{x}_i)_j - y_{ij})^2
		% \end{equation}
		% There exists an interpretation $BS = uncertainty - resolution + reliability$ \parencite{OFR+19}

		On CIFAR-10, we observe that all NGD methods significantly outperform the Vanilla SGD method in terms of Top-1 error and expected calibration error (ECE). 
		In terms of accuracy, there does not seem to be a significant difference between the block-diagonal rank covariance and the block-tridiagonal structure. 
		The arrowhead structure does perform worse compared to these structures.
		Surprisingly, BBB underperforms in both accuracy and ECE compared to all other models despite the changes made in the optimization methodology.  
		Temperature Scaling performs similarly to Vanilla SGD in terms of accuracy but as expected much better with respect to ECE, comparable to the rank covariance parametrization.
		Dropout and LL Dropout show a kind of tradeoff between accuracy and ECE where higher error results in lower ECE. 
		Standard Dropout performs much better with respect to ECE here. 
		For the ensemble methods Deep Ensemble and the GMMs, we observe that bagging independently trained models and averaging their individual component outputs does significantly improve their overall accuracy, especially for the ensemble of independently trained Vanilla SGD models.
		Surprisingly, overall calibration error seems to deteriorate for the GMMs, especially for the block-tridiagonal structure. 
		With respect to expected uncertainty calibration error (UCE), we see that the rank covariance structure models significantly outperform all other models but not so for the other NGD-based methods. 
		Temperature Scaling and the Deep Ensemble also significantly improve Vanilla UCE. 
		While ECE is superior to Vanilla SGD for the block-tridiagonal and GMMs, we see an inverse relationship in terms of UCE for these models. 

		On the much larger Tiny-ImageNet dataset with many more classes, all other NGD-based models perform similarly to Vanilla SGD in terms of accuracy but better in terms of ECE, although now the strong baselines Temperature Scaling and Deep Ensemble outperform them in terms of ECE and accuracy, respectively. 
		BBB significantly improves UCE but at the cost of much worse accuracy which underlines the improved convergence properties of NGD compared to naive GD of the negative ELBO loss $\mathcal{L}$. 
		Again, we see that LL Dropout only slightly improves ECE and UCE, while full Dropout does so significantly.
		For this dataset, there does not seem to be a significant difference in terms of ECE and UCE of the NGD-based models for the different structures or parameters $k$.

		Overall, we observe that Temperature Scaling with negligible overhead significantly improves calibration error as well as Deep Ensemble although this requires a much longer training effort since $K = 5$ models need to be trained. 
		We can see that there does not seem to be a convincing benefit in the increased parameter $k$ or block capacity of the covariance matrix from fully diagonal to block-diagonal to block-tridiagonal.
		For the smaller CIFAR10 dataset, the NGD-based methods are much more competitive than on the larger Tiny-ImageNet dataset. 
		This could be an indication that they are more suited for smaller dataset, however since \parencite{HD19} did not provide a corrupted version of a suitable smaller dataset and our focus also lies on robustness to corruptions, we opted to train on these datasets only. 
		However, it might be interesting to investigate the performance on a dataset such as STL10 \parencite{CNL11} and extend the corruption severities of \parencite{HD19} to this dataset for future research.

		We will now more closely investigate the confidence and predictive uncertainty distribution for a selection of the trained models. 

		\begin{table}[H]
			\centering
			\begin{tabular}{LLLLL}
				\toprule
				\rowfont{\tiny}%
				\textbf{Method} & \textbf{NLL} & \textbf{Top-1 Error} & \textbf{ECE} & \textbf{UCE} \\ 
				\midrule\midrule
				Vanilla & 0.4 & 12.3 & 6.2 & 5.2 \\ 
				Temp Scaling & \textbf{0.36} (-17.7\%) & 12.41 (+0.6\%) & 1.89 (-69.4\%) & 1.03 (-80.2\%) \\ 
				BBB & 1.43 (+225.5\%) & 21.63 (+75.4\%) & 20.93 (+239.1\%) & 32.71 (+526.3\%) \\ 
				LL Dropout & 0.43 (-1.2\%) & 12.45 (+1.0\%) & 5.76 (-6.7\%) & 4.58 (-12.3\%) \\ 
				Dropout & 0.52 (+17.9\%) & 14.14 (+14.7\%) & 2.23 (-63.8\%) & 6.00 (+14.9\%) \\ 
				Deep Ensemble & 0.43 (-1.5\%) & 9.17 (-25.6\%) & 1.43 (-76.9\%) & 2.53 (-51.6\%) \\ 
				\midrule
				NGD (Diag, $k = 0$) & 0.43 (-1.6\%) & 10.75 (-12.8\%) & 1.73 (-72.0\%) & \textbf{0.82} (-84.4\%) \\ 
				NGD (RC, $k = 1$) & 0.46 (+5.0\%) & 11.96 (-3.0\%) & \textbf{1.28} (-79.2\%) & 1.26 (-75.9\%) \\ 
				NGD (RC, $k = 3$) & 0.44 (+0.4\%) & 11.15 (-9.6\%) & 1.68 (-72.8\%) & 1.04 (-80.0\%) \\ 
				NGD (RC, $k = 5$) & 0.43 (-1.0\%) & 11.40 (-7.5\%) & 2.39 (-61.2\%) & 0.95 (-81.8\%) \\ 
				\midrule
				NGD (AH, $k = 1$) & 0.44 (-0.7\%) & 11.33 (-8.1\%) & 2.68 (-56.5\%) & 1.23 (-76.5\%) \\ 
				NGD (AH, $k = 3$) & 0.66 (+49.4\%) & 13.02 (+5.6\%) & 5.18 (-16.1\%) & 9.65 (+84.7\%) \\ 
				NGD (AH, $k = 5$) & 0.45 (+1.9\%) & 11.45 (-7.1\%) & 2.29 (-62.9\%) & 1.31 (-74.9\%) \\ 
				\midrule
				NGD (TD, $k = 0$) & 0.61 (+38.6\%) & 11.37 (-7.8\%) & 2.89 (-53.2\%) & 6.01 (+15.1\%) \\ 
				NGD (TD, $k = 1$) & 0.62 (+41.0\%) & 11.75 (-4.7\%) & 3.19 (-48.4\%) & 6.40 (+22.6\%) \\ 
				NGD (TD, $k = 3$) & 0.60 (+37.4\%) & 10.96 (-11.1\%) & 4.01 (-35.1\%) & 7.45 (+42.6\%) \\ 
				NGD (TD, $k = 5$) & 0.60 (+36.6\%) & 11.63 (-5.7\%) & 3.07 (-50.2\%) & 6.18 (+18.3\%) \\ 
				\midrule
				GMM (RC) & 0.44 (+0.5\%) & 9.07 (-26.4\%) & 2.54 (-58.8\%) & 5.05 (-3.4\%) \\ 
				GMM (AH) & 0.48 (+9.9\%) & \textbf{9.03} (-26.8\%) & 3.90 (-36.9\%) & 7.45 (+42.7\%) \\ 
				GMM (TD) & 0.60 (+35.6\%) & 9.44 (-23.4\%) & 6.22 (+0.8\%) & 10.43 (+99.8\%) \\ 
				\bottomrule
			\end{tabular}
			\caption{Results on CIFAR10 for different Methods and Structures ``Rank Covariance'', ``Arrowhead'', and ``Tridiagonal''}
			\label{table:CIFAR10_clean}
		\end{table}

		% Tiny ImageNet uncorrupted results
		\begin{table}[H]
			\centering
			\begin{tabular}{LLLLLL}
				\toprule
				\rowfont{\tiny}%
				\textbf{Method} & \textbf{NLL} & \textbf{Acc.} & \textbf{ECE} & \textbf{UCE} & \textbf{Top-5 Accuracy} \\ 
				\midrule\midrule
				Vanilla & 2.3 & 48.4 & 13.8 & 26.5 & 74.6 \\ 
				Temp Scaling & \textbf{2.13} (-8.8\%) & 48.98 (+1.2\%) & \textbf{1.28} (-90.7\%) & 13.86 (-47.6\%) & 74.81 (+0.3\%) \\ 
				BBB & 3.91 (+67.6\%) & 30.98 (-36.0\%) & 12.47 (-9.9\%) & \textbf{4.87} (-81.6\%) & 57.61 (-22.7\%) \\ 
				LL Dropout & 2.34 (+0.4\%) & 48.63 (+0.5\%) & 12.08 (-12.7\%) & 25.07 (-5.3\%) & 74.57 ($\pm$0.0\%) \\ 
				Dropout & 2.48 (+6.1\%) & 47.70 (-1.4\%) & 1.87 (-86.5\%) & 12.98 (-51.0\%) & 73.17 (-1.9\%) \\ 
				Deep Ensemble & 2.32 (-0.8\%) & \textbf{56.71} (+17.2\%) & 4.62 (-66.6\%) & 10.65 (-59.8\%) & \textbf{79.13} (+6.1\%) \\ 
				\midrule
				NGD (Diag, $k = 0$) & 2.27 (-2.6\%) & 50.11 (+3.6\%) & 9.65 (-30.3\%) & 22.73 (-14.1\%) & 76.10 (+2.1\%) \\ 
				NGD (RC, $k = 1$) & 2.31 (-1.0\%) & 47.94 (-0.9\%) & 8.48 (-38.7\%) & 22.26 (-15.9\%) & 74.22 (-0.5\%) \\ 
				NGD (RC, $k = 3$) & 2.28 (-2.4\%) & 48.68 (+0.6\%) & 8.35 (-39.7\%) & 21.91 (-17.2\%) & 74.56 (-0.0\%) \\ 
				NGD (RC, $k = 5$) & 2.30 (-1.5\%) & 48.77 (+0.8\%) & 8.20 (-40.7\%) & 21.78 (-17.7\%) & 74.75 (+0.2\%) \\ 
				\midrule
				NGD (AH, $k = 1$) & 2.35 (+0.5\%) & 46.59 (-3.7\%) & 7.87 (-43.1\%) & 21.84 (-17.5\%) & 72.98 (-2.1\%) \\ 
				NGD (AH, $k = 3$) & 2.30 (-1.6\%) & 48.04 (-0.7\%) & 8.22 (-40.6\%) & 21.85 (-17.5\%) & 73.90 (-0.9\%) \\ 
				NGD (AH, $k = 5$) & 2.35 (+0.7\%) & 47.53 (-1.8\%) & 8.23 (-40.5\%) & 22.09 (-16.6\%) & 73.82 (-1.0\%) \\ 
				\midrule
				NGD (TD, $k = 0$) & 2.37 (+1.3\%) & 49.07 (+1.4\%) & 8.39 (-39.4\%) & 22.12 (-16.5\%) & 74.68 (+0.1\%) \\ 
				NGD (TD, $k = 1$) & 2.41 (+3.0\%) & 48.10 (-0.6\%) & 9.51 (-31.3\%) & 23.35 (-11.8\%) & 74.42 (-0.2\%) \\ 
				NGD (TD, $k = 3$) & 2.40 (+2.9\%) & 48.67 (+0.6\%) & 8.94 (-35.4\%) & 22.71 (-14.2\%) & 74.71 (+0.2\%) \\ 
				NGD (TD, $k = 5$) & 2.38 (+1.8\%) & 47.84 (-1.1\%) & 5.78 (-58.3\%) & 20.07 (-24.2\%) & 74.55 (-0.0\%) \\ 
				\bottomrule
			\end{tabular}
			\caption{Results on ImageNet for different Methods and Structures ``Rank Covariance'', ``Arrowhead'', and ``Tridiagonal''}
			\label{table:ImageNet_clean}
		\end{table}

		\subsection{Calibration}
			Recall that a neural network outputs a multiclass probability distribution with the top-label $\hat{y}$ as an output with confidence $\hat{p}$ which is often interpreted as a model's confidence in its prediction. 
			However, this interpretation assumes perfect model calibration, i.e., if we have a confidence of $\hat{p} = 0.8$ for 100 predictions, we would expect it to output the correct label 80\% of the time \parencite{GPS17}. 
			In other words, we expect model accuracy to increases as a linear function of its associated confidence. 
			One can visualize this relationship in a reliability diagram. 

			In Figure \ref{fig:CIFAR10_ReliabilityDiagrams}, we see a selection of reliability diagrams with 10 bins for CIFAR10.
			We see that with the exception of BBB, all methods have very sharp and confident output probabilities seen in the overwhelming majority of confidences falling in the right-most bin which is somewhat expected for modern neural networks trained with an NLL loss function.
			BBB exhibits quite large gaps between its binned confidence and accuracy with much more evenly distributed confidences. 
			While the single methods are all slightly overconfident, the ensemble methods Deep Ensemble and GMM show slight underconfidence and slightly more spread out confidence histograms. 

			In Figure \ref{fig:ImageNet_ReliabilityDiagrams}, we see a selection of reliability diagrams with 10 bins for Tiny-ImageNet.
			Again, BBB exhibits large gaps between its confidence and accuracy output.
			For this dataset, all models are now slightly underconfident with much more evenly spaced confidences overall. 
			As shown in Table \ref{table:ImageNet_clean}, Temperature Scaling significantly improves calibration error and outperforms all other methods in this metric.
			We can again observe slight underconfidence for the Deep Ensemble method.

			We will now investigate the uncertainty callibration diagrams which show top-1 error as a function of normalized predictive uncertainty or entropy \parencite{KT20}
			\begin{equation*}
				\tilde{\mathcal{H}}(\hat{p}) = - \frac{1}{\log C} \sum_{i=1}^C \hat{p}_i \log(\hat{p}_i) \in [0, 1].
			\end{equation*}
			For a perfectly calibrated model, we would expect a model with higher uncertainty to have higher error as well. 
			This allows us to define a similar metric to ECE as 
			\begin{equation*}
				\text{UCE} = \sum_{\ell=1}^{B} \frac{|B_{\ell}|}{N} |\text{err}(B_{\ell}) - \text{uncert}(B_{\ell})|,
			\end{equation*}
			where $B_{\ell} = (\frac{\ell-1}{B}, \frac{\ell}{B}]$ the equally spaced bins and 
			\begin{align*}
				\text{err}(B_{\ell}) := \frac{1}{|B_{\ell}|} \sum_{i \in B_{\ell}} \mathbbm{1}_{\hat{y}_i \neq y_i}, \quad \text{uncert}(B_{\ell}) := \frac{1}{|B_{\ell}|} \sum_{i \in B_{\ell}} \hat{\matr{u}}_i
			\end{align*}
			the binned top-1 error and normalized predictive uncertainty, respectively. 

			The uncertainty diagrams on the clean data are shown in Figures \ref{fig:CIFAR_UncertaintyDiagrams} and \ref{fig:ImageNet_UncertaintyDiagrams}.
			We observe overall sharp entropy outputs in the left most bin and low calibration error for Temperature Scaling on CIFAR10.
			Again, NGD and Deep Ensemble show slight improvements in UCE but no improvement or even an inferior calibration error can be observed for the GMMs. 
			BBB shows a much more evenly distributed confidence histogram with high calibration error.
			
			On the Tiny-ImageNet dataset, we find that the predictive uncertainty is a lot more evenly spaced out.
			We see only a slight improvement in UCE from the NGD-based methods with more significant improvement for Temperature Scaling and Deep Ensemble. 

			% Reliability Diagrams for CIFAR10
			\begin{figure}[H]
				\centering
				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/reliability_diagrams/Vanilla_SGD.pdf}}}
				\end{subfigure}%
				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/reliability_diagrams/Temp Scaling_SGD.pdf}}}
				\end{subfigure}%
				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/reliability_diagrams/BBB_Adam.pdf}}}
				\end{subfigure}
				
				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/reliability_diagrams/NGD (Diagonal, $k = 0$)_NGD (Diagonal, k = 0, M = 1, gamma = 1.0).pdf}}}
				\end{subfigure}%
				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/reliability_diagrams/NGD (RankCov, $k = 3$)_NGD (RankCov, k = 3, M = 1, gamma = 1.0).pdf}}}
				\end{subfigure}%
				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/reliability_diagrams/NGD (RankCov, $k = 5$)_NGD (RankCov, k = 5, M = 1, gamma = 1.0).pdf}}}
				\end{subfigure}

				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/reliability_diagrams/Deep Ensemble_SGD.pdf}}}
				\end{subfigure}%
				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/reliability_diagrams/GMM (RankCov)_SGD.pdf}}}
				\end{subfigure}%
				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/reliability_diagrams/GMM (Tridiagonal)_SGD.pdf}}}
				\end{subfigure}
				\caption{Selection of Reliability Diagrams for Baselines Vanilla SGD, Temperature Scaling, BBB, NGD with Rank Covariance Structure, Deep Ensemble, and GMMs with Block-Diagonal Rank and Block-Tridiagonal Covariance for CIFAR10}
				\label{fig:CIFAR10_ReliabilityDiagrams}
			\end{figure}

			% Reliability Diagrams for Tiny-ImageNet
			\begin{figure}
				\centering
				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/reliability_diagrams/Vanilla_SGD.pdf}}}
				\end{subfigure}%
				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/reliability_diagrams/Temp Scaling_SGD.pdf}}}
				\end{subfigure}%
				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/reliability_diagrams/BBB_Adam.pdf}}}
				\end{subfigure}

				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/reliability_diagrams/Deep Ensemble_SGD.pdf}}}
				\end{subfigure}%
				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/reliability_diagrams/NGD (Diagonal, $k = 0$)_NGD (Diagonal, k = 0, M = 1, gamma = 1.0).pdf}}}
				\end{subfigure}%
				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/reliability_diagrams/NGD (RankCov, $k = 5$)_NGD (RankCov, k = 5, M = 1, gamma = 1.0).pdf}}}
				\end{subfigure}%

				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/reliability_diagrams/NGD (Arrowhead, $k = 5$)_NGD (Arrowhead, k = 5, M = 1, gamma = 1.0).pdf}}}
				\end{subfigure}
				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/reliability_diagrams/NGD (Tridiagonal, $k = 0$)_NGD (Tridiagonal, k = 0, M = 1, gamma = 1.0).pdf}}}
				\end{subfigure}%
				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/reliability_diagrams/NGD (Tridiagonal, $k = 5$)_NGD (Tridiagonal, k = 5, M = 1, gamma = 1.0).pdf}}}
				\end{subfigure}
				\caption{Selection of Reliability Diagrams for Baselines Vanilla SGD, Temperature Scaling, BBB, NGD with Rank Covariance Structure, Deep Ensemble, and GMMs with Block-Diagonal Rank and Block-Tridiagonal Covariance for Tiny-ImageNet}
				\label{fig:ImageNet_ReliabilityDiagrams}
			\end{figure}

			% Uncertainty Diagrams for CIFAR10
			\begin{figure}[H]
				\centering
				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/uncertainty_diagrams/Vanilla_SGD.pdf}}}
				\end{subfigure}%
				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/uncertainty_diagrams/Temp Scaling_SGD.pdf}}}
				\end{subfigure}%
				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/uncertainty_diagrams/BBB_Adam.pdf}}}
				\end{subfigure}
				
				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/uncertainty_diagrams/NGD (Diagonal, $k = 0$)_NGD (Diagonal, k = 0, M = 1, gamma = 1.0).pdf}}}
				\end{subfigure}%
				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/uncertainty_diagrams/NGD (RankCov, $k = 3$)_NGD (RankCov, k = 3, M = 1, gamma = 1.0).pdf}}}
				\end{subfigure}%
				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/uncertainty_diagrams/NGD (RankCov, $k = 5$)_NGD (RankCov, k = 5, M = 1, gamma = 1.0).pdf}}}
				\end{subfigure}

				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/uncertainty_diagrams/Deep Ensemble_SGD.pdf}}}
				\end{subfigure}%
				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/uncertainty_diagrams/GMM (RankCov)_SGD.pdf}}}
				\end{subfigure}%
				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/uncertainty_diagrams/GMM (Tridiagonal)_SGD.pdf}}}
				\end{subfigure}
				\caption{Selection of Uncertainty Diagrams for Baselines Vanilla SGD, Temperature Scaling, BBB, NGD with Rank Covariance Structure, Deep Ensemble, and GMMs with Block-Diagonal Rank and Block-Tridiagonal Covariance for CIFAR10}
				\label{fig:CIFAR10_UncertaintyDiagrams}
			\end{figure}

			% Uncertainty Diagrams for Tiny-ImageNet
			\begin{figure}[H]
				\centering
				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/uncertainty_diagrams/Vanilla_SGD.pdf}}}
				\end{subfigure}%
				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/uncertainty_diagrams/Temp Scaling_SGD.pdf}}}
				\end{subfigure}%
				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/uncertainty_diagrams/BBB_Adam.pdf}}}
				\end{subfigure}

				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/uncertainty_diagrams/Deep Ensemble_SGD.pdf}}}
				\end{subfigure}%
				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/uncertainty_diagrams/NGD (Diagonal, $k = 0$)_NGD (Diagonal, k = 0, M = 1, gamma = 1.0).pdf}}}
				\end{subfigure}%
				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/uncertainty_diagrams/NGD (RankCov, $k = 5$)_NGD (RankCov, k = 5, M = 1, gamma = 1.0).pdf}}}
				\end{subfigure}%

				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/uncertainty_diagrams/NGD (Arrowhead, $k = 5$)_NGD (Arrowhead, k = 5, M = 1, gamma = 1.0).pdf}}}
				\end{subfigure}
				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/uncertainty_diagrams/NGD (Tridiagonal, $k = 0$)_NGD (Tridiagonal, k = 0, M = 1, gamma = 1.0).pdf}}}
				\end{subfigure}%
				\begin{subfigure}{.3\textwidth}
					\centering
					\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/uncertainty_diagrams/NGD (Tridiagonal, $k = 5$)_NGD (Tridiagonal, k = 5, M = 1, gamma = 1.0).pdf}}}
				\end{subfigure}
				\caption{Selection of Reliability Diagrams for Baselines Vanilla SGD, Temperature Scaling, BBB, NGD with Rank Covariance Structure, Deep Ensemble, and GMMs with Block-Diagonal Rank and Block-Tridiagonal Covariance for Tiny-ImageNet}
				\label{fig:ImageNet_UncertaintyDiagrams}
			\end{figure}

		\subsection{Robustness}
			We will now turn our attention to the performance of the methods on the corrupted datasets CIFAR10-C and Tiny-ImageNet-C, respectively. 
			One only trains on the original dataset and then evaluates on these as a hold-out test set.
			A single corruption from one of 5 severities is applied to an uncorrupted input from the original corresponding dataset. 
			We can then evaluate the models grouped by severity.
			\parencite{HD19} also introduce a summary statistic called mean corruption error (mCE) and relative mean corruption error (Rel. mCE) by taking the top-1 error for each corruption $c$ and severity $s$ and summing them up and comparing it to a baseline.
			Rel. mCE works similarly but it measures the deviation of the top-1 error on the corrupted dataset from the clean accuracy. 
			While they introduced this metric to compare different models to each other, we will only limit ourselves to the ResNet32 architecture as described above and compare different training methods as follows
			\begin{align*}
				\text{mCE}_{\text{method}} &= \frac{\sum_{c, s} \text{err}_{\text{method}}}{ \sum_{c, s} \text{err}_{\text{baseline}}} \\
				\text{Rel. mCE}_{\text{method}} &= \frac{\sum_{c, s} (\text{err}_{\text{method} - \text{err}_{\text{method}}(0, \text{clean}))}}{ \sum_{c, s} (\text{err}_{\text{baseline}} - \text{err}_{\text{baseline}}(0, \text{clean}))}.
			\end{align*}
			Thus, for the baseline Vanilla SGD, we have $\text{mCE}_{\text{baseline}} = \text{Rel. mCE}_{\text{baseline}} = 1.0$.
			The lower the corruption error, the better and the more robust the method. 
			There are some shortcomings to this approach since it places equal probability on all corruptions and focusses only on accuracy but completely leaves out many other important metrics such as ECE. 

			For each severity, we can show the performance of the different methods with respect to a different metric in a shift intensity diagram. 
			We expect to see a steady decline in performance as the severity increases and hopefully some methods show a slower decline. 
			Results on CIFAR10-C are shown in Figure \ref{fig:CIFAR10_ShiftIntensity} and results on Tiny-ImageNet-C in Figure \ref{fig:ImageNet_ShiftIntensity}.
			The width of the error bars denotes the distribution of the metric on the corrupted dataset for all corruptions.
			Find the quantitative evaluation in Tables \ref{table:CIFAR10_ShiftIntensity} and \ref{table:ImageNet_ShiftIntensity}.
			We can clearly see a steady decline in performance for all methods with respect to accuracy with no discernible differences between them. 
			However, in terms of ECE, we see that while Temperature Scaling improves calibration error on the uncorrupted dataset, it does not increase its robustness under severity shift and it quickly deteriorates. 
			The diagonal and block-diagonal rank covariance and arrowhead structures for the NGD-based methods perform equally well with the exception of the arrowhead structure for $k = 3$
			Surprisingly, we can observe comparable performance for more intense severities when compared to the Deep Ensemble. 
			The block-tridiagonal covariance structure does show a more robust generalization which suggests that the higher covariance capacity allows the model to represent more complex uncertainty. 
			These methods are approximately equal in ECE compared to the GMMs. 
			We can also observe that full MC Dropout is more robust to corruptions than the single layer LL Dropout. 

			On Tiny-ImageNet-C, we observe a similar steady decline in accuracy.
			For ECE, we can see that again, full MC Dropout is more robust than LL Dropout. 
			While BBB has the lowest ECE overall, it comes at the cost of severely lower accuracy. 
			On this dataset, we can observe now that all NGD-based methods perform equally and Temperature Scaling and Deep Ensemble serve as very strong baselines that our methods are unable to beat. 

			Sorted by clean accuracy on the $x$-axis and with mCE and Rel. mCE, respectively on the $y$-axis, we can compare the different methods in terms of their accuracy robustness to corruptions.
			The results can be found in Figures \ref{fig:CIFAR10_Robustness} and \ref{fig:ImageNet_Robustness}

			% Shift Intensity Diagrams for CIFAR10
			\begin{figure}			
				\centering
				\begin{subfigure}{\textwidth}
					\centering
					\includegraphics[width=0.9\linewidth]{{../code/plots/CIFAR10/corrupted/shift_intensity/all_Accuracy.pdf}}
				\end{subfigure}\\
				\begin{subfigure}{\textwidth}
					\centering
					\includegraphics[width=0.9\linewidth]{{../code/plots/CIFAR10/corrupted/shift_intensity/all_ECE.pdf}}
				\end{subfigure}
				\caption{Shift Intensity Diagram for CIFAR10-C for Accuracy and ECE}
				\label{fig:CIFAR10_ShiftIntensity}
			\end{figure}

			% Shift Intensity Diagrams for Tiny-ImageNet
			\begin{figure}			
				\centering
				\begin{subfigure}{\textwidth}
					\centering
					\includegraphics[width=0.9\linewidth]{{../code/plots/ImageNet/corrupted/shift_intensity/all_Accuracy.pdf}}
				\end{subfigure}\\
				\begin{subfigure}{\textwidth}
					\centering
					\includegraphics[width=0.9\linewidth]{{../code/plots/ImageNet/corrupted/shift_intensity/all_ECE.pdf}}
				\end{subfigure}\\
				\begin{subfigure}{\textwidth}
					\centering
					\includegraphics[width=0.9\linewidth]{{../code/plots/ImageNet/corrupted/shift_intensity/all_UCE.pdf}}
				\end{subfigure}
				\caption{Shift Intensity Diagram for Tiny-ImageNet-C for Metrics Accuracy, ECE, and UCE}
				\label{fig:ImageNet_ShiftIntensity}
			\end{figure}

			% Robustness for CIFAR10
			\begin{figure}			
				\centering
				\includegraphics[width=0.9\linewidth]{{../code/plots/CIFAR10/corrupted/robustness/mCE_all.pdf}}
				\caption{Accuracy Robustness of different Methods on CIFAR10}
				\label{fig:CIFAR10_Robustness}
			\end{figure}

			% Robustness for Tiny-ImageNet
			\begin{figure}			
				\centering
				\includegraphics[width=0.9\linewidth]{{../code/plots/ImageNet/corrupted/robustness/mCE_all.pdf}}
				\caption{Accuracy Robustness of different Methods on CIFAR10}
				\label{fig:ImageNet_Robustness}
			\end{figure}

		\subsection{Ablation Study}
			In this section, we will present a short ablation study on CIFAR10 for a selection of parameters for the structured NGD algorithms with rank covariance.
			These parameters are the rank parameter $k$, the number of MC samples during training $M$, and the tampering parameter in the negative ELBO loss $\mathcal{L}$, $\gamma$. 
			Only a single parameter is changed at a time, so assume the other parameters to be set to the standard values $M = 1$ and $\gamma = 1.0$ if not otherwise specified. 
			We ran experiments for $k \in \{0, 1, 3, 5\}, \quad M \in \{1, 3, 5\}$, and $\gamma \in \{10^{-2}, 10^{-1}, 1\}$, respectively. 
			The results are shown in Figures \ref{fig:k_ablation_study}, \ref{fig:M_ablation_study}, and \ref{fig:gamma_ablation_study}.

			We cannot observe any improvement in any metric with increasing rank parameter $k$. 
			We can also not find a conclusive relationship between increased covariance capacity when going from block-diagonal to block-tridiagonal.
			In terms of the number of MC samples during training, $M$, we find that for larger values, accuracy increases as well. 
			This might be because the variance of the gradient estimate $\matr{\bar{g}} = \frac{1}{M} \sum_{m=1}^M \matr{g}_m$ is reduced and the steps are of higher quality. 
			However, note that taking a gradient samples $\matr{g}_m$ involves a forward and backward pass through the entire model which is where the majority of the time for the update step is spent. 
			Thus, increasing $M$ significantly increases training time. 
			We can also see that while accuracy increases, ECE, mean corruption error (mCE), and relative mean corruption error (Rel. mCE) also increase. 
			Recall that these metrics are all subject to minimization, the lower the better.
			That is, calibration and robustness deteriorate.
			We find that for increasing values of $\gamma$, accuracy increases and calibration as well as robustness improve. 

			\begin{figure}[H]
				\centering
				\begin{subfigure}{.3\textwidth}
					\centering
					\includegraphics[width=0.9\linewidth]{../code/plots/CIFAR10/results_parameters/k_Accuracy.pdf}
				\end{subfigure}%
				\begin{subfigure}{.3\textwidth}
					\centering
					\includegraphics[width=0.9\linewidth]{../code/plots/CIFAR10/results_parameters/k_ECE.pdf}
				\end{subfigure}%
				\begin{subfigure}{.3\textwidth}
					\centering
					\includegraphics[width=0.9\linewidth]{../code/plots/CIFAR10/results_parameters/k_meanCE_legend.pdf}
				\end{subfigure}
				\caption{Influence of Regularization Parameter $k$ on Accuracy, ECE, and mCE for $k \in \{0, 1, 3, 5\}$}
				\label{fig:k_ablation_study}
			\end{figure}

			\begin{figure}[H]
				\centering
				\begin{subfigure}{.3\textwidth}
					\centering
					\includegraphics[width=0.9\linewidth]{../code/plots/CIFAR10/results_parameters/M_Accuracy.pdf}
				\end{subfigure}%
				\begin{subfigure}{.3\textwidth}
					\centering
					\includegraphics[width=0.9\linewidth]{../code/plots/CIFAR10/results_parameters/M_ECE.pdf}
				\end{subfigure}%
				\begin{subfigure}{.3\textwidth}
					\centering
					\includegraphics[width=0.9\linewidth]{../code/plots/CIFAR10/results_parameters/M_meanCE_legend.pdf}
				\end{subfigure}
				\caption{Influence of Regularization Parameter $M$ on Accuracy, ECE, and mCE for $M \in \{1, 3, 5\}$}
				\label{fig:M_ablation_study}
			\end{figure}

			\begin{figure}[H]
				\centering
				\begin{subfigure}{.3\textwidth}
					\centering
					\includegraphics[width=0.9\linewidth]{../code/plots/CIFAR10/results_parameters/gamma_Accuracy.pdf}
				\end{subfigure}%
				\begin{subfigure}{.3\textwidth}
					\centering
					\includegraphics[width=0.9\linewidth]{../code/plots/CIFAR10/results_parameters/gamma_ECE.pdf}
				\end{subfigure}%
				\begin{subfigure}{.3\textwidth}
					\centering
					\includegraphics[width=0.9\linewidth]{../code/plots/CIFAR10/results_parameters/gamma_meanCE_legend.pdf}
				\end{subfigure}
				\caption{Influence of Regularization Parameter $\gamma$ on Accuracy, ECE, and mCE for $\gamma \in \{10^{-2}, 10^{-1}, 1\}$}
				\label{fig:gamma_ablation_study}
			\end{figure}
			
	\section{Outlook}
		lkhasgflksadhgflksadfgh

	\section{Conclusion}
		We have presented an extension of the structured Natural Gradient Descent algorithms presented by \parencite{LNK+21} and showed that their update rule can be extended to block-diagonal and block-tridiagonal covariance matrices according to the layers of a neural network. 
		Our update rule allows the variational posterior to represent intra-layer as well as (adjacent) inter-layer weight dependencies.
		We note that this approach can be extended to include more neighboring layers as well. 
		We unified the update rule with previous approaches \parencite{LSK20} and presented an Adam-like update that with momentum in the local, auxiliary or global parameter spaces, respectively. 
		Our approach can easily be applied to ensemble methods such as Gaussian Mixture Models as well. 
		We also presented a discussion section highlighting the difficulties in high-dimensional structured Natural Gradient Descent. 
		The results show that on a smaller dataset, NGD is very competitive with strong baselines such as Temperature Scaling and Deep Ensemble while only requiring a small computational overhead of 10\% in the case of a block-diagonal covariance structure compared to Vanilla SGD, whereas the ensemble requires training multiple models with SGD and thus, training time is a multiple of standard SGD depending on the number of components. 
		However, this superior performance of the NGD methods could not be confirmed for the larger dataset. 
		In the ablation study, we found that increasing covariance capacity and rank does not significantly improve accuracy, calibration error, nor robustness to corruptions. 
		A simple diagonal covariance matrix seems to be sufficient in most cases.
		On the corrupted data, we find that due to the output averaging, much higher robustness can be achieved and that there is an advantage in modelling weight uncertainty compared to the baselines.

	\newpage
	\printbibliography
	\newpage
	\appendix
		\section{Code Repository}
			The code for the Structured NGD optimizers and plots as well as a lot more plots on different corruption types and severities can be found on the GitLab repository \url{https://gitlab.lrz.de/00000000014965FA/structured-ngd}.

		\section{More Structures}
			lkhasbflahsdjbfsdaf

		\section{Additional Results on Corrupted Data}
			\subsection{Shift Intensity Diagram}
				% Shift Intensity Diagrams for CIFAR10
				\begin{figure}[H]
					\centering
					\begin{subfigure}{\textwidth}
						\centering
						\includegraphics[width=0.9\linewidth]{{../code/plots/CIFAR10/corrupted/shift_intensity/all_BS.pdf}}
					\end{subfigure}\\
					\centering
					\begin{subfigure}{\textwidth}
						\centering
						\includegraphics[width=0.9\linewidth]{{../code/plots/CIFAR10/corrupted/shift_intensity/all_UCE.pdf}}
					\end{subfigure}
					\caption{Shift Intensity Diagram for CIFAR10-C for Brier Score and ECE}
					\label{fig:CIFAR10_ShiftIntensity}
				\end{figure}

				% Shift Intensity Diagrams for Tiny-ImageNet
				\begin{figure}[H]
					\centering
					\begin{subfigure}{\textwidth}
						\centering
						\includegraphics[width=0.9\linewidth]{{../code/plots/ImageNet/corrupted/shift_intensity/all_BS.pdf}}
					\end{subfigure}\\
					\begin{subfigure}{\textwidth}
						\centering
						\includegraphics[width=0.9\linewidth]{{../code/plots/ImageNet/corrupted/shift_intensity/all_Top-5 Accuracy.pdf}}
					\end{subfigure}\\
					\begin{subfigure}{\textwidth}
						\centering
						\includegraphics[width=0.9\linewidth]{{../code/plots/ImageNet/corrupted/shift_intensity/all_UCE.pdf}}
					\end{subfigure}\\
					\begin{subfigure}{\textwidth}
						\centering
						\includegraphics[width=0.9\linewidth]{{../code/plots/ImageNet/corrupted/shift_intensity/all_ACE.pdf}}
					\end{subfigure}\\
					\begin{subfigure}{\textwidth}
						\centering
						\includegraphics[width=0.9\linewidth]{{../code/plots/ImageNet/corrupted/shift_intensity/all_SCE.pdf}}
					\end{subfigure}
					\caption{Shift Intensity Diagram for Tiny-ImageNet-C for Brier Score, Top-5 Accuracy, UCE, ACE, and SCE}
					\label{fig:ImageNet_ShiftIntensity}
				\end{figure}
			\subsection{Reliability Diagrams}
				% Corrupted Reliability Diagrams for CIFAR10-C
				\begin{figure}
					\centering
					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/corrupted/reliability_diagrams/Vanilla_all.pdf}}}
					\end{subfigure}%
					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/corrupted/reliability_diagrams/Temp Scaling_all.pdf}}}
					\end{subfigure}%
					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/corrupted/reliability_diagrams/BBB_all.pdf}}}
					\end{subfigure}
					
					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/corrupted/reliability_diagrams/NGD (Diagonal, $k = 0$)_all.pdf}}}
					\end{subfigure}%
					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/corrupted/reliability_diagrams/NGD (RankCov, $k = 3$)_all.pdf}}}
					\end{subfigure}%
					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/corrupted/reliability_diagrams/NGD (RankCov, $k = 5$)_all.pdf}}}
					\end{subfigure}

					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/corrupted/reliability_diagrams/Deep Ensemble_all.pdf}}}
					\end{subfigure}%
					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/corrupted/reliability_diagrams/GMM (RankCov)_all.pdf}}}
					\end{subfigure}%
					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/corrupted/reliability_diagrams/GMM (Tridiagonal)_all.pdf}}}
					\end{subfigure}
					\caption{Selection of Reliability Diagrams for Baselines Vanilla SGD, Temperature Scaling, BBB, NGD with Rank Covariance Structure, Deep Ensemble, and GMMs with Block-Diagonal Rank and Block-Tridiagonal Covariance for corrupted CIFAR10-C}
					% \label{fig:CIFAR10_ReliabilityDiagrams}
				\end{figure}

				% Corrupted Reliability Diagrams for Tiny-ImageNet-C
				\begin{figure}
					\centering
					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/corrupted/reliability_diagrams/Vanilla_all.pdf}}}
					\end{subfigure}%
					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/corrupted/reliability_diagrams/Temp Scaling_all.pdf}}}
					\end{subfigure}%
					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/corrupted/reliability_diagrams/BBB_all.pdf}}}
					\end{subfigure}

					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/corrupted/reliability_diagrams/Deep Ensemble_all.pdf}}}
					\end{subfigure}%
					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/corrupted/reliability_diagrams/NGD (Diagonal, $k = 0$)_all.pdf}}}
					\end{subfigure}%
					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/corrupted/reliability_diagrams/NGD (RankCov, $k = 5$)_all.pdf}}}
					\end{subfigure}%

					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/corrupted/reliability_diagrams/NGD (Arrowhead, $k = 5$)_all.pdf}}}
					\end{subfigure}
					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/corrupted/reliability_diagrams/NGD (Tridiagonal, $k = 0$)_all.pdf}}}
					\end{subfigure}%
					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/corrupted/reliability_diagrams/NGD (Tridiagonal, $k = 5$)_all.pdf}}}
					\end{subfigure}
					\caption{Selection of Reliability Diagrams for Baselines Vanilla SGD, Temperature Scaling, BBB, NGD with Rank Covariance Structure, Deep Ensemble, and GMMs with Block-Diagonal Rank and Block-Tridiagonal Covariance for corrupted Tiny-ImageNet-C}
					% \label{fig:ImageNet_ReliabilityDiagrams}
				\end{figure}

			\subsection{Uncertainty Diagrams}
				% Corrupted Uncertainty Diagrams for CIFAR10-C
				\begin{figure}[H]
					\centering
					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/corrupted/uncertainty_diagrams/Vanilla_all.pdf}}}
					\end{subfigure}%
					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/corrupted/uncertainty_diagrams/Temp Scaling_all.pdf}}}
					\end{subfigure}%
					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/corrupted/uncertainty_diagrams/BBB_all.pdf}}}
					\end{subfigure}
					
					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/corrupted/uncertainty_diagrams/NGD (Diagonal, $k = 0$)_all.pdf}}}
					\end{subfigure}%
					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/corrupted/uncertainty_diagrams/NGD (RankCov, $k = 3$)_all.pdf}}}
					\end{subfigure}%
					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/corrupted/uncertainty_diagrams/NGD (RankCov, $k = 5$)_all.pdf}}}
					\end{subfigure}

					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/corrupted/uncertainty_diagrams/Deep Ensemble_all.pdf}}}
					\end{subfigure}%
					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/corrupted/uncertainty_diagrams/GMM (RankCov)_all.pdf}}}
					\end{subfigure}%
					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/CIFAR10/corrupted/uncertainty_diagrams/GMM (Tridiagonal)_all.pdf}}}
					\end{subfigure}
					\caption{Selection of Uncertainty Diagrams for Baselines Vanilla SGD, Temperature Scaling, BBB, NGD with Rank Covariance Structure, Deep Ensemble, and GMMs with Block-Diagonal Rank and Block-Tridiagonal Covariance for corrupted CIFAR10-C}
					% \label{fig:CIFAR10_UncertaintyDiagrams}
				\end{figure}

				% Corrupted Uncertainty Diagrams for Tiny-ImageNet-C
				\begin{figure}[H]
					\centering
					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/corrupted/uncertainty_diagrams/Vanilla_all.pdf}}}
					\end{subfigure}%
					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/corrupted/uncertainty_diagrams/Temp Scaling_all.pdf}}}
					\end{subfigure}%
					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/corrupted/uncertainty_diagrams/BBB_all.pdf}}}
					\end{subfigure}

					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/corrupted/uncertainty_diagrams/Deep Ensemble_all.pdf}}}
					\end{subfigure}%
					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/corrupted/uncertainty_diagrams/NGD (Diagonal, $k = 0$)_all.pdf}}}
					\end{subfigure}%
					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/corrupted/uncertainty_diagrams/NGD (RankCov, $k = 5$)_all.pdf}}}
					\end{subfigure}%

					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/corrupted/uncertainty_diagrams/NGD (Arrowhead, $k = 5$)_all.pdf}}}
					\end{subfigure}
					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/corrupted/uncertainty_diagrams/NGD (Tridiagonal, $k = 0$)_all.pdf}}}
					\end{subfigure}%
					\begin{subfigure}{.3\textwidth}
						\centering
						\frame{\includegraphics[width=\linewidth]{{../code/plots/ImageNet/corrupted/uncertainty_diagrams/NGD (Tridiagonal, $k = 5$)_all.pdf}}}
					\end{subfigure}
					\caption{Selection of Reliability Diagrams for Baselines Vanilla SGD, Temperature Scaling, BBB, NGD with Rank Covariance Structure, Deep Ensemble, and GMMs with Block-Diagonal Rank and Block-Tridiagonal Covariance for corrupted Tiny-ImageNet-C}
					% \label{fig:ImageNet_UncertaintyDiagrams}
				\end{figure}
			
			\subsection{Robustness}
				\begin{table}[H]
					\centering
					\begin{tabular}{LLLLLLL}
						\toprule
						\rowfont{\tiny}%
						\textbf{Method} & \textbf{Acc.} & \textbf{ECE} & \textbf{UCE} & \textbf{mCE} & \textbf{Rel. mCE} & \textbf{BS} \\ 
						\midrule\midrule
						Vanilla & 87.67 & 6.17 & 5.22 & 100.0 & 100.0 & 0.19 \\ 
						Temp Scaling & 87.59 & 1.89 & 1.03 & 100.62 & 100.47 & 0.18 \\ 
						BBB & 78.37 & 20.93 & 32.71 & 131.75 & 102.19 & 0.37 \\ 
						LL Dropout & 87.55 & 5.76 & 4.58 & 95.92 & 93.4 & 0.19 \\ 
						Dropout & 85.86 & 2.23 & 6.0 & 111.93 & 107.97 & 0.2 \\ 
						Deep Ensemble & 90.83 & 1.43 & 2.53 & 86.2 & 92.98 & 0.14 \\ 
						\midrule
						NGD (Diag, $k = 0$) & 89.25 & 1.73 & \textbf{0.82} & 95.59 & 101.41 & 0.16 \\ 
						NGD (RC, $k = 1$) & 88.04 & \textbf{1.28} & 1.26 & 101.16 & 102.43 & 0.17 \\ 
						NGD (RC, $k = 3$) & 88.85 & 1.68 & 1.04 & 95.86 & 100.38 & 0.16 \\ 
						NGD (RC, $k = 5$) & 88.6 & 2.39 & 0.95 & 93.32 & 92.59 & 0.16 \\ 
						\midrule
						NGD (AH, $k = 1$) & 88.67 & 2.68 & 1.23 & 92.32 & 92.79 & 0.17 \\ 
						NGD (AH, $k = 3$) & 86.98 & 5.18 & 9.65 & 99.07 & 97.1 & 0.2 \\ 
						NGD (AH, $k = 5$) & 88.55 & 2.29 & 1.31 & 92.41 & 92.66 & 0.17 \\ 
						\midrule
						NGD (TD, $k = 0$) & 88.63 & 2.89 & 6.01 & 95.82 & 97.54 & 0.17 \\ 
						NGD (TD, $k = 1$) & 88.25 & 3.19 & 6.4 & 99.2 & 100.56 & 0.17 \\ 
						NGD (TD, $k = 3$) & 89.04 & 4.01 & 7.45 & 91.73 & 94.17 & 0.17 \\ 
						NGD (TD, $k = 5$) & 88.37 & 3.07 & 6.18 & 96.84 & 96.54 & 0.17 \\ 
						\midrule
						GMM (RC) & 90.93 & 2.54 & 5.05 & 85.85 & 91.75 & \textbf{0.13} \\ 
						GMM (AH) & \textbf{90.97} & 3.9 & 7.45 & \textbf{82.83} & \textbf{87.91} & 0.14 \\ 
						GMM (TD) & 90.56 & 6.22 & 10.43 & 87.66 & 93.65 & 0.15 \\ 
						\bottomrule
					\end{tabular}
					\caption{Results on Corrupted Data for CIFAR10-C}
					\label{table:CIFAR10_ShiftIntensity}
				\end{table}

				\begin{table}[H]
					\centering
					\begin{tabular}{LLLLLLLL}
						\toprule
						\rowfont{\tiny}%
						\textbf{Method} & \textbf{Acc.} & \textbf{Top-5 Acc.} & \textbf{ECE} & \textbf{UCE} & \textbf{mCE} & \textbf{Rel. mCE} & \textbf{BS} \\ %\rowfont{\scriptsize}%
						\midrule\midrule
						Vanilla & 48.38 & 74.57 & 13.84 & 26.47 & 100.0 & 100.0 & 0.68 \\ 
						Temp Scaling & 48.98 & 74.81 & \textbf{1.28} & 13.86 & 99.41 & 100.24 & 0.65 \\ 
						BBB & 30.98 & 57.61 & 12.47 & \textbf{4.87} & 105.47 & \textbf{63.45} & 0.85 \\ 
						LL Dropout & 48.63 & 74.57 & 12.08 & 25.07 & 99.45 & 99.43 & 0.67 \\ 
						Dropout & 47.7 & 73.17 & 1.87 & 12.98 & 100.99 & 100.66 & 0.66 \\ 
						Deep Ensemble & \textbf{56.71} & \textbf{79.13} & 4.62 & 10.65 & \textbf{96.51} & 115.37 & \textbf{0.57} \\ 
						\midrule
						NGD (Diag, $k = 0$) & 50.11 & 76.1 & 9.65 & 22.73 & 96.68 & 96.49 & 0.65 \\ 
						NGD (RC, $k = 1$) & 47.94 & 74.22 & 8.48 & 22.26 & 97.4 & 92.03 & 0.66 \\ 
						NGD (RC, $k = 3$) & 48.68 & 74.56 & 8.35 & 21.91 & 97.35 & 94.08 & 0.65 \\ 
						NGD (RC, $k = 5$) & 48.77 & 74.75 & 8.2 & 21.78 & 97.2 & 93.97 & 0.66 \\ 
						\midrule
						NGD (AH, $k = 1$) & 46.59 & 72.98 & 7.87 & 21.84 & 98.19 & 90.22 & 0.68 \\ 
						NGD (AH, $k = 3$) & 48.04 & 73.9 & 8.22 & 21.85 & 97.74 & 93.15 & 0.66 \\ 
						NGD (AH, $k = 5$) & 47.53 & 73.82 & 8.23 & 22.09 & 98.95 & 94.87 & 0.67 \\ 
						\midrule
						NGD (TD, $k = 0$) & 49.07 & 74.68 & 8.39 & 22.12 & 99.4 & 100.52 & 0.65 \\ 
						NGD (TD, $k = 1$) & 48.1 & 74.42 & 9.51 & 23.35 & 99.53 & 98.03 & 0.66 \\ 
						NGD (TD, $k = 3$) & 48.67 & 74.71 & 8.94 & 22.71 & 100.64 & 102.56 & 0.66 \\ 
						NGD (TD, $k = 5$) & 47.84 & 74.55 & 5.78 & 20.07 & 100.14 & 98.84 & 0.66 \\ 
						\bottomrule
					\end{tabular}
					\caption{Results on Corrupted Data for Tiny-ImageNet-C}
					\label{table:ImageNet_ShiftIntensity}
				\end{table}
\end{document}
